<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Arthur Katossky &amp; Rémi Pépin">
<meta name="dcterms.date" content="2022-04-01">

<title>Big Data - From Big Data to High-performance computing</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Big Data</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-labs" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Labs</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-labs">    
        <li>
    <a class="dropdown-item" href="../../../docs/lab/lab0/lab0.html" rel="" target="">
 <span class="dropdown-text">Lab 0</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../docs/lab/lab1/lab1.html" rel="" target="">
 <span class="dropdown-text">Lab 1</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../docs/lab/lab2/lab2.html" rel="" target="">
 <span class="dropdown-text">Lab 2</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../docs/lab/lab3/lab3.html" rel="" target="">
 <span class="dropdown-text">Lab 3</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://quarto.org/" rel="" target="_blank">
 <span class="menu-text">Quarto</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://quarto.org/docs/presentations/revealjs/" rel="" target="_blank">
 <span class="menu-text">Revealjs</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/ludo2ne/ENSAI-2A-Big-Data" rel="" target="_blank"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">From Big Data to High-performance computing</h1>
<p class="subtitle lead">Lab 2 – Spark ML</p>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Arthur Katossky &amp; Rémi Pépin </p>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            ENSAI (Rennes, France)
          </p>
      </div>
    </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 1, 2022</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>In this tutorial, we are going to perform exploratory and explanatory analyses of a massive dataset consisting in hundreds of thousands of AirBnB listings, as made available by the Inside AirBnB project <a href="http://insideairbnb.com/get-the-data.html">here</a>. Rémi Pépin has loaded a lot these listings on AWS at this address: <code>s3://spark‑lab‑input‑data‑ensai20212022/airbnbb/</code></p>
<section id="a.-tutorial-setup-12" class="level2">
<h2 class="anchored" data-anchor-id="a.-tutorial-setup-12">A. Tutorial setup (1/2)</h2>
<p>Section B is purposefully independent from section A. So while you wait for your cluster to launch, you can start the tutorial!</p>
<section id="actions" class="level3">
<h3 class="anchored" data-anchor-id="actions">ACTIONS</h3>
<p>⚙️ <strong>A1.</strong> On AWS, use the EMR service to launch a notebook backed by a Spark cluster.</p>
</section>
</section>
<section id="b.-how-to-distribute-elementary-statistical-tasks" class="level2">
<h2 class="anchored" data-anchor-id="b.-how-to-distribute-elementary-statistical-tasks">B. How to distribute elementary statistical tasks?</h2>
<p><strong>The map and reduce principle</strong></p>
<p>When your data is distributed, i.e is spread out across multiple hard disks / memories on different logical or physical machines, it is clearly not possible to load everything in memory to perform some computation. (No computer from the cluster would have enough storage space / memory space to load the full data set, and the exchange of information <em>between</em> the nodes of the cluster would take considerable amounts of time.) What can you do then?</p>
<p>A surprisingly satisfying situation is when your algorithm can be expressed in a <strong>map-and-reduce model</strong><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. A <strong>map</strong> step, in computer science, is the equivalent a function in mathematics: from a given entry, return an output. Examples include counting the number of occurrences of a word in a text, squaring some number, subtracting some number, etc. A <strong>reduce</strong> step takes two inputs and produces one input, and can be called recursively onto its own outputs, progressively yielding the final result through a pyramid of <strong>accumulators</strong> (see diagram here under). Popular reduce functions include (pairwise) concatenation of character strings, (pairwise) product, (pairwise) minimum and (pairwise) maximum. But <strong>pairwise addition</strong> is probably the most used reduce function, with the aim goal of performing a complete addition:</p>
<p><img src="img/reduce.png" class="img-fluid"></p>
<p><strong>Why is the map-and-reduce scheme so interesting?</strong> Well, say you have <span class="math inline">\(n\)</span> entries and <span class="math inline">\(k\)</span> worker nodes at your disposal. The map operation can always be performed locally on each node, since the transformation does not depend on the rest of the data set. This is an <strong>embarrassingly parallel problem</strong> and we roughly divide the execution time by <span class="math inline">\(k\)</span>. Then, most of the reduce steps can also happen on the worker nodes, until the local data has been completely summarized. This also an <span class="math inline">\(k\)</span>_fold acceleration! Then, there remains only <span class="math inline">\(k\)</span> reduce steps, and since <span class="math inline">\(k \ll n\)</span>, this is usually quite negligible, even though the (potentially high) networking costs happen at this step. There is still some cost of task coordination and data exchange, but this usually small compared to the costs of parallelisation.</p>
<p><img src="img/map-end-reduce.png" class="img-fluid"></p>
<p><strong>The reduce step</strong></p>
<p><strong>A reduce function is an associative function</strong> <span class="math inline">\(f: E \times E \mapsto E\)</span>, where associativity means <span class="math inline">\(\forall (a,b,c) \in E^3, f(a,f(b,c))=f(f(a,b),c)\)</span>. This is required because the distribution of data blocks across the nodes is random, and that we want to minimize data transmission between the nodes.</p>
<p>Moreover, <strong><span class="math inline">\(f\)</span> may or may not be commutative</strong>, in the sense that <span class="math inline">\(f(a,b)=f(b,a)\)</span>. If it is the case, such as with addition and multiplication, then the computing may happen in no particular order. This means that the central node need not wait for some partial results to be returned by a belated node. On the contrary, if <span class="math inline">\(f\)</span> is not commutative, (a) the worker nodes must apply the function in a defined order, (b) the central node needs to reduce the intermediate outputs in a defined order, (c) it may have to delay the final reduce steps because of a lingering node.</p>
<p>The reduce function must not be defined on <span class="math inline">\(E=\mathbb{R}\)</span>. For instance, in the context where data is a collection of text documents, a word-count function may return accumulator objects looking like: <code>((word1,count1), (word2,count2))</code>. Also, the accumulators — that is, the outputs of the each intermediate reduce step — are not necessarily exactly the cumulative version of the final statistic our algorithm outputs! Rather, <strong>accumulators are information-dense, fast-to-compute summary statistics</strong> from which the required final statistics can be obtained.</p>
<p>Imagine you want to count the frequency of the vocal E in English, given a collection of texts. It is faster to count the number of Es as well as the total number of characters than to accumulate directly the frequencies, as shown in this diagram:</p>
<p><img src="img/reduce-frequency.png" class="img-fluid"></p>
<p><strong>Online algorithms</strong></p>
<p>An <strong>online algorithm</strong> is an algorithm with an inner state that can be actualized at low cost for any new arrival of data. A good metaphor is track-keeping of the number of people on a bus: every time a person enters or leaves, you apply ±1 to the count, without the need to systematically recount everyone. Said otherwise, an online algorithm is any algorithm whose last result can be actualized from new data, at a smaller cost than an alternative algorithm that uses both old and new data from scratch.</p>
<p>It turns out that <strong>respecting the map-and-reduce model gives us online algorithms for free</strong>, where the <strong>inner state</strong> of the algorithm is the output from the last reduce call. Indeed, writing <span class="math inline">\(s_\text{old}\)</span> and <span class="math inline">\(s_\text{new}\)</span> the old and new states (the old and new summary statistics), and <span class="math inline">\(x_new\)</span> the latest data point, we have:</p>
<p><span class="math display">\[s_\text{new}=\text{reduce}(s_\text{old}, \text{map}(x_\text{new}))\]</span></p>
<p>Thus, writing an algorithm following the map-and-reduce model gives you both a parallelized batch algorithm and a stream algorithm at once.</p>
<p><strong>Number of passes</strong></p>
<p>So far we have discussed algorithms that require only one map and one reduce functions. But for some statistics, it is not sufficient. For instance, if we want to count the number of texts where the letter E is more common than average, we first have to compute the average frequency in a first pass, then to count the texts where the frequency exceed this number with a second one. We can NOT do this in only one run, since the global average frequency is not known !</p>
<p>Each run is called a <strong>pass</strong> and some algorithms require several passes.</p>
<p><strong>Limits</strong></p>
<ul>
<li>Not all statistical algorithms can be expressed according to the map-and-reduce algorithm, and when they can, it may require a significant re-writing compared to the standard algorithms.</li>
<li>There may be a trade-off between the number of passes, the speed of each map / reduce steps and the volume of data transferred between each reduce step.</li>
</ul>
<section id="questions" class="level3">
<h3 class="anchored" data-anchor-id="questions">QUESTIONS</h3>
<p>💡 <strong>Q1.</strong> You are given <code>errors</code>, a distributed vector of prediction errors. Write a map-and-reduce algorithm for computing the <strong>total sum of squares</strong>. <em>(You may want to create a Python version of this algorithm, using the <code>map(function, vector)</code> and <code>reduce(function, vector)</code> functions. <code>reduce</code> lives in the <code>functools</code> module. You may use lambda-functions.)</em></p>
<!--
```
map :          e -> e^2
reduce : (s1,s2) -> s1+s2
```

```python
from functools import reduce
reduce(lambda s1, s2: s1+s2, map(lambda e: e**2, errors))
```
-->
<p>💡 <strong>Q2.</strong> Write <strong>two</strong> different map-and-reduce algorithm for computing the <strong><em>mean</em> sum of squares</strong>. <em>(One may include a final <span class="math inline">\(O(1)\)</span> step.)</em></p>
<!--
```
map :               e -> {m=e^2, w=1}
reduce : (obj1, obj2) -> {
    m = (obj1.m * obj1.w + obj2.m * obj2.w) / (obj1.w + obj2.w),
    w = obj1.w + obj2.w
)
```
```python
from functools import reduce
def weighted_mean(obj1, obj2):
  w = obj1["w"] + obj2["w"]
  return ({
      "m" : (obj1["m"] * obj1["w"] + obj2["m"] * obj2["w"]) / w,
      "w" : w
  })
squared_errors = map(lambda e: {"m" : e**2, "w" : 1}, errors)
reduce( weighted_mean, squared_errors )
```
... or:

```
map :               e -> {s=e^2, n=1}
reduce : (obj1, obj2) -> {
    s = obj1.s + obj2.s,
    n = obj1.n + obj2.n
final_step : s/n
```
```python
from functools import reduce

def sum_and_n(obj1, obj2) :
  return {
    "sum" : obj1["sum"] + obj2["sum"],
    "n"   : obj1["n"] + obj2["n"]
  }

squared_errors = map(lambda e: {"sum" : e**2, "n" : 1}, errors)
total_sum_and_n = reduce( sum_and_n, squared_errors )
total_sum_and_n["sum"]/total_sum_and_n["n"]
```
-->
<p>💡 <strong>Q3.</strong> Imagine you have a cluster consisting into one central node and two executor nodes, with local data already available. Let us assume that each executor node holds blocks representing approximately half of the <code>errors</code> vector. The executors perform as many of the map and reduce steps they can from the mean-computing algorithm from (<strong>Q2.</strong>). Then the central node elects one of the two executors to perform the remaining reduce steps. How much data has been transferred <strong>between</strong> the nodes? Is this susceptible to slow down the algorithm?</p>
<!--
Almost nothing:
assume executor 1 was elected for the final reduce step
executor 2 reduces all its data to a single {sum,n} object and sends the object to executor 1
executor 1 performs the final reduce step then sends it to central node

The data transfer is not susceptible to slow down the algorithm. But there is an overhead from organizing the tasks across the cluster!
-->
<p>💡 <strong>Q4</strong> Is the median easy to write as a map-and-reduce algorithm? Why?</p>
<!-- Exact median computation is not readily decomposable into a map-and-reduce algorithm, because the full set of numbers is needed in order. Parallel algorithm exist for the median, using the pivot technique. This is beyond the scope of this tutorial. -->
<p>💡 <strong>Q5</strong> Given a (distributed) series of numbers, the variance can be straightforwardly expressed as a two-pass algorithm: (a) in a first pass, compute the mean, then (b) in a second pass, compute the mean of the errors to the mean. Can it be expressed as a one-pass only algorithm? Is it more expensive to compute variance <em>and</em> mean instead of the variance alone?</p>
<!-- Yes, using the fact that V(X)=E((X-E(X))^2)=E(X^2)-E(X)^2. E(X) and E(X^2) can be computed during the same reduce map:

map :               x -> {s1=x, s2=x^2, n=1}
reduce : (obj1, obj2) -> {
    s1 = obj1.s1 + obj2.s1,
    s2 = obj1.s2 + obj2.s2,
    n = obj1.n + obj2.n
final_step : s2/n-s1^2/n

When you compute the variance, the means is computed "for free".
-->
</section>
</section>
<section id="c.-tutorial-setup-22" class="level2">
<h2 class="anchored" data-anchor-id="c.-tutorial-setup-22">C. Tutorial setup (2/2)</h2>
<section id="actions-1" class="level3">
<h3 class="anchored" data-anchor-id="actions-1">ACTIONS</h3>
<p>⚙️ <strong>A2.</strong> Use the script here under to import one file, then the whole directory.</p>
<!-- Open a notebook on AWS. Read data. -->
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># configuration: accept to pay for the data transfer</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>spark._jsc.hadoopConfiguration().<span class="bu">set</span>(<span class="st">"fs.s3.useRequesterPaysHeader"</span>,<span class="st">"true"</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.sql.types <span class="im">import</span> FloatType, IntegerType, DateType</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> regexp_replace, col</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>listings_raw <span class="op">=</span> spark.read.csv(<span class="st">"s3://spark-lab-input-data-ensai20212022/airbnb/"</span>, header<span class="op">=</span><span class="va">True</span>, multiLine<span class="op">=</span><span class="va">True</span>, escape<span class="op">=</span><span class="st">'"'</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>listings <span class="op">=</span> (listings_raw</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>  .withColumn(<span class="st">"beds"</span>,     listings_raw[<span class="st">"beds"</span>    ].cast(IntegerType()))</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>  .withColumn(<span class="st">"bedrooms"</span>, listings_raw[<span class="st">"bedrooms"</span>].cast(IntegerType()))</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>  .withColumn(<span class="st">"time"</span>, listings_raw[<span class="st">"last_scraped"</span>].cast(DateType()))</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>  .withColumn(<span class="st">"price"</span>, regexp_replace(<span class="st">'price'</span>, <span class="st">'[$</span><span class="ch">\\</span><span class="st">,]'</span>, <span class="st">''</span>).cast(FloatType()))</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>  .select(<span class="st">"id"</span>, <span class="st">"beds"</span>, <span class="st">"bedrooms"</span>, <span class="st">"price"</span>, <span class="st">"city"</span>, <span class="st">"time"</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>  .dropna() <span class="co"># remove lines with missing values</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="questions-1" class="level3">
<h3 class="anchored" data-anchor-id="questions-1">QUESTIONS</h3>
<p>💡 <strong>Q6.</strong> How many lines do the raw and the formatted datasets have?</p>
<!--
listings_raw.count()
listings.count()
-->
<p>💡 <strong>Q7.</strong> How many columns are there? Can you list all the available columns?</p>
<!--
len(listings_raw.columns)
listings_raw.columns
-->
<p>💡 <strong>Q8.</strong> What are the columns types?</p>
<!--
listings_raw.schema
listings_raw.printSchema()
listings_raw.dtypes
-->
</section>
</section>
<section id="d.-univariate-statistics-with-spark" class="level2">
<h2 class="anchored" data-anchor-id="d.-univariate-statistics-with-spark">D. Univariate statistics with Spark</h2>
<p>In practice, we don’t re-implement a map-and-reduce algorithm each time we want to compute a new summary. But there are actually several ways to compute most of univariate statistics, some you know already, some that are new.</p>
<p><strong>Spark SQL’s built-in functions</strong></p>
<p>The Spark SQL module gives grants us with <strong>built-in functions</strong> such as <code>mean()</code>, <code>sum()</code>, etc.These functions live in the <code>sql.functions</code> module:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> count, <span class="bu">min</span>, <span class="bu">max</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>listings.select(count(<span class="st">"price"</span>)).show()</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># listings.select(count(listings.price)).show() # same</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># listings.count()                              # same</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>listings.select(<span class="bu">min</span>(<span class="st">"price"</span>)).show() </span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>listings.select(<span class="bu">max</span>(<span class="st">"price"</span>)).show()</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># listings.select(min(listings.price)).show()   # same</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># listings.select(max(listings.price)).show()   # same</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Spark SQL’s <code>summary()</code> method</strong></p>
<p>In Spark SQL, <strong>elementary univariate summary statistics can also be obtained through the <code>summary()</code> method</strong>. The <code>summary()</code> method takes either the names of the statistics to compute, or nothing, in which case it computes every possible statistics:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>listings.summary(<span class="st">"count"</span>, <span class="st">"min"</span>, <span class="st">"max"</span>).show() <span class="co"># computes the selection of statistics</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>listings.summary().show() <span class="co"># computes every possible statistics</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This is a way to incite you to compute all the statistics you want at the same moment : it avoids an extra pass on the data set because all accumulators can be computed simultaneously. You can fin a list of all supported statistics <a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.summary.html">here</a> in PySpark documentation: count, mean, standard-deviation, minimum, maximum, approximate median, approximate first and last quartiles. Null (missing) values will be ignored in numerical columns before calculation.</p>
<p><strong>Spark ML</strong></p>
<p>Spark ML is a Spark module that allow us to execute parallelised versions of most popular machine-learning algorithms, such as linear or logistic regression. However, we can also use Spark ML to compute elementaty univariate summary statistics. However the philosophy is quite different, and is worth explaining<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<p><strong>Step 1: vectorisation.</strong> A little counter-intuitively, spark ML operates on a single column of your data frame, typically called <code>features</code>. (Features is the word used in the machine-learning community for “variables”, see “Vocabulary” section hereunder.) This <code>features</code> column has the <code>Vector</code> type: each element contains an array of floating-point numbers, representing a subst of the variables from your dataset. The key is that this <code>features</code> column is usually redundant with the rest of the data frame: it just ensures the proper conversion from any type we wish (string, integer…) to a standardized numeric format. Indeed, it is often derived from the other columns, as this image illustrates:</p>
<p><img src="img/vector-format.png" class="img-fluid"></p>
<p>Unfortunately for us, the construction the <code>features</code> column is not performed automatically under the hood by Spark, like when doing statistics in R. On the contrary, we have to construct the column explicitly. The <code>VectorAssembler()</code> constructor is here for that:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> VectorAssembler</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> VectorAssembler(</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    inputCols     <span class="op">=</span> [<span class="st">"price"</span>, <span class="st">"beds"</span>, <span class="st">"bedrooms"</span>], <span class="co"># the columns we want to put in the features column</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    outputCol     <span class="op">=</span> <span class="st">"features"</span>,                    <span class="co"># the name of the column ("features")</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    handleInvalid <span class="op">=</span> <span class="st">'skip'</span>                         <span class="co"># skip rows with missing / invalid values</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>listings_vec <span class="op">=</span> vectorizer.transform(listings)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Reminders:</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Spark data sets are immutable: a copy is returned, and the original is unchanged.</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Spark operations are lazy: listings_vec just contains the recipe for building vector column</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="co"># but no item of the column is computed unless explicitly asked to.</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>listings_vec.show(<span class="dv">5</span>) <span class="co"># The first 5 values of the features column are computed.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Step 2: summarization.</strong> Now that we have a vector column, we can use a <code>Summarizer</code> object to declare all the statistics we want to compute, in a similar fashion than with the Spark SQL <code>summary()</code> method. The following statistics are known: mean*, sum*, variance*, standard-deviation*, count*, number of non-zero entries, maximum*, minimum*, L2-norm, L1-norm, as can be read in <a href="https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.ml.stat.Summarizer.html">the documentation</a>. <em>(Stars (*) denote statistics that could also be computed with the <code>summary()</code> method. Approximate quartiles are not computed.)</em> Summarizers are created with the <code>Summarizer.metrics()</code> constructor. Here again, you are incited to declare all the summaries at once, so that they can all be computed in one pass:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.ml.stat    <span class="im">import</span> Summarizer</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>summarizer <span class="op">=</span> Summarizer.metrics(<span class="st">"count"</span>, <span class="st">"min"</span>, <span class="st">"max"</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>listings_vec.select( summarizer.summary(listings_vec.features), ).show(truncate<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co"># By default, the output of columns is capped to a maximum width.</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># truncate=False prevents this behaviour.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This produces the output:</p>
<p><img src="img/summary.png" class="img-fluid"></p>
<section id="questions-2" class="level3">
<h3 class="anchored" data-anchor-id="questions-2">Questions</h3>
<p>💡 <strong>Q9.</strong> Is <code>listings.summary()</code> slower to run than <code>listings.summary("count", "min", "max")</code> ? Why? <em>You can measure time in Python with this simple template:</em></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> timeit <span class="im">import</span> default_timer <span class="im">as</span> t</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> t()</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># the thing you want to measure</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Time:"</span>, t()<span class="op">-</span>start)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<!--
from timeit import default_timer as t

## version 1 ------------------

start = t()

listings.summary("count", "min", "max").show()

print ("Execution time for sumary(\"count\"...):", t()-start, "s")

## version 2 ------------------

start = t()

listings.summary().show()

print ("Execution time for sumary():", t()-start, "s")

# 0.004s and 0.002s respectively on my personal computer for 1 single AirBnB file
# The restriction to only "count", "min", "max" is only marginally faster
# This is because in both cases only one pass is performed, and the other
# statistics come almost for free.
-->
<p><strong>Q10.</strong> Compute the average number of beds per property in Barcelona in four different ways:</p>
<ol type="1">
<li><p>directly with the Spark SQL mean function,</p></li>
<li><p>using <code>summary()</code>,</p></li>
<li><p>using a <code>Sumarizer</code> object and</p></li>
<li><p>locally after you collected the bed columns.</p>
<p><em>Despite the operation being very common, Spark does <strong>not</strong> provide a simple syntax to collect a column as a local array. A work-around is to use the Pandas package and the <code>asPanda()</code> method (<a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.toPandas.html">documentation</a>). First install Pandas with <code>sc.install_pypi_package("pandas==0.25.1")</code>. Then you can collect a local copy of a dataframe called <code>df</code> with: <code>df_local = df.toPandas()</code>. A Pandas data frame possesses a <code>mean()</code> method, that compute the mean of each column of the data frame: more details are in Pandas’ <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mean.html">documentation</a>.</em></p>
<p>Which method is the fastest?</p></li>
</ol>
<!--
# 1.
from pyspark.sql.functions import mean
listings.select(mean('beds'))

# 2.
listings.select('beds').summary("mean").show()

# 3.
vectorizer = VectorAssembler(
    inputCols     = ["beds"],
    outputCol     = "features",
    handleInvalid = 'skip' # skip rows with missing / invalid values
)
summarizer = Summarizer.metrics("mean")
listings_vec = vectorizer.transform(listings)
listings_vec.select( summarizer.summary( listings_vec.features) ).show(truncate=False)

# 4.
listings.select('beds').toPandas().mean()
-->
<p><strong>Q11.</strong> The most simple model is often surprisingly difficult to beat! Compute the mean price on the data set as a predictor for an AirBnB listing’s price and the total sum of squares. (We will elaborate in the next section.)</p>
</section>
</section>
<section id="d.-regression-with-spark-ml" class="level2">
<h2 class="anchored" data-anchor-id="d.-regression-with-spark-ml">D. Regression with Spark ML</h2>
<p>A better way to predict prices is to build a regression mode, which in Spark falls under the broad category of machine-learning problems. Regressions thus belong the the <code>ml</code> module, often called Spark ML, like the summarizer that we saw just before<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<p>The <code>ml</code> module is built in a distinctive fashion than the rest of Spark. <strong>Firstly</strong> we have seen with <code>Summarizer</code> that we can not readily use the columns and that instead <strong>columns have to be first converted to a <code>Vector</code> format</strong> with the <code>VectorAssembler</code> function.</p>
<p><strong>Secondly</strong>, we need to distinguish between two different types of object classes: transformers and estimators classes. <strong>Transformers</strong> are a class of objects representing any process that modifies the dataset, and returns the modified version. It has a <strong>transform()</strong> method. <strong>Estimators</strong> on the other hand are classes of objects representing any process that produces a transformer based on some computed parameters from the data set. It has a <strong><code>fit()</code></strong> method. It is easier with an example. In the following example, <code>regressor</code> is an estimator, and we compute the regression coefficients with the <code>fit()</code> method. This produces <code>model</code>, the regression model itself, which is of class transformer. Indeed, we can use its <code>transform()</code> method to add predictions to the initial dataset.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> VectorAssembler</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.ml.regression <span class="im">import</span> LinearRegression</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> VectorAssembler( <span class="co"># copy-pasted from previous section...</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    inputCols     <span class="op">=</span> [<span class="st">"beds"</span>, <span class="st">"bedrooms"</span>], <span class="co"># ... but without price</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    outputCol     <span class="op">=</span> <span class="st">"features"</span>,</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    handleInvalid <span class="op">=</span> <span class="st">'skip'</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>listings_vec <span class="op">=</span> vectorizer.transform(listings)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>regressor <span class="op">=</span> LinearRegression(featuresCol<span class="op">=</span><span class="st">"features"</span>, labelCol<span class="op">=</span><span class="st">"price"</span>)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>model     <span class="op">=</span> regressor.fit(listings_vec)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>model.coefficients</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>model.intercept</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>listings_pred <span class="op">=</span> model.transform(listings_vec)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>listings_pred.show() <span class="co"># model and predictions from the regression</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Vocabulary</strong></p>
<p>The machine-learning community leaves at the border between computer science and mathematics. They borrow vocabulary from both sides, and it can sometimes be confusing when reading software documentation. Spark’s <code>lib</code> module uses conventions from this community :</p>
<ul>
<li><strong>label</strong>, rather than “independent variable”. This comes from the fact that historically, machine-learning has originated from problems such as image labeling (for instance digit recognition). Even for continuous variables, machine-learners may use “label”</li>
<li><strong>features</strong>, rather than “dependent variables” ; the number of features is often dubbed <span class="math inline">\(d\)</span> like dimension (instead of <span class="math inline">\(p\)</span> in statistics)</li>
<li>machine-learners don’t use the word “observation” or “unit” and prefer <strong>row</strong></li>
</ul>
<p><strong>Pipelines</strong></p>
<p>If you come to repeat several times the same series of transformations, you may take advantage of the pipeline objects. A <strong>pipeline</strong> is just a collections of steps applied to the same dataset. This helpful when you:</p>
<ul>
<li>repeat the same analysis for different regions / periods</li>
<li>want to control predictions on a new, unseen test set, and ant to apply exactly the same process</li>
</ul>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.ml.feature <span class="im">import</span> VectorAssembler</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.ml.regression <span class="im">import</span> LinearRegression</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.ml <span class="im">import</span> Pipeline</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> VectorAssembler( <span class="co"># same vectorizer as before</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    inputCols     <span class="op">=</span> [<span class="st">"beds"</span>, <span class="st">"bedrooms"</span>],</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    outputCol     <span class="op">=</span> <span class="st">"features"</span>,</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    handleInvalid <span class="op">=</span> <span class="st">'skip'</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>regressor <span class="op">=</span> LinearRegression(featuresCol<span class="op">=</span><span class="st">"features"</span>, labelCol<span class="op">=</span><span class="st">"price"</span>) <span class="co"># same regressor</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>pipeline  <span class="op">=</span> Pipeline(stages <span class="op">=</span> [vectorizer, regressor]) <span class="co"># ... but now we pack them into a pipeline</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>listings_beij <span class="op">=</span> listings.<span class="bu">filter</span>(listings.city<span class="op">==</span><span class="st">"Beijing"</span>)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>listings_barc <span class="op">=</span> listings.<span class="bu">filter</span>(listings.city<span class="op">==</span><span class="st">"Barcelona"</span>)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>model_beij <span class="op">=</span> pipeline.fit(listings_beij) <span class="co"># vectorizer AND regressor are applied</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>model_barc <span class="op">=</span> pipeline.fit(listings_barc)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model_beij.stages[<span class="dv">1</span>].coefficients) <span class="co"># model.stages[0] is the first step, model.stages[1] the second...</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model_beij.stages[<span class="dv">1</span>].intercept)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model_barc.stages[<span class="dv">1</span>].coefficients)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model_barc.stages[<span class="dv">1</span>].intercept)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Q12</strong> Interpret the results of the general regression.</p>
<!-- Average price of 263$ plus 3.8$ per bed, and minus -1.6$ per room. This should be investigated further. -->
<p><strong>Q13</strong> Collect the model’s <span class="math inline">\(R^2\)</span>. How good is our model? <em>Models have a <code>summary</code> property, that you can explore with <code>dir(model.summary)</code>.</em></p>
<!-- Very bad model, with a R2 very close to 0. -->
<p><strong>Q14</strong> Repeat the estimation separately for beijing, paris and rome. Are the coefficients stable? <em>You will build a pipeline object.</em></p>
<!-- not they are not, this is because our dataset is not balanced through time
listings.groupBy(year("time")).agg(mean("price")).show()
-->
<p><strong>Q15</strong> Are the <code>fit()</code> and <code>transform()</code> methods called eagerly or lazily? Check the execution plan with the <code>explain()</code> method for lazy evaluations.</p>
<!-- fit() is eager ; transform() is lazy -->
</section>
<section id="e.-diving-deeper" class="level2">
<h2 class="anchored" data-anchor-id="e.-diving-deeper">E. Diving deeper</h2>
<p>You are in autonomy for this section. You will find helpful:</p>
<ul>
<li>The general Spark documentation for the <code>ml</code> module: https://spark.apache.org/docs/latest/ml-guide.html</li>
<li>The PySpark documentation: https://spark.apache.org/docs/latest/api/python/index.html</li>
</ul>
<p><strong>Q16.</strong> Add a categorical variable to the regression.</p>
<p><strong>Q17.</strong> Compute the p-values of your model as well as confidence intervals for the predictions.</p>
<p><strong>Q18.</strong> Time the regression in different settings and report the results on <a href="https://docs.google.com/spreadsheets/d/1KSCLMgiepoKKiDdRrwlQv_0XYn5ptzXRB7TP-TRXCAw/edit?usp=sharing">this shared spreadsheet</a>. How does it scale with the number of listings (<span class="math inline">\(n\)</span>) ? the number of regressors (<span class="math inline">\(p\)</span>) ? the number of nodes in your cluster (<span class="math inline">\(k\)</span>) ? <em>You will only try a couple of configurations that have not been tested by others. Remember that you can order and revoke nodes from your cluster at any time from the AWS’s cluster view, in the hardware tab, on on the CORE line, “resize”.</em></p>
<p><strong>Q19.</strong> Down-sample your data set to <span class="math inline">\(n=100000\)</span>, while still keeping a few variables. Save it on S3, then download it on your computer. Run the regression locally on your computer in R. In your opinion, is the extra precision (in term of <span class="math inline">\(R^2\)</span>) is worth the extra computation time?</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Hadoop’s MapReduce is the name of what was to become today Apache Spark. The persons behind this framework were among the first to advocate for the map-and-reduce mode in order to achieve efficient parallelisation. Unfortunately, the similarity of the names causes a lot of confusion between the map-and-reduce theoretical model and the concrete Hadoop implementation. I will use “map-and-reduce” to help distinguish the algorithmic concept from the MapReduce program, but this is <em>not</em> standard in the literature.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>The syntax of Spark ML may feel artificially convoluted ; this not only an impression, it <em>is</em> convoluted. However, there are grounds for this situation : 1. Spark ML has been built on top of Spark years into the project, and the core of Spark is not well adapted to machine-learning ; 2. Spark ML is intended for much more advanced treatments than unviariate statistics, and we will see linear regression as an exemple at the end of this tutorial<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>There is an old module called <code>mllib</code> that is also called “Spark ML”. That can cause confusion.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'alternate';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>