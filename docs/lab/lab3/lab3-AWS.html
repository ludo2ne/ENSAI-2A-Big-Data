<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Big Data - Lab 3 - Stream processing with Spark</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Big Data</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-labs" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Labs</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-labs">    
        <li>
    <a class="dropdown-item" href="../../../docs/lab/lab-setup.html" rel="" target="">
 <span class="dropdown-text">Lab setup</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../docs/lab/lab0/lab0.html" rel="" target="">
 <span class="dropdown-text">Lab 0</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../docs/lab/lab1/lab1-SSPCloud.html" rel="" target="">
 <span class="dropdown-text">Lab 1</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../docs/lab/lab2/lab2-SSPCloud.html" rel="" target="">
 <span class="dropdown-text">Lab 2</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../docs/lab/lab3/lab3.html" rel="" target="">
 <span class="dropdown-text">Lab 3</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/ludo2ne/ENSAI-2A-Big-Data" rel="" target="_blank"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Lab 3 - Stream processing with Spark</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p><strong>DO NOT FORGET TO TURN YOUR CLUSTER OFF A THE END OF THIS TUTORIAL!</strong></p>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">1. üîîOutline</h2>
<p>This tutorial will teach you the basic of <strong>stream processing with Spark</strong>. As soon as an application compute something with business value (for instance customer activity), and new inputs arrive continuously, companies will want to compute this result continuously too. Spark makes possible to process stream with the <strong>Structured Streaming API</strong>. This lab will teach you the basics of this Spark‚Äôs API. Because the Structured Streaming API is based on the DataFrame API most syntaxes of tutorial 1 are still relevant.</p>
</section>
<section id="service-creation-in-onyxia" class="level2">
<h2 class="anchored" data-anchor-id="service-creation-in-onyxia">‚õÖ‚Äã 2. Service creation in Onyxia</h2>
<p>First: <strong>DO NOT FORGET TO TURN YOUR SERVICE OFF A THE END OF THIS TUTORIAL!</strong></p>
</section>
<section id="notebook-configuration" class="level2">
<h2 class="anchored" data-anchor-id="notebook-configuration">‚öô 3. Notebook configuration</h2>
<p>Because Spark Streaming need to write some data on the underlying HDFS cluster, we need to configure our cluster. The cluster configuration will be a little bit more complexe than the previous ones. 1. You need to select a SSH key to your cluster 2. Once you cluster is running, go to the EC2 dashboard like in lab 0 3. Click on the instance with the security groups : <code>ElasticMapReduceEditors-Livy,ElasticMapReduce-master</code> 4. Go to the security tab 5. Click on the security group <code>ElasticMapReduce-master</code> 6. Clink on edit <code>inbound rules</code> (<code>Modifier les r√®gles entrantes</code> en fran√ßais) 7. Add a rule SSH, to all IPv4 8. Save your rules 9. Go bask to the EC2 instance with the security group <code>ElasticMapReduce-master</code> 10. Click on <code>Connect</code> ( <code>Se connecter</code> en fran√ßais) 11. Again clik on <code>Connect</code> 12. In the cloud shell copy/paste : <code>sudo usermod -a -G hdfsadmingroup livy</code>. It will give to the the user ‚Äúlivy‚Äù admin right.</p>
<div class="cell" data-tags="[]" data-vscode="{&quot;languageId&quot;:&quot;json&quot;}">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Configuraion</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># The user pay the data transfer</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>spark._jsc.hadoopConfiguration().<span class="bu">set</span>(<span class="st">"fs.s3.useRequesterPaysHeader"</span>,<span class="st">"true"</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the number of shufflre partitions</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>spark.conf.<span class="bu">set</span>(<span class="st">"spark.sql.shuffle.partitions"</span>, <span class="dv">5</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>spark.conf.<span class="bu">set</span>(<span class="st">'spark.driver.memory'</span>, <span class="st">"1000M"</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import all the needed library</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> time <span class="im">import</span> sleep</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> from_json, window, col, expr, size, explode, avg, <span class="bu">min</span>, <span class="bu">max</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.sql.types <span class="im">import</span> StructType,StructField, StringType, IntegerType, ArrayType, TimestampType, BooleanType, LongType, DoubleType</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="tutorial-3---stream-processing-with-spark" class="level1">
<h1>Tutorial 3 - Stream processing with Spark</h1>
<p><strong>DO NOT FORGET TO TURN YOUR CLUSTER OFF A THE END OF THIS TUTORIAL!</strong></p>
<section id="outline-1" class="level2">
<h2 class="anchored" data-anchor-id="outline-1">1. üîîOutline</h2>
<p>This tutorial will teach you the basic of <strong>stream processing with Spark</strong>. As soon as an application compute something with business value (for instance customer activity), and new inputs arrive continuously, companies will want to compute this result continuously too. Spark makes possible to process stream with the <strong>Structured Streaming API</strong>. This lab will teach you the basics of this Spark‚Äôs API. Because the Structured Streaming API is based on the DataFrame API most syntaxes of tutorial 1 are still relevant.</p>
</section>
<section id="spark-cluster-creation-in-aws" class="level2">
<h2 class="anchored" data-anchor-id="spark-cluster-creation-in-aws">‚õÖ‚Äã 2. Spark cluster creation in AWS</h2>
<p>First: <strong>DO NOT FORGET TO TURN YOUR CLUSTER OFF A THE END OF THIS TUTORIAL!</strong></p>
<p>Instructions are at the beginning of lab 2. Or you can just clone you cluster ;)</p>
</section>
<section id="notebook-configuration-1" class="level2">
<h2 class="anchored" data-anchor-id="notebook-configuration-1">‚öô 3. Notebook configuration</h2>
<p>Because Spark Streaming need to write some data on the underlying HDFS cluster, we need to configure our cluster. The cluster configuration will be a little bit more complexe than the previous ones. 1. You need to select a SSH key to your cluster 2. Once you cluster is running, go to the EC2 dashboard like in lab 0 3. Click on the instance with the security groups : <code>ElasticMapReduceEditors-Livy,ElasticMapReduce-master</code> 4. Go to the security tab 5. Click on the security group <code>ElasticMapReduce-master</code> 6. Clink on edit <code>inbound rules</code> (<code>Modifier les r√®gles entrantes</code> en fran√ßais) 7. Add a rule SSH, to all IPv4 8. Save your rules 9. Go bask to the EC2 instance with the security group <code>ElasticMapReduce-master</code> 10. Click on <code>Connect</code> ( <code>Se connecter</code> en fran√ßais) 11. Again clik on <code>Connect</code> 12. In the cloud shell copy/paste : <code>sudo usermod -a -G hdfsadmingroup livy</code>. It will give to the the user ‚Äúlivy‚Äù admin right.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;json&quot;}">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Configuraion</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># The user pay the data transfer</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>spark._jsc.hadoopConfiguration().<span class="bu">set</span>(<span class="st">"fs.s3.useRequesterPaysHeader"</span>,<span class="st">"true"</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the number of shuffle partitions</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>spark.conf.<span class="bu">set</span>(<span class="st">"spark.sql.shuffle.partitions"</span>, <span class="dv">5</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the worker memory</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>spark.<span class="bu">set</span>(<span class="st">'spark.driver.memory'</span>, <span class="st">"1000M"</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Import all the needed library</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> time <span class="im">import</span> sleep</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> from_json, window, col, expr, size, explode, avg, <span class="bu">min</span>, <span class="bu">max</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.sql.types <span class="im">import</span> StructType,StructField, StringType, IntegerType, ArrayType, TimestampType, BooleanType, LongType, DoubleType</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Explanation:</strong></p>
<ul>
<li><p><code>spark._jsc.hadoopConfiguration().set("fs.s3.useRequesterPaysHeader","true")</code> : likes in lab2, you will be charged for the data transfer. without this configuration you can‚Äôt access the data.</p></li>
<li><p><code>spark.conf.set("spark.sql.shuffle.partitions", 5)</code> : set the number of partitions for the shuffle phase. A partition is in Spark the name of a bloc of data. By default Spark use 200 partitions to shuffle data. But in this lab, our mini-batch will be small, and to many partitions will lead to performance issues.</p>
<pre><code>spark.conf.set("spark.sql.shuffle.partitions", 5)</code></pre>
<blockquote class="blockquote">
<p>:thinking: The shuffle dispatches data according to their key between a <em>map</em> and a <em>reduce</em> phase. For instance, if you are counting how many records have each <code>g</code> group, the <em>map</em> phase involve counting each group member in each Spark partition : <code>{g1:5, g2:10, g4:1, g5:3}</code> for one partition, <code>{g1:1, g2:2, g3:23, g5:12}</code> for another. The <em>shuffle</em> phase dispatch those first results and group them by key in the same partition, one partition gets <code>{g1:5, g1:1, g2:10, g2:2}</code>, the other gets : <code>{g4:1, g5:3, g3:23, g5:12}</code> Then each <em>reduce</em> can be done efficiently.</p>
</blockquote></li>
<li><p><code>spark.set('spark.driver.memory', "1000M")</code> : set the worker memory to 1Gb of Ram.</p></li>
<li><p>:package: Import all needed library</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> time <span class="im">import</span> sleep</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> from_json, window, col, expr</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.sql.types <span class="im">import</span> StructType,StructField, StringType, IntegerType, ArrayType, TimestampType, BooleanType, LongType, DoubleType</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul>
</section>
<section id="stream-processing" class="level2">
<h2 class="anchored" data-anchor-id="stream-processing">4.üöø Stream processing</h2>
<p>Stream processing is the act to process data in real-time. When a new record is available, it is processed. There is no real beginning nor end to the process, and there is no ‚Äúresult‚Äù. The result is updated in real time, hence multiple versions of the results exist. For instance, you want to count how many tweet about cat are posted in twitter every hour. Until the end of an hour, you do not have you final result. And even at this moment, your result can change. Maybe some technical problems created some latency and you will get some tweets later. And you will need to update your previous count.</p>
<p>Some commons use cases of stream processing are :</p>
<ul>
<li><strong>Notifications and alerting :</strong> real-time bank fraud detection ; electric grid monitoring with smart meters ; medical monitoring with smart meters, etc.</li>
<li><strong>Real time reporting:</strong> traffic in a website updated every minute; impact of a publicity campaign ; stock option portfolio, etc.</li>
<li><strong>Incremental ELT (extract transform load):</strong> new unstructured data are always available and they need to be processed (cleaned, filtered, put in a structured format) before their integration in the company IT system.</li>
<li><strong>Online machine learning :</strong> new data are always available and used by a ML algorithm to improve its performance dynamically.</li>
</ul>
<p>Unfortunately, stream processing has some issues. First because there is no end to the process, you cannot keep all the data in memory. Second, process a chain of event can be complex. How do you raise an alert when you receive the value 5, 6 and 3 consecutively ? Don‚Äôt forget you are in a distributed environment, and there is latency. Hence, the received order can be different from the emitted order.</p>
</section>
<section id="spark-and-stream-processing" class="level2">
<h2 class="anchored" data-anchor-id="spark-and-stream-processing">5. ‚ú® Spark and stream processing üöø</h2>
<p>Stream processing was gradually incorporated in Spark. In 2012 Spark Streaming and it‚Äôs DStreams API was added to Spark (it was before an external project). This made it possible use high-level operator like <code>map</code> and <code>reduce</code> to process stream of data. Because of its implementation, this API has some limitations, and its syntax was different from the DataFrame one. Thus, in 2016 a new API was added, the Structured Streaming API. This API is directly build built on DataFrame, unlike DStreams. <strong>This has an advantage, you can process your stream like static data</strong>. Of course there are some limitations, but the core syntaxes is the same. You will chain transformations, because each transformation takes a DataFrame as input and produces a DataFrame as output. The big change is there is no action at the end, but an <strong><a href="#2.2%20How%20to%20output%20a%20stream%20?">output sink</a></strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://databricks.com/wp-content/uploads/2016/07/image01-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">data stream</figcaption>
</figure>
</div>
<p><em>Figure 1 : data stream representation (source <a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#basic-concepts">structured streaming programming guide</a>)</em></p>
<p>Spark offer two ways to process stream, one <strong>record at a time</strong>, or processing <strong>micro batching</strong> (processing a small amount of line at once).</p>
<ul>
<li><strong>one record at a time</strong> every time a new record is available it‚Äôs processed. This has a big advantage, it achieves <strong>very low latency </strong>. But there is a drawback, the system can not handle too much data at the same time (low throughput). It‚Äôs the default mode. Because in this lab, you will process files with record, even if you will process one file at a time, you will process mini batch of records</li>
<li>as for <strong>micro batching</strong> it process new records every <code>t</code> seconds. Hence records are not process really in ‚Äúreal-time‚Äù, but periodically, <strong>the latency will be higher, and so the throughput</strong>. Unless you really need low latency, make it you first choice option.</li>
</ul>
<blockquote class="blockquote">
<p>üßê To get the best ratio latency/throughput, a good practice is to decrease the micro-batch size until the mini-batch throughput is the same as the input throughput. Then increase the size to have some margin</p>
</blockquote>
<p><img src="https://github.com/HealerMikado/panorama_big_data_2021/blob/main/labs/lab%203%20-%20Spark%20Stream/img/spart_latency_requirement.png?raw=true" class="img-fluid"></p>
<p><em>Figure 2 : which Spark solution suit best giving latency requirement (source : <a href="https://pages.databricks.com/rs/094-YMS-629/images/LearningSpark2.0.pdf">Learning Spark, O‚ÄôReilly</a>)</em></p>
<blockquote class="blockquote">
<p>To understand why processing one record at a time has lower latency and throughput than batch processing, imagine a restaurant. Every time a client order something the chef cooks its order independently of the other current orders. So if two clients order pizza, the chief makes two small doughs, and cook them individually. If clients there is only a few clients, the chief can finish each order before a new client comes. The latency is the lowest possible when the chief is idle when a client come. Know imagine a restaurant were the chief process the orders by batch. He waits some minutes to gather all the orders than he mutualizes the cooking. If there are 5 pizza orders, he only does one big dough, divides it in five, add the toppings then cook all five at once. The latency is higher because the chief waits before cooking, but so the throughput because he can cook multiple things at once.</p>
</blockquote>
</section>
<section id="the-basics-of-sparks-structured-streaming" class="level2">
<h2 class="anchored" data-anchor-id="the-basics-of-sparks-structured-streaming">6. ü•â The basics of Spark‚Äôs Structured Streaming</h2>
<section id="the-different-sources-for-stream-processing-in-spark" class="level3">
<h3 class="anchored" data-anchor-id="the-different-sources-for-stream-processing-in-spark">6.1. üìö The different sources for stream processing in Spark</h3>
<p>In lab 2 you discovered Spark DataFrame, in this lab you will learn about <a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html">Structured Streaming</a>. It‚Äôs a stream processing framework built on the Spark SQL engine, and it uses the existing structured APIs in Spark. So one you define a way to read a stream, you will get a DataFrame. Like in lab2 ! <strong>So except state otherwise, all transformations presented in lab2 are still relevant in this lab</strong>.</p>
<p>Spark Streaming supports several input source for reading in a streaming fashion :</p>
<ul>
<li><a href="https://kafka.apache.org/">Apache Kafka</a> an open-source distributed event streaming platform (not show in this lab)</li>
<li>Files on distributed file system like HDFS or S3 (Spark will continuously read new files in a directory)</li>
<li>A network socket : an end-point in a communication across a network (sort of very simple webservice). It‚Äôs not recommend for <em>production</em> application, because a socket connection doesn‚Äôt provide any mechanism to check the consistency of data.</li>
</ul>
<p>Defining an input source is like loading a DataFrame but, you have to replace <code>spark.read</code> by <code>spark.readStream</code>. For instance, if I want to open a stream to a folder located in S3 you have to read every new files put in it, just write</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>my_first_stream <span class="op">=</span> spark<span class="op">\</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>.readStream<span class="op">\</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>.schema(schema_tweet)<span class="op">\</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>.json(<span class="st">"s3://my-awesome-bucket/my-awesome-folder"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The major difference with lab2, it is Spark cannot infer the schema of the stream. You have to pass it to Spark. There is two ways :</p>
<ul>
<li>A reliable way : you define the schema by yourself and gave it to Spark</li>
<li>A quick way : you load one file of the folder in a DataFrame, extract the schema and use it. It works, but the schema can be incomplete. It‚Äôs a better solution to create the schema by hand and use it.</li>
</ul>
<p>For Apache Kafka, or socket , it‚Äôs a slightly more complex, <em>(not used today, it‚Äôs jute for you personal knowledge)</em> :</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>my_first_stream <span class="op">=</span> spark<span class="op">\</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>.readStream<span class="op">\</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>.<span class="bu">format</span>(<span class="st">"kafka"</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>.option(<span class="st">"kafka.bootstrat.servers"</span>, <span class="st">"host1:port1, host2:port2 etc"</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>.option(<span class="st">"subscribePattern"</span>, <span class="st">"topic name"</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>.load()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="why-is-a-folder-a-relevant-source-in-stream-processing" class="level4">
<h4 class="anchored" data-anchor-id="why-is-a-folder-a-relevant-source-in-stream-processing">6.1.1 ü§®Why is a folder a relevant source in stream processing ?</h4>
<p>Previously, in lab 1, you loaded all the files in a folder stored in S3 with Spark. And it worked pretty well. But this folder was static, in other words, Its content didn‚Äôt change. But in some cases, new data are constantly written into a folder. For instance, in this lab you will process a stream of tweets. A python script is running in a EC2 machine reading tweets from the Twitter‚Äôs web service and writing them in a S3 buckets. Every 2 seconds or so, a new file is added to the bucket with 1000 tweets. If you use DataFrame like in lab 1, your process cannot proceed those new files. You should relaunch your process every time. But with Structured Streaming Spark will dynamically load new files.</p>
<p><img src="https://github.com/HealerMikado/panorama_big_data_2021/blob/main/labs/lab%203%20-%20Spark%20Stream/img/stream_pipeline_twitter.png?raw=true" class="img-fluid"></p>
<p><em>Figure 3 : Complete lab architecture to stream process tweets</em></p>
<blockquote class="blockquote">
<p>The remaining question is, why don‚Äôt we connect Spark to the twitter webservice directly ? And the answer is : we can‚Äôt. Spark cannot be connected to a webservice directly. You need a middle-man between Spark and a webservice. There are multiple solutions, but an easy and reliable one is to write tweet to s3 (because we use AWS services, if you use Microsoft Azure, Google Cloud Platform or OVH cloud replace S3 by their storage service).</p>
</blockquote>
</section>
</section>
<section id="hand-on-1-open-a-stream" class="level3">
<h3 class="anchored" data-anchor-id="hand-on-1-open-a-stream">‚úçHand-on 1 : open a stream</h3>
<p>Like in lab 1, you will use tweets in this lab. The tweets are stored in jsonl file (<em>json line</em> every line of the file is a complete json). Here is an example. The schema changed a little, because this time tweets aren‚Äôt pre-processed.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"data"</span><span class="op">:</span> {</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>        <span class="st">"public_metrics"</span><span class="op">:</span> {</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>            <span class="st">"retweet_count"</span><span class="op">:</span> <span class="dv">0</span><span class="op">,</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>            <span class="st">"reply_count"</span><span class="op">:</span> <span class="dv">0</span><span class="op">,</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>            <span class="st">"like_count"</span><span class="op">:</span> <span class="dv">0</span><span class="op">,</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>            <span class="st">"quote_count"</span><span class="op">:</span> <span class="dv">0</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        }<span class="op">,</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        <span class="st">"text"</span><span class="op">:</span> <span class="st">"Day 93. Tweeting every day until Colby cheez its come back #bringcolbyback @cheezit"</span><span class="op">,</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        <span class="st">"possibly_sensitive"</span><span class="op">:</span> <span class="kw">false</span><span class="op">,</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">"created_at"</span><span class="op">:</span> <span class="st">"2021-05-03T07:55:46.000Z"</span><span class="op">,</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>        <span class="st">"id"</span><span class="op">:</span> <span class="st">"1389126523853148162"</span><span class="op">,</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        <span class="st">"entities"</span><span class="op">:</span> {</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>            <span class="st">"annotations"</span><span class="op">:</span> [</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>                {</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"start"</span><span class="op">:</span> <span class="dv">33</span><span class="op">,</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"end"</span><span class="op">:</span> <span class="dv">43</span><span class="op">,</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"probability"</span><span class="op">:</span> <span class="fl">0.5895</span><span class="op">,</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"type"</span><span class="op">:</span> <span class="st">"Person"</span><span class="op">,</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"normalized_text"</span><span class="op">:</span> <span class="st">"Colby cheez"</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>                }</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>            ]<span class="op">,</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>            <span class="st">"mentions"</span><span class="op">:</span> [</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>                {</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"start"</span><span class="op">:</span> <span class="dv">75</span><span class="op">,</span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"end"</span><span class="op">:</span> <span class="dv">83</span><span class="op">,</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"username"</span><span class="op">:</span> <span class="st">"cheezit"</span></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>                }</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>            ]<span class="op">,</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>            <span class="st">"hashtags"</span><span class="op">:</span> [</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>                {</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"start"</span><span class="op">:</span> <span class="dv">59</span><span class="op">,</span></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"end"</span><span class="op">:</span> <span class="dv">74</span><span class="op">,</span></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"tag"</span><span class="op">:</span> <span class="st">"bringcolbyback"</span></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>                }</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>        }<span class="op">,</span></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>        <span class="st">"lang"</span><span class="op">:</span> <span class="st">"en"</span><span class="op">,</span></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>        <span class="st">"source"</span><span class="op">:</span> <span class="st">"Twitter for iPhone"</span><span class="op">,</span></span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>        <span class="st">"author_id"</span><span class="op">:</span> <span class="st">"606856313"</span></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>    }<span class="op">,</span></span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>    <span class="st">"includes"</span><span class="op">:</span> {</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>        <span class="st">"users"</span><span class="op">:</span> [</span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>            {</span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>                <span class="st">"created_at"</span><span class="op">:</span> <span class="st">"2012-06-13T03:36:00.000Z"</span><span class="op">,</span></span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>                <span class="st">"username"</span><span class="op">:</span> <span class="st">"DivinedHavoc"</span><span class="op">,</span></span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>                <span class="st">"verified"</span><span class="op">:</span> <span class="kw">false</span><span class="op">,</span></span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>                <span class="st">"name"</span><span class="op">:</span> <span class="st">"Justin"</span><span class="op">,</span></span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>                <span class="st">"id"</span><span class="op">:</span> <span class="st">"606856313"</span></span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Q1</strong> Define a variable with this schema (you will find a file <em>schema pyspark tweet</em> on moodle with the schema to copy /aste)</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.sql.types <span class="im">import</span> StructType,StructField, StringType, IntegerType, ArrayType, TimestampType, BooleanType, LongType, DoubleType</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>StructType([</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>  StructField(<span class="st">"data"</span>, StructType([</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>      StructField(<span class="st">"author_id"</span>,StringType(),<span class="va">True</span>),</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>      StructField(<span class="st">"text"</span>,StringType(),<span class="va">True</span>),</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>      StructField(<span class="st">"source"</span>,StringType(),<span class="va">True</span>),</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>      StructField(<span class="st">"lang"</span>,StringType(),<span class="va">True</span>),</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>      StructField(<span class="st">"created_at"</span>,TimestampType(),<span class="va">True</span>),</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>      StructField(<span class="st">"entities"</span>,StructType([</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>          StructField(<span class="st">"annotations"</span>, ArrayType(StructType([</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"end"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"normalized_text"</span>, StringType(), <span class="va">True</span>),</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"probability"</span>, DoubleType(), <span class="va">True</span>),</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"start"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"type"</span>, StringType(), <span class="va">True</span>)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>          ]),<span class="va">True</span>),<span class="va">True</span>),</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>          StructField(<span class="st">"cashtags"</span>, ArrayType(StructType([</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"end"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"start"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"tag"</span>, StringType(), <span class="va">True</span>)</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>          ]),<span class="va">True</span>),<span class="va">True</span>),</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>           StructField(<span class="st">"hashtags"</span>, ArrayType(StructType([</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"end"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"start"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"tag"</span>, StringType(), <span class="va">True</span>)</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>          ]),<span class="va">True</span>),<span class="va">True</span>),</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>          StructField(<span class="st">"mentions"</span>, ArrayType(StructType([</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"end"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"start"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"username"</span>, StringType(), <span class="va">True</span>)</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>          ]),<span class="va">True</span>),<span class="va">True</span>),</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>          StructField(<span class="st">"urls"</span>, ArrayType(StructType([</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"description"</span>, StringType(), <span class="va">True</span>),</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"display_url"</span>, StringType(), <span class="va">True</span>),</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"end"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"expanded_url"</span>, StringType(), <span class="va">True</span>),</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"images"</span>, ArrayType(StructType([</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>                      StructField(<span class="st">"height"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>                      StructField(<span class="st">"url"</span>, StringType(), <span class="va">True</span>),</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>                      StructField(<span class="st">"width"</span>, LongType(), <span class="va">True</span>)</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>                  ]),<span class="va">True</span>),<span class="va">True</span>),</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"start"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"status"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"title"</span>, StringType(), <span class="va">True</span>),</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"unwound_url"</span>, StringType(), <span class="va">True</span>),</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"url"</span>, StringType(), <span class="va">True</span>),</span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>          ]),<span class="va">True</span>),<span class="va">True</span>),</span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>      ]),<span class="va">True</span>),</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>      StructField(<span class="st">"public_metrics"</span>, StructType([</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>          StructField(<span class="st">"like_count"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a>          StructField(<span class="st">"reply_count"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>          StructField(<span class="st">"retweet_count"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>          StructField(<span class="st">"quote_count"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a>      ]),<span class="va">True</span>)</span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a>    ]),<span class="va">True</span>),</span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a>  StructField(<span class="st">"includes"</span>, StructType([</span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a>      StructField(<span class="st">"users"</span>, ArrayType(StructType([</span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a>          StructField(<span class="st">"created_at"</span>, TimestampType(), <span class="va">True</span>),</span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a>          StructField(<span class="st">"id"</span>, StringType(), <span class="va">True</span>),</span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a>          StructField(<span class="st">"name"</span>, StringType(), <span class="va">True</span>),</span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a>          StructField(<span class="st">"username"</span>, StringType(), <span class="va">True</span>),</span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a>          StructField(<span class="st">"verified"</span>, BooleanType(), <span class="va">True</span>)</span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a>      ]),<span class="va">True</span>),<span class="va">True</span>)</span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a>  ]),<span class="va">True</span>)</span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a>  ])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Q2</strong> Create a stream to this s3 bucket : <code>s3://spark-lab-input-data-ensai20222023/stream_tweet/</code>. Name it <code>tweet_stream</code></p>
<blockquote class="blockquote">
<p>ü§î Nothing happen ? It‚Äôs normal ! Do not forget, Spark use lazy evaluation. It doesn‚Äôt use data if you don‚Äôt define an action. For now Spark only know how to get the stream, that‚Äôs all.</p>
</blockquote>
<p><strong>Q3</strong> In a cell just execute <code>tweet_stream</code>. It should print the type of <code>tweet_stream</code> and the associated schema. You can see you created a DataFrame like in lab2 !</p>
<p><strong>Q4</strong> Print the size of your DataFrame by using this piece of code :</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>stream_size_query<span class="op">=</span> tweet_stream<span class="op">\</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>.writeStream<span class="op">\</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>.queryName(<span class="st">"stream_size"</span>)<span class="op">\</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>.<span class="bu">format</span>(<span class="st">"memory"</span>)<span class="op">\</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>.start()</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>): <span class="co"># we use an _ because the variable isn't used. You can use i if you prefere</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    sleep(<span class="dv">3</span>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    spark.sql(<span class="st">"""</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="st">      SELECT count(1) FROM stream_size</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="st">    """</span>).show()</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>stream_size_query.stop() <span class="co">#needed to close the query</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="section" class="level3">
<h3 class="anchored" data-anchor-id="section"></h3>
</section>
<section id="how-to-output-a-stream" class="level3">
<h3 class="anchored" data-anchor-id="how-to-output-a-stream">6.2. üõí How to output a stream ?</h3>
<p>Remember, Spark has two types of methods to process DataFrame:</p>
<ul>
<li>Transformations which take a DataFrame has input and produce an other Dataframe</li>
<li>And actions, which effectively run computation and produce something, like a file, or a output in you notebook/console.</li>
</ul>
<p>Stream processing looks the same as DataFrame processing. Hence, <strong>you still have transformations</strong>, the exact same one that can be apply on classic DataFrame (with some restriction, for example you can not sample a stream with the <code>sample()</code> transformation). The action part is a little different. Because a stream runs continuously, you cannot just print the data or run a count at the end of the process. <strong>In fact actions will nor work on stream</strong>. To tackle this issue, Spark proposes different <a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks">outputs sinks</a>. An output sink is a possible output for your stream. The different output sink are (this part came from the official Spark <a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks">documentation)</a> :</p>
<ul>
<li><strong>File sink</strong> - Stores the output to a file. The file can be stored locally (on the cluster), remotely (on S3). The file format can be json, csv etc</li>
</ul>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>writeStream<span class="op">\</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>.<span class="bu">format</span>(<span class="st">'json'</span>)<span class="op">\</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>.option(<span class="st">"checkpointLocation"</span>, <span class="st">"output_folder/history"</span>) \ </span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>.option(<span class="st">"path"</span>, <span class="st">"output_folder"</span>)<span class="op">\</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>.start()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><p><strong>Kafka sink</strong> - Stores the output to one or more topics in Kafka.</p></li>
<li><p><strong>Foreach sink</strong> - Runs arbitrary computation on the records in the output. It does not produce an DataFrame. Each processed lines lost</p></li>
</ul>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>writeStream</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    .foreach(...)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    .start()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>Console sink (for debugging)</strong> - Prints the output to the console standard output (<em>stdout</em>) every time there is a trigger. This should be used for debugging purposes on low data volumes as the entire output is collected and stored in the driver‚Äôs memory after every trigger. <em>Sadly console sink does not work with jupyter notebook</em>.</li>
</ul>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>writeStream</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    .<span class="bu">format</span>(<span class="st">"console"</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    .start()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>Memory sink (for debugging)</strong> - The output is stored in memory as an in-memory table. This should be used for debugging purposes on low data volumes as the entire output is collected and stored in the driver‚Äôs memory. Hence, use it with caution. Because we are in a simple lab, you will use this solution. But keep in mind it‚Äôs a very bad idea because data must fit in the the ram of the driver node. And in a big data context it‚Äôs impossible. Because it‚Äôs not a big data problem if one computer can tackle it.</li>
</ul>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>writeStream</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    .<span class="bu">format</span>(<span class="st">"memory"</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    .queryName(<span class="st">"tableName"</span>) <span class="co"># to resquest the table with spark.sql()</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    .start()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We just talked where we can output a stream, but there is another question, how ?</p>
<p>To understand why it‚Äôs a issue, let‚Äôs talk about two things that spark can do with streams : filter data and group by + aggregation</p>
<ul>
<li><strong>Filter</strong> : your process is really simple. Every time you get a new data you just compute a score and drop records with a score less than a threshold. Then you write into a file every kept record. In a nutshell, you just append new data to a file. Spark does not have to read an already written row, it just add new data.</li>
<li><strong>Group by + aggregation</strong> : in this case you want to group by your data by key than compute a simple count. Then you want to write the result in a file. But now there is an issue, Spark needs to update some existing rows in your file every time it writes somethings. But is your file is stored in HDFS of S3, it‚Äôs impossible to update in a none append way a file. In a nutshell, it‚Äôs impossible to output in a file your operation.</li>
</ul>
<p>To deal with this issue, Spark proposes 3 mode. <strong>And you cannot use every mode with every output sink, with every transformation</strong>. The 3 modes are (<a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#starting-streaming-queries">more info here</a>) :</p>
<ul>
<li><strong>Append mode (default)</strong> - This is the default mode, where only the new rows added to the Result Table since the last trigger will be outputted to the sink. This is supported for only those queries where rows added to the Result Table is never going to change. Hence, this mode guarantees that each row will be output only once (assuming fault-tolerant sink). For example, queries with only <code>select</code>, <code>where</code>, <code>map</code>, <code>flatMap</code>, <code>filter</code>, <code>join</code>, etc. will support Append mode.</li>
<li><strong>Complete mode</strong> - The whole Result Table will be outputted to the sink after every trigger. This is supported for aggregation queries.</li>
<li><strong>Update mode</strong> - (<em>Available since Spark 2.1.1</em>) Only the rows in the Result Table that were updated since the last trigger will be outputted to the sink. More information to be added in future releases.</li>
</ul>
<table class="table">
<thead>
<tr class="header">
<th>Sink</th>
<th>Supported Output Modes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>File Sink</strong></td>
<td>Append</td>
</tr>
<tr class="even">
<td><strong>Kafka Sink</strong></td>
<td>Append, Update, Complete</td>
</tr>
<tr class="odd">
<td><strong>Foreach Sink</strong></td>
<td>Append, Update, Complete</td>
</tr>
<tr class="even">
<td><strong>ForeachBatch Sink</strong></td>
<td>Append, Update, Complete</td>
</tr>
<tr class="odd">
<td><strong>Console Sink</strong></td>
<td>Append, Update, Complete</td>
</tr>
<tr class="even">
<td><strong>Memory Sink</strong></td>
<td>Append, Complete</td>
</tr>
</tbody>
</table>
</section>
<section id="how-to-output-a-stream-summary" class="level3">
<h3 class="anchored" data-anchor-id="how-to-output-a-stream-summary">6.3. üë®‚Äçüè´ How to output a stream : summary</h3>
<p>To sum up to output a stream you need</p>
<ul>
<li>DataFrame (because once load a stream is a DataFrame)</li>
<li>A format for your output, like console to print in console, memory to keep the Result Table in memory, json to write it to a file etc</li>
<li>A mode to specify how the Result Table will be updated.</li>
</ul>
<p>For instance for the memory sink</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>memory_sink <span class="op">=</span> df<span class="op">\</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>.writeStream<span class="op">\</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>.queryName(<span class="st">"my_awesome_name"</span>)<span class="op">\</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>.<span class="bu">format</span>(<span class="st">'memory'</span>)<span class="op">\</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>.outputMode(<span class="st">"complete"</span> <span class="kw">or</span> <span class="st">"append"</span>)<span class="op">\</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>.start() <span class="co">#needed to start the stream</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="https://github.com/HealerMikado/panorama_big_data_2021/blob/main/labs/lab%203%20-%20Spark%20Stream/img/stream%20processing.png?raw=true" class="img-fluid"></p>
<p><em>Figure 4 : The different phases of stream processing in Spark</em></p>
</section>
<section id="hand-on-2-output-a-stream" class="level3">
<h3 class="anchored" data-anchor-id="hand-on-2-output-a-stream">‚úçHand-on 2 : output a stream</h3>
<section id="lang-count" class="level4">
<h4 class="anchored" data-anchor-id="lang-count">üåè Lang count</h4>
<p><strong>Q5</strong> Compute a DataFrame that group and count data by the <code>lang</code> column. Name your DataFrame <code>lang_count</code></p>
<p><strong>Q6</strong>Use this DataFrame to create a output stream with the following configuration :</p>
<ul>
<li>Names the variable <code>lang_query</code></li>
<li>Memory sink</li>
<li>Complete mode (because we are doing an agregation)</li>
<li>Name you query <code>lang_count</code></li>
</ul>
<p><strong>Q7</strong> Then past this code</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>): <span class="co"># we use an _ because the variable isn't use. You can use i if you prefere</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    sleep(<span class="dv">3</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    spark.sql(<span class="st">"""</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="st">    SELECT * FROM lang_count"""</span>).show()</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>lang_query.stop() <span class="co">#needed to close the stream</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><p>After 30 seconds, 10 tables will appeared in your notebook. Each table represents the contain of <code>lang_count</code> at a certain time. The <code>.stop()</code> method close the stream.</p>
<p>In the rest of this tutorial, to will need two steps to print data :</p>
<ol type="1">
<li>Define a stream query with a memory sink</li>
<li>Request this stream with the <code>spark.sql()</code> function</li>
</ol>
<p>Instead of a for loop, you can just write you <code>spark.sql()</code> statement in a cell and rerun it. In this case you will need a third cell with a <code>stop()</code> method to close your stream.</p>
<p>For instance:</p></li>
<li><p>Cell 1</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>my_query <span class="op">=</span> my_df<span class="op">\</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    .writeStream<span class="op">\</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    .<span class="bu">format</span>(<span class="st">"memory"</span>)<span class="op">\</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    .queryName(<span class="st">"query_table"</span>)<span class="op">\</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    .start()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Cell 2</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>spark.sql(<span class="st">"SELECT * FROM query_table"</span>).show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Cell 3</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>my_query.stop()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul>
</section>
<section id="count-tweets-with-and-without-hashtag" class="level4">
<h4 class="anchored" data-anchor-id="count-tweets-with-and-without-hashtag">‚ùå Count tweets with and without hashtag</h4>
<p><strong>Q8</strong> Add a column <code>has_hashtag</code> to your DataFrame. This column equals True if <code>data.entities.hashtags</code> is not null. Else it‚Äôs false. Use the <code>withColumn</code> transformation to add a column. You can count the size of <code>data.entities.hashtags</code> to check if it‚Äôs empty or not.</p>
<p><strong>Q9</strong> Group and count by the <code>has_hashtag</code> column</p>
<p><strong>Q10</strong> Print some results</p>
</section>
</section>
<section id="debugging-tip" class="level3">
<h3 class="anchored" data-anchor-id="debugging-tip">Debugging tip</h3>
<p>If at any moment of this lab you encounter an error like this one :</p>
<pre><code>'Cannot start query with name has_hashtag as a query with that name is already active'
Traceback (most recent call last):
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py", line 1109, in start
    return self._sq(self._jwrite.start())
  File "/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 79, in deco
    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)
pyspark.sql.utils.IllegalArgumentException: 'Cannot start query with name has_hashtag as a query with that name is already active'</code></pre>
<p>Run in a cell the following code :</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> stream <span class="kw">in</span> spark.streams.active:</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    stream.stop()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><code>spark.streams.active</code> returns an array with all the active stream, and the code loops over all the active stream and closes them.</p>
</section>
</section>
<section id="stream-processing-basics" class="level2">
<h2 class="anchored" data-anchor-id="stream-processing-basics">7. ü•àStream processing basics</h2>
<section id="hand-on-3-transformations-on-stream" class="level3">
<h3 class="anchored" data-anchor-id="hand-on-3-transformations-on-stream">‚úçHand-on 3 : transformations on stream üßô‚Äç‚ôÇÔ∏è</h3>
<p><strong>Q11</strong> üî™Filter all records with missing / null value then count how many records you keep</p>
<ul>
<li>For this filter, you will use the <code>na.drop("any")</code> transformation. The <code>na.drop("any")</code> drop every line with a null value in at least one column. It‚Äôs simpler than using a <code>filter()</code> transformation because you don‚Äôt have to specify all the column. For more precise filter you can use <code>na.drop("any" or "all", subset=list of col)</code> (<code>all</code> will drop rows with only null value in all columns or in the specified list).</li>
<li>Use the SQL <code>COUNT(1)</code> function in the sql request to get the count</li>
<li>Because you don‚Äôt perform aggregation the <code>outputMode()</code> must be <code>append</code></li>
</ul>
<p>You will notice no record are dropped.</p>
<p><strong>Q12</strong> üïµÔ∏è‚Äç‚ôÇÔ∏è Drop all records with unverified (<code>includes.users.verified == True</code>)user then group the remaining records by <code>hashtag</code>.</p>
<ul>
<li><code>includes.users</code> is an array with only one element. You will need to extract it.</li>
<li><code>data.entities.hashtags</code> is an array too ! To group by tag (the hashtag content) you will need to explode it too.</li>
</ul>
<p><strong>Q13</strong>üîª Find ukraine related tweet (or any other topic like cat, dog, spring, batman, dogecoin etc) :</p>
<ul>
<li><p>Define a new column, name <code>ukraine_related</code>. This column is equal to <code>True</code> if <code>data.text</code> contains ‚Äúukraine‚Äù, else it‚Äôs equal to<code>False</code>.</p></li>
<li><p>Use the <code>withColumn()</code> transformation, and the <code>expr()</code> function to define the column. <code>expr()</code> takes as input an SQL expression. You do not need a full SQL statement (<code>SELECT ... FROM ... WHERE ...</code>) but just an SQL expression that return True or False if <code>data.text</code> contains ‚Äúukraine‚Äù. To help you :</p>
<ul>
<li><code>LOWER()</code> put in lower case a string</li>
<li><code>input_string LIKE wanted_string</code> return <code>True</code> if <code>input_string</code> is equal to <code>wanted_string</code></li>
<li>You can use <code>%</code> as wildcards</li>
</ul>
<p><a href="https://www.w3schools.com/sql/sql_like.asp">For more help</a></p></li>
<li><p>Only keep <code>data.text</code>, <code>data.lang</code>, <code>data.public_metrics</code> and <code>data.created_at</code></p></li>
</ul>
</section>
<section id="hand-on-4-aggregation-and-grouping-on-stream" class="level3">
<h3 class="anchored" data-anchor-id="hand-on-4-aggregation-and-grouping-on-stream">‚úçHand-on 4 : Aggregation and grouping on stream üß≤</h3>
<p><strong>Q14</strong> Count the number of different hashtag.</p>
<p><strong>Q15</strong> Group by hashtag and compute the average, min and max of <code>like_count</code></p>
<ul>
<li>Use the <code>groupBy()</code> and <code>agg()</code> transformations</li>
</ul>
<p><strong>Q16</strong> Compute the average of <code>like_count</code>, <code>retweet_count</code> and <code>quote_count</code> :</p>
<ul>
<li>across all <code>hashtag</code> and <code>lang</code></li>
<li>for each <code>lang</code> across all <code>hashtag</code></li>
<li>for each <code>hashtag</code> across all <code>lang</code></li>
<li>for each <code>hashtag</code> and each <code>lang</code></li>
</ul>
<p>To do so, replace the <code>groupBy()</code> transformation by the <code>cube()</code> one. <code>cube()</code> group compute all possible cross between dimensions passed as parameter. You will get something like this</p>
<table class="table">
<colgroup>
<col style="width: 11%">
<col style="width: 6%">
<col style="width: 25%">
<col style="width: 30%">
<col style="width: 26%">
</colgroup>
<thead>
<tr class="header">
<th>hashtag</th>
<th>lang</th>
<th>avg(like_count)</th>
<th>avg(retweet_count)</th>
<th>avg(quote_count)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>cat</td>
<td>null</td>
<td>1</td>
<td>2</td>
<td>3</td>
</tr>
<tr class="even">
<td>dog</td>
<td>null</td>
<td>4</td>
<td>5</td>
<td>6</td>
</tr>
<tr class="odd">
<td>‚Ä¶</td>
<td>‚Ä¶</td>
<td>‚Ä¶</td>
<td>‚Ä¶</td>
<td>‚Ä¶</td>
</tr>
<tr class="even">
<td>bird</td>
<td>fr</td>
<td>7</td>
<td>8</td>
<td>9</td>
</tr>
<tr class="odd">
<td>null</td>
<td>en</td>
<td>10</td>
<td>11</td>
<td>12</td>
</tr>
<tr class="even">
<td>null</td>
<td>null</td>
<td>13</td>
<td>14</td>
<td>15</td>
</tr>
</tbody>
</table>
<p>A <code>null</code> value mean this dimension wasn‚Äôt use for this row. For instance, the first row gives the averages when <code>hashtag==cat</code> independently of the <code>lang</code>. The before last row gives averages when <code>lang==en</code> independently of the <code>hashtag</code>. And the last row gives the averages for the full DataFrame.</p>
</section>
</section>
<section id="event-time-processing" class="level2">
<h2 class="anchored" data-anchor-id="event-time-processing">8. ü•á ‚åõEvent-time processing</h2>
<p>Event-time processing consists in processing information with <strong>respect to the time that it was created, not received</strong>. It‚Äôs a hot topic because sometime you will receive data in an order different from the creation order. For example, you are monitoring servers distributed across the globe. Your main datacentre is located in Paris. Something append in New York, and a few milliseconds after something append in Toulouse. Due to location, the event in Toulouse is likely to show up in your datacentre before the New York one. If you analyse data bases on the received time the order will be different than the event time. Computers and network are unreliable. Hence, when temporality is important, you must consider the creation time of the event and not it‚Äôs received time.</p>
<p>Hopefully, Spark will handle all this complexity for you ! If you have a timestamp column with the event creation spark can update data accordingly to the event time.</p>
<p>For instance is you process some data with a time window, Spark will update the result based on the event-time not the received time. So previous windows can be updated in the future.</p>
<p><img src="https://github.com/HealerMikado/panorama_big_data_2021/blob/main/labs/lab%203%20-%20Spark%20Stream/img/lata%20data%20handling%20without%20watermarks.png?raw=true" class="img-fluid"></p>
<p><em>Figure 5 : Time-event processing, event grouped by time windows</em></p>
<p>To work with time windows, Spark offers two type of windows</p>
<ul>
<li>Normal windows. You only consider event in a given windows. All windows are disjoint, and a event is only in one window.</li>
<li>Sliding windows. You have a fix window size (for example 1 hour) and a trigger time (for example 10 minute). Every 10 minute, you will process the data with an event time less than 1h.</li>
</ul>
<p><img src="https://github.com/HealerMikado/panorama_big_data_2021/blob/main/labs/lab%203%20-%20Spark%20Stream/img/time%20windows.png?raw=true" class="img-fluid"></p>
<p><em>Figure 6 : Time-event processing, event grouped by sliding time windows</em></p>
<p>To create time windows, you need :</p>
<ul>
<li><p>to define a time window : <code>window(column_with_time_event : str or col, your_time_window : str, timer_for_sliding_window) : str</code></p></li>
<li><p>grouping row by event-time using your window : <code>df.groupeBy(window(...))</code></p></li>
</ul>
<p>To produce the above processes :</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Need some import</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> window, col</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co"># word count + classic time window</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>df_with_event_time.groupBy(</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    window(df_with_event_time.event_time, <span class="st">"5 minutes"</span>),</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>    df_with_event_time.word).count()</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="co"># word count + sliding time window</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>df_with_event_time.groupBy(</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    window(df_with_event_time.event_time, <span class="st">"10 minutes"</span>, <span class="st">"5 minutes"</span>),</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    df_with_event_time.word).count()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="hand-on-5-event-time-processing" class="level3">
<h3 class="anchored" data-anchor-id="hand-on-5-event-time-processing">‚úçHand-on 5 : Event-time processing ‚åõ</h3>
<p>Count the number of event with a 10 seconds time window (use the <code>created_at</code> column)</p>
<p>Count the number of event by verified / unverified user with a 10 seconds time window (use the <code>Creation_Time</code> column)</p>
<p>Count the number of event with a 10 seconds time window sliding every 5 seconds</p>
</section>
<section id="handling-late-data-with-watermarks" class="level3">
<h3 class="anchored" data-anchor-id="handling-late-data-with-watermarks">8.1.üèÜ ‚è≥ ‚ÄãHandling late data with watermarks</h3>
<p>Processing accordingly to time-event is great, but currently there is one flaw. We never specified how late we expect to see data. This means, Spark will keep some data in memory forever. Because streams never end, Spark will keep in memory every time windows, to be able to update some previous results. But in some cases, you know that after some time, you don‚Äôt expect new data, or very late data aren‚Äôt relevant any more. In other words, after a certain amount of time you want to freeze old results.</p>
<p>Once again, Spark can handle such process, with watermarks.</p>
<p><img src="https://github.com/HealerMikado/panorama_big_data_2021/blob/main/labs/lab%203%20-%20Spark%20Stream/img/lata%20data%20handling%20with%20watermarks.png?raw=true" class="img-fluid"> <em>Figure 7 : Time-event processing with watermark</em></p>
<p>To do so, you have to define column as watermark and a the max delay. You have to use the <code>withWatermark(column, max_delay)</code> method.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Need some import</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> window, col</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co"># word count + classic time window</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>df_with_event_time.withWatermark(df_with_event_time.event_time, <span class="st">"4 minutes"</span>)<span class="op">\</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>.groupBy(</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>    window(df_with_event_time.event_time, <span class="st">"5 minutes"</span>),</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>    df_with_event_time.word).count()</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="co"># word count + sliding time window</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>df_with_event_time.withWatermark(df_with_event_time.event_time, <span class="st">"4 minutes"</span>)<span class="op">\</span></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>.groupBy(</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>    window(df_with_event_time.event_time, <span class="st">"10 minutes"</span>, <span class="st">"5 minutes"</span>),</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    df_with_event_time.word).count()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Be careful, the watermark field cannot be a nested field (<a href="https://www.waitingforcode.com/apache-spark-structured-streaming/nested-fields-dropduplicates-watermark-apache-spark-structured-streaming/read">link</a>)</p>
<section id="hand-on-6-handling-late-data-with-watermarks-hourglass_flowing_sand" class="level4">
<h4 class="anchored" data-anchor-id="hand-on-6-handling-late-data-with-watermarks-hourglass_flowing_sand">‚úçHand-on 6 : Handling late data with watermarks :hourglass_flowing_sand:</h4>
<p>Count the number of event with a 10 seconds time window (use the <code>created_at</code> column) with a 5 seconds watermark</p>
<p>Count the number of event by hashtag with a 30 seconds time window with a 1 minute watermark</p>
<p>Count the number of events post by verified user with a 10 seconds time window sliding every 5 seconds with 25 seconds watermark. Write the the result in a file sorted in S3.</p>
</section>
</section>
</section>
<section id="for-more-details" class="level2">
<h2 class="anchored" data-anchor-id="for-more-details">For more details</h2>
<ul>
<li><a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#input-sources">Spark official documentation</a></li>
<li>ZAHARIA, B. C. M. (2018). <em>Spark: the Definitive Guide</em>. , O‚ÄôReilly Media, Inc.&nbsp;https://proquest.safaribooksonline.com/9781491912201</li>
<li>https://databricks.com/blog/2018/03/13/introducing-stream-stream-joins-in-apache-spark-2-3.html</li>
<li>https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html</li>
<li>https://databricks.com/blog/2015/07/30/diving-into-apache-spark-streamings-execution-model.html</li>
</ul>
<p><strong>DO NOT FORGET TO TURN YOUR CLUSTER OFF A THE END OF THIS TUTORIAL!</strong></p>
<p><strong>DO NOT FORGET TO TURN YOUR CLUSTER OFF A THE END OF THIS TUTORIAL!</strong></p>
<p><strong>DO NOT FORGET TO TURN YOUR CLUSTER OFF A THE END OF THIS TUTORIAL!</strong></p>
<p><strong>DO NOT FORGET TO TURN YOUR CLUSTER OFF A THE END OF THIS TUTORIAL!</strong></p>
<p><strong>Explanation:</strong></p>
<ul>
<li><p><code>spark._jsc.hadoopConfiguration().set("fs.s3.useRequesterPaysHeader","true")</code> : likes in lab2, you will be charged for the data transfer. without this configuration you can‚Äôt access the data.</p></li>
<li><p><code>spark.conf.set("spark.sql.shuffle.partitions", 5)</code> : set the number of partitions for the shuffle phase. A partition is in Spark the name of a bloc of data. By default Spark use 200 partitions to shuffle data. But in this lab, our mini-batch will be small, and to many partitions will lead to performance issues.</p>
<pre><code>spark.conf.set("spark.sql.shuffle.partitions", 5)</code></pre>
<blockquote class="blockquote">
<p>ü§î The shuffle dispatches data according to their key between a <em>map</em> and a <em>reduce</em> phase. For instance, if you are counting how many records have each <code>g</code> group, the <em>map</em> phase involve counting each group member in each Spark partition : <code>{g1:5, g2:10, g4:1, g5:3}</code> for one partition, <code>{g1:1, g2:2, g3:23, g5:12}</code> for another. The <em>shuffle</em> phase dispatch those first results and group them by key in the same partition, one partition gets <code>{g1:5, g1:1, g2:10, g2:2}</code>, the other gets : <code>{g4:1, g5:3, g3:23, g5:12}</code> Then each <em>reduce</em> can be done efficiently.</p>
</blockquote></li>
<li><p>üì¶ Import all needed library</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> time <span class="im">import</span> sleep</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> from_json, window, col, expr</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.sql.types <span class="im">import</span> StructType,StructField, StringType, IntegerType, ArrayType, TimestampType, BooleanType, LongType, DoubleType</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul>
</section>
<section id="stream-processing-1" class="level2">
<h2 class="anchored" data-anchor-id="stream-processing-1">4.üöø Stream processing</h2>
<p>Stream processing is the act to process data in real-time. When a new record is available, it is processed. There is no real beginning nor end to the process, and there is no ‚Äúresult‚Äù. The result is updated in real time, hence multiple versions of the results exist. For instance, you want to count how many tweet about cat are posted in twitter every hour. Until the end of an hour, you do not have you final result. And even at this moment, your result can change. Maybe some technical problems created some latency and you will get some tweets later. And you will need to update your previous count.</p>
<p>Some commons use cases of stream processing are :</p>
<ul>
<li><strong>Notifications and alerting :</strong> real-time bank fraud detection ; electric grid monitoring with smart meters ; medical monitoring with smart meters, etc.</li>
<li><strong>Real time reporting:</strong> traffic in a website updated every minute; impact of a publicity campaign ; stock option portfolio, etc.</li>
<li><strong>Incremental ELT (extract transform load):</strong> new unstructured data are always available and they need to be processed (cleaned, filtered, put in a structured format) before their integration in the company IT system.</li>
<li><strong>Online machine learning :</strong> new data are always available and used by a ML algorithm to improve its performance dynamically.</li>
</ul>
<p>Unfortunately, stream processing has some issues. First because there is no end to the process, you cannot keep all the data in memory. Second, process a chain of event can be complex. How do you raise an alert when you receive the value 5, 6 and 3 consecutively ? Don‚Äôt forget you are in a distributed environment, and there is latency. Hence, the received order can be different from the emitted order.</p>
</section>
<section id="spark-and-stream-processing-shower" class="level2">
<h2 class="anchored" data-anchor-id="spark-and-stream-processing-shower">5. ‚ú® Spark and stream processing :shower:</h2>
<p>Stream processing was gradually incorporated in Spark. In 2012 Spark Streaming and it‚Äôs DStreams API was added to Spark (it was before an external project). This made it possible use high-level operator like <code>map</code> and <code>reduce</code> to process stream of data. Because of its implementation, this API has some limitations, and its syntax was different from the DataFrame one. Thus, in 2016 a new API was added, the Structured Streaming API. This API is directly build built on DataFrame, unlike DStreams. <strong>This has an advantage, you can process your stream like static data</strong>. Of course there are some limitations, but the core syntaxes is the same. You will chain transformations, because each transformation takes a DataFrame as input and produces a DataFrame as output. The big change is there is no action at the end, but an <strong><a href="#2.2%20How%20to%20output%20a%20stream%20?">output sink</a></strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://databricks.com/wp-content/uploads/2016/07/image01-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">data stream</figcaption>
</figure>
</div>
<p><em>Figure 1 : data stream representation (source <a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#basic-concepts">structured streaming programming guide</a>)</em></p>
<p>Spark offer two ways to process stream, one <strong>record at a time</strong>, or processing <strong>micro batching</strong> (processing a small amount of line at once).</p>
<ul>
<li><strong>one record at a time</strong> every time a new record is available it‚Äôs processed. This has a big advantage, it achieves <strong>very low latency </strong>. But there is a drawback, the system can not handle too much data at the same time (low throughput). It‚Äôs the default mode. Because in this lab, you will process files with record, even if you will process one file at a time, you will process mini batch of records</li>
<li>as for <strong>micro batching</strong> it process new records every <code>t</code> seconds. Hence records are not process really in ‚Äúreal-time‚Äù, but periodically, <strong>the latency will be higher, and so the throughput</strong>. Unless you really need low latency, make it you first choice option.</li>
</ul>
<blockquote class="blockquote">
<p>üßê To get the best ratio latency/throughput, a good practice is to decrease the micro-batch size until the mini-batch throughput is the same as the input throughput. Then increase the size to have some margin</p>
</blockquote>
<p><img src="https://github.com/HealerMikado/panorama_big_data_2021/blob/main/labs/lab%203%20-%20Spark%20Stream/img/spart_latency_requirement.png?raw=true" class="img-fluid"></p>
<p><em>Figure 2 : which Spark solution suit best giving latency requirement (source : <a href="https://pages.databricks.com/rs/094-YMS-629/images/LearningSpark2.0.pdf">Learning Spark, O‚ÄôReilly</a>)</em></p>
<blockquote class="blockquote">
<p>To understand why processing one record at a time has lower latency and throughput than batch processing, imagine a restaurant. Every time a client order something the chef cooks its order independently of the other current orders. So if two clients order pizza, the chief makes two small doughs, and cook them individually. If clients there is only a few clients, the chief can finish each order before a new client comes. The latency is the lowest possible when the chief is idle when a client come. Know imagine a restaurant were the chief process the orders by batch. He waits some minutes to gather all the orders than he mutualizes the cooking. If there are 5 pizza orders, he only does one big dough, divides it in five, add the toppings then cook all five at once. The latency is higher because the chief waits before cooking, but so the throughput because he can cook multiple things at once.</p>
</blockquote>
</section>
<section id="the-basics-of-sparks-structured-streaming-1" class="level2">
<h2 class="anchored" data-anchor-id="the-basics-of-sparks-structured-streaming-1">6. ü•â The basics of Spark‚Äôs Structured Streaming</h2>
<section id="the-different-sources-for-stream-processing-in-spark-1" class="level3">
<h3 class="anchored" data-anchor-id="the-different-sources-for-stream-processing-in-spark-1">6.1. üìö The different sources for stream processing in Spark</h3>
<p>In lab 2 you discovered Spark DataFrame, in this lab you will learn about <a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html">Structured Streaming</a>. It‚Äôs a stream processing framework built on the Spark SQL engine, and it uses the existing structured APIs in Spark. So one you define a way to read a stream, you will get a DataFrame. Like in lab2 ! <strong>So except state otherwise, all transformations presented in lab2 are still relevant in this lab</strong>.</p>
<p>Spark Streaming supports several input source for reading in a streaming fashion :</p>
<ul>
<li><a href="https://kafka.apache.org/">Apache Kafka</a> an open-source distributed event streaming platform (not show in this lab)</li>
<li>Files on distributed file system like HDFS or S3 (Spark will continuously read new files in a directory)</li>
<li>A network socket : an end-point in a communication across a network (sort of very simple webservice). It‚Äôs not recommend for <em>production</em> application, because a socket connection doesn‚Äôt provide any mechanism to check the consistency of data.</li>
</ul>
<p>Defining an input source is like loading a DataFrame but, you have to replace <code>spark.read</code> by <code>spark.readStream</code>. For instance, if I want to open a stream to a folder located in S3 you have to read every new files put in it, just write</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>my_first_stream <span class="op">=</span> spark<span class="op">\</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>.readStream<span class="op">\</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>.schema(schema_tweet)<span class="op">\</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>.json(<span class="st">"s3://my-awesome-bucket/my-awesome-folder"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The major difference with lab2, it is Spark cannot infer the schema of the stream. You have to pass it to Spark. There is two ways :</p>
<ul>
<li>A reliable way : you define the schema by yourself and gave it to Spark</li>
<li>A quick way : you load one file of the folder in a DataFrame, extract the schema and use it. It works, but the schema can be incomplete. It‚Äôs a better solution to create the schema by hand and use it.</li>
</ul>
<p>For Apache Kafka, or socket , it‚Äôs a slightly more complex, <em>(not used today, it‚Äôs jute for you personal knowledge)</em> :</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>my_first_stream <span class="op">=</span> spark<span class="op">\</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>.readStream<span class="op">\</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>.<span class="bu">format</span>(<span class="st">"kafka"</span>)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>.option(<span class="st">"kafka.bootstrat.servers"</span>, <span class="st">"host1:port1, host2:port2 etc"</span>)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>.option(<span class="st">"subscribePattern"</span>, <span class="st">"topic name"</span>)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>.load()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="why-is-a-folder-a-relevant-source-in-stream-processing-1" class="level4">
<h4 class="anchored" data-anchor-id="why-is-a-folder-a-relevant-source-in-stream-processing-1">6.1.1 ü§®Why is a folder a relevant source in stream processing ?</h4>
<p>Previously, in lab 1, you loaded all the files in a folder stored in MinIO with Spark. Powered by Kubernetes, MinIO delivers scalable, secure, S3 compatible object storage to every public cloud. And it worked pretty well. But this folder was static, in other words, Its content didn‚Äôt change. But in some cases, new data are constantly written into a folder. For instance, in this lab you will process a stream of tweets. A python script is running in a VS Code service reading tweets from the Twitter‚Äôs web service and writing them in a S3 buckets. Every 2 seconds or so, a new file is added to the bucket with 1000 tweets. If you use DataFrame like in lab 1, your process cannot proceed those new files. You should relaunch your process every time. But with Structured Streaming Spark will dynamically load new files.</p>
<p><img src="stream_pipeline_twitter.drawio.png" class="img-fluid"></p>
<p><em>Figure 3 : Complete lab architecture to stream process tweets</em></p>
<blockquote class="blockquote">
<p>The remaining question is, why don‚Äôt we connect Spark to the twitter webservice directly ? And the answer is : we can‚Äôt. Spark cannot be connected to a webservice directly. You need a middle-man between Spark and a webservice. There are multiple solutions, but an easy and reliable one is to write tweet to MinIO, the open implementation of s3 (because we use Onyxia, if you use Microsoft Azure, Google Cloud Platform or OVH cloud replace S3 by their storage service).</p>
</blockquote>
</section>
</section>
<section id="hand-on-1-open-a-stream-1" class="level3">
<h3 class="anchored" data-anchor-id="hand-on-1-open-a-stream-1">‚úçHand-on 1 : open a stream</h3>
<p>Like in lab 1, you will use tweets in this lab. The tweets are stored in jsonl file (<em>json line</em> every line of the file is a complete json). Here is an example. The schema changed a little, because this time tweets aren‚Äôt pre-processed.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"data"</span><span class="op">:</span> {</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>        <span class="st">"public_metrics"</span><span class="op">:</span> {</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>            <span class="st">"retweet_count"</span><span class="op">:</span> <span class="dv">0</span><span class="op">,</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>            <span class="st">"reply_count"</span><span class="op">:</span> <span class="dv">0</span><span class="op">,</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>            <span class="st">"like_count"</span><span class="op">:</span> <span class="dv">0</span><span class="op">,</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>            <span class="st">"quote_count"</span><span class="op">:</span> <span class="dv">0</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>        }<span class="op">,</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>        <span class="st">"text"</span><span class="op">:</span> <span class="st">"Day 93. Tweeting every day until Colby cheez its come back #bringcolbyback @cheezit"</span><span class="op">,</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>        <span class="st">"possibly_sensitive"</span><span class="op">:</span> <span class="kw">false</span><span class="op">,</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">"created_at"</span><span class="op">:</span> <span class="st">"2021-05-03T07:55:46.000Z"</span><span class="op">,</span></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>        <span class="st">"id"</span><span class="op">:</span> <span class="st">"1389126523853148162"</span><span class="op">,</span></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>        <span class="st">"entities"</span><span class="op">:</span> {</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>            <span class="st">"annotations"</span><span class="op">:</span> [</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>                {</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"start"</span><span class="op">:</span> <span class="dv">33</span><span class="op">,</span></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"end"</span><span class="op">:</span> <span class="dv">43</span><span class="op">,</span></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"probability"</span><span class="op">:</span> <span class="fl">0.5895</span><span class="op">,</span></span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"type"</span><span class="op">:</span> <span class="st">"Person"</span><span class="op">,</span></span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"normalized_text"</span><span class="op">:</span> <span class="st">"Colby cheez"</span></span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>                }</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>            ]<span class="op">,</span></span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>            <span class="st">"mentions"</span><span class="op">:</span> [</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>                {</span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"start"</span><span class="op">:</span> <span class="dv">75</span><span class="op">,</span></span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"end"</span><span class="op">:</span> <span class="dv">83</span><span class="op">,</span></span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"username"</span><span class="op">:</span> <span class="st">"cheezit"</span></span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a>                }</span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a>            ]<span class="op">,</span></span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a>            <span class="st">"hashtags"</span><span class="op">:</span> [</span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a>                {</span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"start"</span><span class="op">:</span> <span class="dv">59</span><span class="op">,</span></span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"end"</span><span class="op">:</span> <span class="dv">74</span><span class="op">,</span></span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"tag"</span><span class="op">:</span> <span class="st">"bringcolbyback"</span></span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a>                }</span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a>        }<span class="op">,</span></span>
<span id="cb27-38"><a href="#cb27-38" aria-hidden="true" tabindex="-1"></a>        <span class="st">"lang"</span><span class="op">:</span> <span class="st">"en"</span><span class="op">,</span></span>
<span id="cb27-39"><a href="#cb27-39" aria-hidden="true" tabindex="-1"></a>        <span class="st">"source"</span><span class="op">:</span> <span class="st">"Twitter for iPhone"</span><span class="op">,</span></span>
<span id="cb27-40"><a href="#cb27-40" aria-hidden="true" tabindex="-1"></a>        <span class="st">"author_id"</span><span class="op">:</span> <span class="st">"606856313"</span></span>
<span id="cb27-41"><a href="#cb27-41" aria-hidden="true" tabindex="-1"></a>    }<span class="op">,</span></span>
<span id="cb27-42"><a href="#cb27-42" aria-hidden="true" tabindex="-1"></a>    <span class="st">"includes"</span><span class="op">:</span> {</span>
<span id="cb27-43"><a href="#cb27-43" aria-hidden="true" tabindex="-1"></a>        <span class="st">"users"</span><span class="op">:</span> [</span>
<span id="cb27-44"><a href="#cb27-44" aria-hidden="true" tabindex="-1"></a>            {</span>
<span id="cb27-45"><a href="#cb27-45" aria-hidden="true" tabindex="-1"></a>                <span class="st">"created_at"</span><span class="op">:</span> <span class="st">"2012-06-13T03:36:00.000Z"</span><span class="op">,</span></span>
<span id="cb27-46"><a href="#cb27-46" aria-hidden="true" tabindex="-1"></a>                <span class="st">"username"</span><span class="op">:</span> <span class="st">"DivinedHavoc"</span><span class="op">,</span></span>
<span id="cb27-47"><a href="#cb27-47" aria-hidden="true" tabindex="-1"></a>                <span class="st">"verified"</span><span class="op">:</span> <span class="kw">false</span><span class="op">,</span></span>
<span id="cb27-48"><a href="#cb27-48" aria-hidden="true" tabindex="-1"></a>                <span class="st">"name"</span><span class="op">:</span> <span class="st">"Justin"</span><span class="op">,</span></span>
<span id="cb27-49"><a href="#cb27-49" aria-hidden="true" tabindex="-1"></a>                <span class="st">"id"</span><span class="op">:</span> <span class="st">"606856313"</span></span>
<span id="cb27-50"><a href="#cb27-50" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb27-51"><a href="#cb27-51" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb27-52"><a href="#cb27-52" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb27-53"><a href="#cb27-53" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Q1</strong> Define a variable with this schema (you will find a file <em>schema pyspark tweet</em> on moodle with the schema to copy /aste)</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.sql.types <span class="im">import</span> StructType,StructField, StringType, IntegerType, ArrayType, TimestampType, BooleanType, LongType, DoubleType</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>StructType([</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>  StructField(<span class="st">"data"</span>, StructType([</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>      StructField(<span class="st">"author_id"</span>,StringType(),<span class="va">True</span>),</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>      StructField(<span class="st">"text"</span>,StringType(),<span class="va">True</span>),</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>      StructField(<span class="st">"source"</span>,StringType(),<span class="va">True</span>),</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>      StructField(<span class="st">"lang"</span>,StringType(),<span class="va">True</span>),</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>      StructField(<span class="st">"created_at"</span>,TimestampType(),<span class="va">True</span>),</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>      StructField(<span class="st">"entities"</span>,StructType([</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>          StructField(<span class="st">"annotations"</span>, ArrayType(StructType([</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"end"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"normalized_text"</span>, StringType(), <span class="va">True</span>),</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"probability"</span>, DoubleType(), <span class="va">True</span>),</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"start"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"type"</span>, StringType(), <span class="va">True</span>)</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>          ]),<span class="va">True</span>),<span class="va">True</span>),</span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>          StructField(<span class="st">"cashtags"</span>, ArrayType(StructType([</span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"end"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"start"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"tag"</span>, StringType(), <span class="va">True</span>)</span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a>          ]),<span class="va">True</span>),<span class="va">True</span>),</span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a>           StructField(<span class="st">"hashtags"</span>, ArrayType(StructType([</span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"end"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"start"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"tag"</span>, StringType(), <span class="va">True</span>)</span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a>          ]),<span class="va">True</span>),<span class="va">True</span>),</span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a>          StructField(<span class="st">"mentions"</span>, ArrayType(StructType([</span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"end"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"start"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"username"</span>, StringType(), <span class="va">True</span>)</span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a>          ]),<span class="va">True</span>),<span class="va">True</span>),</span>
<span id="cb28-33"><a href="#cb28-33" aria-hidden="true" tabindex="-1"></a>          StructField(<span class="st">"urls"</span>, ArrayType(StructType([</span>
<span id="cb28-34"><a href="#cb28-34" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"description"</span>, StringType(), <span class="va">True</span>),</span>
<span id="cb28-35"><a href="#cb28-35" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"display_url"</span>, StringType(), <span class="va">True</span>),</span>
<span id="cb28-36"><a href="#cb28-36" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"end"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb28-37"><a href="#cb28-37" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"expanded_url"</span>, StringType(), <span class="va">True</span>),</span>
<span id="cb28-38"><a href="#cb28-38" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"images"</span>, ArrayType(StructType([</span>
<span id="cb28-39"><a href="#cb28-39" aria-hidden="true" tabindex="-1"></a>                      StructField(<span class="st">"height"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb28-40"><a href="#cb28-40" aria-hidden="true" tabindex="-1"></a>                      StructField(<span class="st">"url"</span>, StringType(), <span class="va">True</span>),</span>
<span id="cb28-41"><a href="#cb28-41" aria-hidden="true" tabindex="-1"></a>                      StructField(<span class="st">"width"</span>, LongType(), <span class="va">True</span>)</span>
<span id="cb28-42"><a href="#cb28-42" aria-hidden="true" tabindex="-1"></a>                  ]),<span class="va">True</span>),<span class="va">True</span>),</span>
<span id="cb28-43"><a href="#cb28-43" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"start"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb28-44"><a href="#cb28-44" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"status"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb28-45"><a href="#cb28-45" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"title"</span>, StringType(), <span class="va">True</span>),</span>
<span id="cb28-46"><a href="#cb28-46" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"unwound_url"</span>, StringType(), <span class="va">True</span>),</span>
<span id="cb28-47"><a href="#cb28-47" aria-hidden="true" tabindex="-1"></a>              StructField(<span class="st">"url"</span>, StringType(), <span class="va">True</span>),</span>
<span id="cb28-48"><a href="#cb28-48" aria-hidden="true" tabindex="-1"></a>          ]),<span class="va">True</span>),<span class="va">True</span>),</span>
<span id="cb28-49"><a href="#cb28-49" aria-hidden="true" tabindex="-1"></a>      ]),<span class="va">True</span>),</span>
<span id="cb28-50"><a href="#cb28-50" aria-hidden="true" tabindex="-1"></a>      StructField(<span class="st">"public_metrics"</span>, StructType([</span>
<span id="cb28-51"><a href="#cb28-51" aria-hidden="true" tabindex="-1"></a>          StructField(<span class="st">"like_count"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb28-52"><a href="#cb28-52" aria-hidden="true" tabindex="-1"></a>          StructField(<span class="st">"reply_count"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb28-53"><a href="#cb28-53" aria-hidden="true" tabindex="-1"></a>          StructField(<span class="st">"retweet_count"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb28-54"><a href="#cb28-54" aria-hidden="true" tabindex="-1"></a>          StructField(<span class="st">"quote_count"</span>, LongType(), <span class="va">True</span>),</span>
<span id="cb28-55"><a href="#cb28-55" aria-hidden="true" tabindex="-1"></a>      ]),<span class="va">True</span>)</span>
<span id="cb28-56"><a href="#cb28-56" aria-hidden="true" tabindex="-1"></a>    ]),<span class="va">True</span>),</span>
<span id="cb28-57"><a href="#cb28-57" aria-hidden="true" tabindex="-1"></a>  StructField(<span class="st">"includes"</span>, StructType([</span>
<span id="cb28-58"><a href="#cb28-58" aria-hidden="true" tabindex="-1"></a>      StructField(<span class="st">"users"</span>, ArrayType(StructType([</span>
<span id="cb28-59"><a href="#cb28-59" aria-hidden="true" tabindex="-1"></a>          StructField(<span class="st">"created_at"</span>, TimestampType(), <span class="va">True</span>),</span>
<span id="cb28-60"><a href="#cb28-60" aria-hidden="true" tabindex="-1"></a>          StructField(<span class="st">"id"</span>, StringType(), <span class="va">True</span>),</span>
<span id="cb28-61"><a href="#cb28-61" aria-hidden="true" tabindex="-1"></a>          StructField(<span class="st">"name"</span>, StringType(), <span class="va">True</span>),</span>
<span id="cb28-62"><a href="#cb28-62" aria-hidden="true" tabindex="-1"></a>          StructField(<span class="st">"username"</span>, StringType(), <span class="va">True</span>),</span>
<span id="cb28-63"><a href="#cb28-63" aria-hidden="true" tabindex="-1"></a>          StructField(<span class="st">"verified"</span>, BooleanType(), <span class="va">True</span>)</span>
<span id="cb28-64"><a href="#cb28-64" aria-hidden="true" tabindex="-1"></a>      ]),<span class="va">True</span>),<span class="va">True</span>)</span>
<span id="cb28-65"><a href="#cb28-65" aria-hidden="true" tabindex="-1"></a>  ]),<span class="va">True</span>)</span>
<span id="cb28-66"><a href="#cb28-66" aria-hidden="true" tabindex="-1"></a>  ])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Q2</strong> Create a stream to this s3 bucket : <code>s3a:/s3a://remipepin/diffusion/ensai/stream_twee/remipepin/diffusion/ensai/stream_tweet</code>. Name it <code>tweet_stream</code>. Use the option <code>option("maxFilePerTrigger", "1")</code> to process each new files one by one.</p>
<blockquote class="blockquote">
<p>ü§î Nothing happen ? It‚Äôs normal ! Do not forget, Spark use lazy evaluation. It doesn‚Äôt use data if you don‚Äôt define an action. For now Spark only know how to get the stream, that‚Äôs all.</p>
</blockquote>
<div class="cell" data-tags="[]" data-vscode="{&quot;languageId&quot;:&quot;json&quot;}">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>tweet_stream <span class="op">=</span> spark<span class="op">\</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>.readStream<span class="op">\</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>.schema(schema_tweet)<span class="op">\</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>.option(<span class="st">"maxFilePerTrigger"</span>, <span class="st">"1"</span>)<span class="op">\</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>.json(<span class="st">"s3a://remipepin/diffusion/ensai/stream_tweet"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Q3</strong> In a cell just execute <code>tweet_stream</code>. It should print the type of <code>tweet_stream</code> and the associated schema. You can see you created a DataFrame like in lab2 !</p>
<p><strong>Q4</strong> Print the size of your DataFrame by using this piece of code :</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>stream_size_query<span class="op">=</span> tweet_stream<span class="op">\</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>.writeStream<span class="op">\</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>.queryName(<span class="st">"stream_size"</span>)<span class="op">\</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>.<span class="bu">format</span>(<span class="st">"memory"</span>)<span class="op">\</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>.start()</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>): <span class="co"># we use an _ because the variable isn't used. You can use i if you prefere</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>    sleep(<span class="dv">3</span>)</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>    spark.sql(<span class="st">"""</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a><span class="st">      SELECT count(1) FROM stream_size</span></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a><span class="st">    """</span>).show()</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>stream_size_query.stop() <span class="co">#needed to close the query</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="section-1" class="level3">
<h3 class="anchored" data-anchor-id="section-1"></h3>
</section>
<section id="how-to-output-a-stream-1" class="level3">
<h3 class="anchored" data-anchor-id="how-to-output-a-stream-1">6.2. üõí How to output a stream ?</h3>
<p>Remember, Spark has two types of methods to process DataFrame:</p>
<ul>
<li>Transformations which take a DataFrame has input and produce an other Dataframe</li>
<li>And actions, which effectively run computation and produce something, like a file, or a output in you notebook/console.</li>
</ul>
<p>Stream processing looks the same as DataFrame processing. Hence, <strong>you still have transformations</strong>, the exact same one that can be apply on classic DataFrame (with some restriction, for example you can not sample a stream with the <code>sample()</code> transformation). The action part is a little different. Because a stream runs continuously, you cannot just print the data or run a count at the end of the process. <strong>In fact actions will nor work on stream</strong>. To tackle this issue, Spark proposes different <a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks">outputs sinks</a>. An output sink is a possible output for your stream. The different output sink are (this part came from the official Spark <a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks">documentation)</a> :</p>
<ul>
<li><strong>File sink</strong> - Stores the output to a file. The file can be stored locally (on the cluster), remotely (on S3). The file format can be json, csv etc</li>
</ul>
<div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>writeStream<span class="op">\</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>.<span class="bu">format</span>(<span class="st">'json'</span>)<span class="op">\</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>.option(<span class="st">"checkpointLocation"</span>, <span class="st">"output_folder/history"</span>) \ </span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>.option(<span class="st">"path"</span>, <span class="st">"output_folder"</span>)<span class="op">\</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>.start()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><p><strong>Kafka sink</strong> - Stores the output to one or more topics in Kafka.</p></li>
<li><p><strong>Foreach sink</strong> - Runs arbitrary computation on the records in the output. It does not produce an DataFrame. Each processed lines lost</p></li>
</ul>
<div class="sourceCode" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>writeStream</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>    .foreach(...)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>    .start()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>Console sink (for debugging)</strong> - Prints the output to the console standard output (<em>stdout</em>) every time there is a trigger. This should be used for debugging purposes on low data volumes as the entire output is collected and stored in the driver‚Äôs memory after every trigger. <em>Sadly console sink does not work with jupyter notebook</em>.</li>
</ul>
<div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>writeStream</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    .<span class="bu">format</span>(<span class="st">"console"</span>)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>    .start()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>Memory sink (for debugging)</strong> - The output is stored in memory as an in-memory table. This should be used for debugging purposes on low data volumes as the entire output is collected and stored in the driver‚Äôs memory. Hence, use it with caution. Because we are in a simple lab, you will use this solution. But keep in mind it‚Äôs a very bad idea because data must fit in the the ram of the driver node. And in a big data context it‚Äôs impossible. Because it‚Äôs not a big data problem if one computer can tackle it.</li>
</ul>
<div class="sourceCode" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>writeStream</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    .<span class="bu">format</span>(<span class="st">"memory"</span>)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>    .queryName(<span class="st">"tableName"</span>) <span class="co"># to resquest the table with spark.sql()</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>    .start()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We just talked where we can output a stream, but there is another question, how ?</p>
<p>To understand why it‚Äôs a issue, let‚Äôs talk about two things that spark can do with streams : filter data and group by + aggregation</p>
<ul>
<li><strong>Filter</strong> : your process is really simple. Every time you get a new data you just compute a score and drop records with a score less than a threshold. Then you write into a file every kept record. In a nutshell, you just append new data to a file. Spark does not have to read an already written row, it just add new data.</li>
<li><strong>Group by + aggregation</strong> : in this case you want to group by your data by key than compute a simple count. Then you want to write the result in a file. But now there is an issue, Spark needs to update some existing rows in your file every time it writes somethings. But is your file is stored in HDFS of S3, it‚Äôs impossible to update in a none append way a file. In a nutshell, it‚Äôs impossible to output in a file your operation.</li>
</ul>
<p>To deal with this issue, Spark proposes 3 mode. <strong>And you cannot use every mode with every output sink, with every transformation</strong>. The 3 modes are (<a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#starting-streaming-queries">more info here</a>) :</p>
<ul>
<li><strong>Append mode (default)</strong> - This is the default mode, where only the new rows added to the Result Table since the last trigger will be outputted to the sink. This is supported for only those queries where rows added to the Result Table is never going to change. Hence, this mode guarantees that each row will be output only once (assuming fault-tolerant sink). For example, queries with only <code>select</code>, <code>where</code>, <code>map</code>, <code>flatMap</code>, <code>filter</code>, <code>join</code>, etc. will support Append mode.</li>
<li><strong>Complete mode</strong> - The whole Result Table will be outputted to the sink after every trigger. This is supported for aggregation queries.</li>
<li><strong>Update mode</strong> - (<em>Available since Spark 2.1.1</em>) Only the rows in the Result Table that were updated since the last trigger will be outputted to the sink. More information to be added in future releases.</li>
</ul>
<table class="table">
<thead>
<tr class="header">
<th>Sink</th>
<th>Supported Output Modes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>File Sink</strong></td>
<td>Append</td>
</tr>
<tr class="even">
<td><strong>Kafka Sink</strong></td>
<td>Append, Update, Complete</td>
</tr>
<tr class="odd">
<td><strong>Foreach Sink</strong></td>
<td>Append, Update, Complete</td>
</tr>
<tr class="even">
<td><strong>ForeachBatch Sink</strong></td>
<td>Append, Update, Complete</td>
</tr>
<tr class="odd">
<td><strong>Console Sink</strong></td>
<td>Append, Update, Complete</td>
</tr>
<tr class="even">
<td><strong>Memory Sink</strong></td>
<td>Append, Complete</td>
</tr>
</tbody>
</table>
</section>
<section id="how-to-output-a-stream-summary-1" class="level3">
<h3 class="anchored" data-anchor-id="how-to-output-a-stream-summary-1">6.3. üë®‚Äçüè´ How to output a stream : summary</h3>
<p>To sum up to output a stream you need</p>
<ul>
<li>DataFrame (because once load a stream is a DataFrame)</li>
<li>A format for your output, like console to print in console, memory to keep the Result Table in memory, json to write it to a file etc</li>
<li>A mode to specify how the Result Table will be updated.</li>
</ul>
<p>For instance for the memory sink</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>memory_sink <span class="op">=</span> df<span class="op">\</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>.writeStream<span class="op">\</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>.queryName(<span class="st">"my_awesome_name"</span>)<span class="op">\</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>.<span class="bu">format</span>(<span class="st">'memory'</span>)<span class="op">\</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>.outputMode(<span class="st">"complete"</span> <span class="kw">or</span> <span class="st">"append"</span>)<span class="op">\</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>.start() <span class="co">#needed to start the stream</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="https://github.com/HealerMikado/panorama_big_data_2021/blob/main/labs/lab%203%20-%20Spark%20Stream/img/stream%20processing.png?raw=true" class="img-fluid"></p>
<p><em>Figure 4 : The different phases of stream processing in Spark</em></p>
</section>
<section id="hand-on-2-output-a-stream-1" class="level3">
<h3 class="anchored" data-anchor-id="hand-on-2-output-a-stream-1">‚úçHand-on 2 : output a stream</h3>
<section id="lang-count-1" class="level4">
<h4 class="anchored" data-anchor-id="lang-count-1">üåè Lang count</h4>
<p><strong>Q5</strong> Compute a DataFrame that group and count data by the <code>lang</code> column. Name your DataFrame <code>lang_count</code></p>
<p><strong>Q6</strong>Use this DataFrame to create a output stream with the following configuration :</p>
<ul>
<li>Names the variable <code>lang_query</code></li>
<li>Memory sink</li>
<li>Complete mode (because we are doing an agregation)</li>
<li>Name you query <code>lang_count</code></li>
</ul>
<p><strong>Q7</strong> Then past this code</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>): <span class="co"># we use an _ because the variable isn't use. You can use i if you prefere</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    sleep(<span class="dv">3</span>)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>    spark.sql(<span class="st">"""</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="st">    SELECT * FROM lang_count"""</span>).show()</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>lang_query.stop() <span class="co">#needed to close the stream</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><p>After 30 seconds, 10 tables will appeared in your notebook. Each table represents the contain of <code>lang_count</code> at a certain time. The <code>.stop()</code> method close the stream.</p>
<p>In the rest of this tutorial, to will need two steps to print data :</p>
<ol type="1">
<li>Define a stream query with a memory sink</li>
<li>Request this stream with the <code>spark.sql()</code> function</li>
</ol>
<p>Instead of a for loop, you can just write you <code>spark.sql()</code> statement in a cell and rerun it. In this case you will need a third cell with a <code>stop()</code> method to close your stream.</p>
<p>For instance:</p></li>
<li><p>Cell 1</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>my_query <span class="op">=</span> my_df<span class="op">\</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>    .writeStream<span class="op">\</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>    .<span class="bu">format</span>(<span class="st">"memory"</span>)<span class="op">\</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>    .queryName(<span class="st">"query_table"</span>)<span class="op">\</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>    .start()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Cell 2</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>spark.sql(<span class="st">"SELECT * FROM query_table"</span>).show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Cell 3</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>my_query.stop()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul>
</section>
<section id="count-tweets-with-and-without-hashtag-1" class="level4">
<h4 class="anchored" data-anchor-id="count-tweets-with-and-without-hashtag-1">‚ùå Count tweets with and without hashtag</h4>
<p><strong>Q8</strong> Add a column <code>has_hashtag</code> to your DataFrame. This column equals True if <code>data.entities.hashtags</code> is not null. Else it‚Äôs false. Use the <code>withColumn</code> transformation to add a column. You can count the size of <code>data.entities.hashtags</code> to check if it‚Äôs empty or not.</p>
<p><strong>Q9</strong> Group and count by the <code>has_hashtag</code> column</p>
<p><strong>Q10</strong> Print some results</p>
</section>
</section>
<section id="debugging-tip-1" class="level3">
<h3 class="anchored" data-anchor-id="debugging-tip-1">Debugging tip</h3>
<p>If at any moment of this lab you encounter an error like this one :</p>
<pre><code>'Cannot start query with name has_hashtag as a query with that name is already active'
Traceback (most recent call last):
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py", line 1109, in start
    return self._sq(self._jwrite.start())
  File "/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 79, in deco
    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)
pyspark.sql.utils.IllegalArgumentException: 'Cannot start query with name has_hashtag as a query with that name is already active'</code></pre>
<p>Run in a cell the following code :</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> stream <span class="kw">in</span> spark.streams.active:</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    stream.stop()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><code>spark.streams.active</code> returns an array with all the active stream, and the code loops over all the active stream and closes them.</p>
</section>
</section>
<section id="stream-processing-basics-1" class="level2">
<h2 class="anchored" data-anchor-id="stream-processing-basics-1">7. ü•àStream processing basics</h2>
<section id="hand-on-3-transformations-on-stream-1" class="level3">
<h3 class="anchored" data-anchor-id="hand-on-3-transformations-on-stream-1">‚úçHand-on 3 : transformations on stream üßô‚Äç‚ôÇÔ∏è</h3>
<p><strong>Q11</strong> üî™Filter all records with missing / null value then count how many records you keep</p>
<ul>
<li>For this filter, you will use the <code>na.drop("any")</code> transformation. The <code>na.drop("any")</code> drop every line with a null value in at least one column. It‚Äôs simpler than using a <code>filter()</code> transformation because you don‚Äôt have to specify all the column. For more precise filter you can use <code>na.drop("any" or "all", subset=list of col)</code> (<code>all</code> will drop rows with only null value in all columns or in the specified list).</li>
<li>Use the SQL <code>COUNT(1)</code> function in the sql request to get the count</li>
<li>Because you don‚Äôt perform aggregation the <code>outputMode()</code> must be <code>append</code></li>
</ul>
<p>You will notice no record are dropped.</p>
<p><strong>Q12</strong> üïµÔ∏è‚Äç‚ôÇÔ∏è Drop all records with unverified (<code>includes.users.verified == True</code>)user then group the remaining records by <code>hashtag</code>.</p>
<ul>
<li><code>includes.users</code> is an array with only one element. You will need to extract it.</li>
<li><code>data.entities.hashtags</code> is an array too ! To group by tag (the hashtag content) you will need to explode it too.</li>
</ul>
<div class="cell" data-tags="[]" data-vscode="{&quot;languageId&quot;:&quot;json&quot;}">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Q13</strong>üîª Find ukraine related tweet (or any other topic like cat, dog, spring, batman, dogecoin etc) :</p>
<ul>
<li><p>Define a new column, name <code>ukraine_related</code>. This column is equal to <code>True</code> if <code>data.text</code> contains ‚Äúukraine‚Äù, else it‚Äôs equal to<code>False</code>.</p></li>
<li><p>Use the <code>withColumn()</code> transformation, and the <code>expr()</code> function to define the column. <code>expr()</code> takes as input an SQL expression. You do not need a full SQL statement (<code>SELECT ... FROM ... WHERE ...</code>) but just an SQL expression that return True or False if <code>data.text</code> contains ‚Äúukraine‚Äù. To help you :</p>
<ul>
<li><code>LOWER()</code> put in lower case a string</li>
<li><code>input_string LIKE wanted_string</code> return <code>True</code> if <code>input_string</code> is equal to <code>wanted_string</code></li>
<li>You can use <code>%</code> as wildcards</li>
</ul>
<p><a href="https://www.w3schools.com/sql/sql_like.asp">For more help</a></p></li>
<li><p>Only keep <code>data.text</code>, <code>data.lang</code>, <code>data.public_metrics</code> and <code>data.created_at</code></p></li>
</ul>
</section>
<section id="hand-on-4-aggregation-and-grouping-on-stream-1" class="level3">
<h3 class="anchored" data-anchor-id="hand-on-4-aggregation-and-grouping-on-stream-1">‚úçHand-on 4 : Aggregation and grouping on stream üß≤</h3>
<p><strong>Q14</strong> Count the number of different hashtag.</p>
<p><strong>Q15</strong> Group by hashtag and compute the average, min and max of <code>like_count</code></p>
<ul>
<li>Use the <code>groupBy()</code> and <code>agg()</code> transformations</li>
</ul>
<p><strong>Q16</strong> Compute the average of <code>like_count</code>, <code>retweet_count</code> and <code>quote_count</code> :</p>
<ul>
<li>across all <code>hashtag</code> and <code>lang</code></li>
<li>for each <code>lang</code> across all <code>hashtag</code></li>
<li>for each <code>hashtag</code> across all <code>lang</code></li>
<li>for each <code>hashtag</code> and each <code>lang</code></li>
</ul>
<p>To do so, replace the <code>groupBy()</code> transformation by the <code>cube()</code> one. <code>cube()</code> group compute all possible cross between dimensions passed as parameter. You will get something like this</p>
<table class="table">
<colgroup>
<col style="width: 11%">
<col style="width: 6%">
<col style="width: 25%">
<col style="width: 30%">
<col style="width: 26%">
</colgroup>
<thead>
<tr class="header">
<th>hashtag</th>
<th>lang</th>
<th>avg(like_count)</th>
<th>avg(retweet_count)</th>
<th>avg(quote_count)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>cat</td>
<td>null</td>
<td>1</td>
<td>2</td>
<td>3</td>
</tr>
<tr class="even">
<td>dog</td>
<td>null</td>
<td>4</td>
<td>5</td>
<td>6</td>
</tr>
<tr class="odd">
<td>‚Ä¶</td>
<td>‚Ä¶</td>
<td>‚Ä¶</td>
<td>‚Ä¶</td>
<td>‚Ä¶</td>
</tr>
<tr class="even">
<td>bird</td>
<td>fr</td>
<td>7</td>
<td>8</td>
<td>9</td>
</tr>
<tr class="odd">
<td>null</td>
<td>en</td>
<td>10</td>
<td>11</td>
<td>12</td>
</tr>
<tr class="even">
<td>null</td>
<td>null</td>
<td>13</td>
<td>14</td>
<td>15</td>
</tr>
</tbody>
</table>
<p>A <code>null</code> value mean this dimension wasn‚Äôt use for this row. For instance, the first row gives the averages when <code>hashtag==cat</code> independently of the <code>lang</code>. The before last row gives averages when <code>lang==en</code> independently of the <code>hashtag</code>. And the last row gives the averages for the full DataFrame.</p>
</section>
</section>
<section id="event-time-processing-1" class="level2">
<h2 class="anchored" data-anchor-id="event-time-processing-1">8. ü•á ‚åõEvent-time processing</h2>
<p>Event-time processing consists in processing information with <strong>respect to the time that it was created, not received</strong>. It‚Äôs a hot topic because sometime you will receive data in an order different from the creation order. For example, you are monitoring servers distributed across the globe. Your main datacentre is located in Paris. Something append in New York, and a few milliseconds after something append in Toulouse. Due to location, the event in Toulouse is likely to show up in your datacentre before the New York one. If you analyse data bases on the received time the order will be different than the event time. Computers and network are unreliable. Hence, when temporality is important, you must consider the creation time of the event and not it‚Äôs received time.</p>
<p>Hopefully, Spark will handle all this complexity for you ! If you have a timestamp column with the event creation spark can update data accordingly to the event time.</p>
<p>For instance is you process some data with a time window, Spark will update the result based on the event-time not the received time. So previous windows can be updated in the future.</p>
<p><img src="https://github.com/HealerMikado/panorama_big_data_2021/blob/main/labs/lab%203%20-%20Spark%20Stream/img/lata%20data%20handling%20without%20watermarks.png?raw=true" class="img-fluid"></p>
<p><em>Figure 5 : Time-event processing, event grouped by time windows</em></p>
<p>To work with time windows, Spark offers two type of windows</p>
<ul>
<li>Normal windows. You only consider event in a given windows. All windows are disjoint, and a event is only in one window.</li>
<li>Sliding windows. You have a fix window size (for example 1 hour) and a trigger time (for example 10 minute). Every 10 minute, you will process the data with an event time less than 1h.</li>
</ul>
<p><img src="https://github.com/HealerMikado/panorama_big_data_2021/blob/main/labs/lab%203%20-%20Spark%20Stream/img/time%20windows.png?raw=true" class="img-fluid"></p>
<p><em>Figure 6 : Time-event processing, event grouped by sliding time windows</em></p>
<p>To create time windows, you need :</p>
<ul>
<li><p>to define a time window : <code>window(column_with_time_event : str or col, your_time_window : str, timer_for_sliding_window) : str</code></p></li>
<li><p>grouping row by event-time using your window : <code>df.groupeBy(window(...))</code></p></li>
</ul>
<p>To produce the above processes :</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Need some import</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> window, col</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="co"># word count + classic time window</span></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>df_with_event_time.groupBy(</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    window(df_with_event_time.event_time, <span class="st">"5 minutes"</span>),</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>    df_with_event_time.word).count()</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a><span class="co"># word count + sliding time window</span></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>df_with_event_time.groupBy(</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>    window(df_with_event_time.event_time, <span class="st">"10 minutes"</span>, <span class="st">"5 minutes"</span>),</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>    df_with_event_time.word).count()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="hand-on-5-event-time-processing-1" class="level3">
<h3 class="anchored" data-anchor-id="hand-on-5-event-time-processing-1">‚úçHand-on 5 : Event-time processing ‚åõ</h3>
<p>Count the number of event with a 10 seconds time window (use the <code>created_at</code> column)</p>
<div class="cell" data-tags="[]" data-vscode="{&quot;languageId&quot;:&quot;json&quot;}">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Count the number of event by verified / unverified user with a 10 seconds time window (use the <code>Creation_Time</code> column)</p>
<p>Count the number of event with a 10 seconds time window sliding every 5 seconds</p>
</section>
<section id="handling-late-data-with-watermarks-1" class="level3">
<h3 class="anchored" data-anchor-id="handling-late-data-with-watermarks-1">8.1.üèÜ ‚è≥ ‚ÄãHandling late data with watermarks</h3>
<p>Processing accordingly to time-event is great, but currently there is one flaw. We never specified how late we expect to see data. This means, Spark will keep some data in memory forever. Because streams never end, Spark will keep in memory every time windows, to be able to update some previous results. But in some cases, you know that after some time, you don‚Äôt expect new data, or very late data aren‚Äôt relevant any more. In other words, after a certain amount of time you want to freeze old results.</p>
<p>Once again, Spark can handle such process, with watermarks.</p>
<p><img src="https://github.com/HealerMikado/panorama_big_data_2021/blob/main/labs/lab%203%20-%20Spark%20Stream/img/lata%20data%20handling%20with%20watermarks.png?raw=true" class="img-fluid"> <em>Figure 7 : Time-event processing with watermark</em></p>
<p>To do so, you have to define column as watermark and a the max delay. You have to use the <code>withWatermark(column, max_delay)</code> method.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Need some import</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> window, col</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a><span class="co"># word count + classic time window</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>df_with_event_time.withWatermark(df_with_event_time.event_time, <span class="st">"4 minutes"</span>)<span class="op">\</span></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>.groupBy(</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>    window(df_with_event_time.event_time, <span class="st">"5 minutes"</span>),</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>    df_with_event_time.word).count()</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a><span class="co"># word count + sliding time window</span></span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>df_with_event_time.withWatermark(df_with_event_time.event_time, <span class="st">"4 minutes"</span>)<span class="op">\</span></span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>.groupBy(</span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>    window(df_with_event_time.event_time, <span class="st">"10 minutes"</span>, <span class="st">"5 minutes"</span>),</span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>    df_with_event_time.word).count()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Be careful, the watermark field cannot be a nested field (<a href="https://www.waitingforcode.com/apache-spark-structured-streaming/nested-fields-dropduplicates-watermark-apache-spark-structured-streaming/read">link</a>)</p>
<section id="hand-on-6-handling-late-data-with-watermarks" class="level4">
<h4 class="anchored" data-anchor-id="hand-on-6-handling-late-data-with-watermarks">‚úçHand-on 6 : Handling late data with watermarks ‚è≥</h4>
<p>Count the number of event with a 10 seconds time window (use the <code>created_at</code> column) with a 5 seconds watermark</p>
<p>Count the number of event by hashtag with a 30 seconds time window with a 1 minute watermark</p>
<p>Count the number of events post by verified user with a 10 seconds time window sliding every 5 seconds with 25 seconds watermark. Write the the result in a file sorted in S3.</p>
</section>
</section>
</section>
<section id="for-more-details-1" class="level2">
<h2 class="anchored" data-anchor-id="for-more-details-1">For more details</h2>
<ul>
<li><a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#input-sources">Spark official documentation</a></li>
<li>ZAHARIA, B. C. M. (2018). <em>Spark: the Definitive Guide</em>. , O‚ÄôReilly Media, Inc.&nbsp;https://proquest.safaribooksonline.com/9781491912201</li>
<li>https://databricks.com/blog/2018/03/13/introducing-stream-stream-joins-in-apache-spark-2-3.html</li>
<li>https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html</li>
<li>https://databricks.com/blog/2015/07/30/diving-into-apache-spark-streamings-execution-model.html</li>
</ul>
<p><strong>DO NOT FORGET TO TURN YOUR CLUSTER OFF A THE END OF THIS TUTORIAL!</strong></p>
<p><strong>DO NOT FORGET TO TURN YOUR CLUSTER OFF A THE END OF THIS TUTORIAL!</strong></p>
<p><strong>DO NOT FORGET TO TURN YOUR CLUSTER OFF A THE END OF THIS TUTORIAL!</strong></p>
<p><strong>DO NOT FORGET TO TURN YOUR CLUSTER OFF A THE END OF THIS TUTORIAL!</strong></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'alternate';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.dev/ludo2ne/ENSAI-2A-Big-Data/blob/main/docs/lab/lab3/lab3-AWS.ipynb" class="toc-action">Edit this page</a></p><p><a href="https://github.com/ludo2ne/ENSAI-2A-Big-Data/issues/new" class="toc-action">Report an issue</a></p></div></div></div>
    <div class="nav-footer-right">Website built with <a href="https://quarto.org/" target="_blank">Quarto</a><br> <a href="https://github.com/ludo2ne/ENSAI-2A-Big-Data" target="_blank">Source code</a></div>
  </div>
</footer>



</body></html>