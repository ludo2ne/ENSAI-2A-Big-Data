[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Big Data",
    "section": "",
    "text": "Lessons and Labs created by Rémi Pépin and Arthur Katossky"
  },
  {
    "objectID": "index.html#objectives",
    "href": "index.html#objectives",
    "title": "Introduction to Big Data",
    "section": "Objectives",
    "text": "Objectives\n\nUnderstand the basics of computation in the real world, the bottlenecks and how to solve them\nUnderstand the basics of cloud computing and how to use AWS (or SSP Cloud)\nGet familiar with big data technologies and the most common paradigm\nLearn how to use Spark for data exploration on data at rest or streamed data, and how to use some basics ML algorithm on big data"
  },
  {
    "objectID": "index.html#organisation",
    "href": "index.html#organisation",
    "title": "Introduction to Big Data",
    "section": "Organisation",
    "text": "Organisation\n\nLessons : 7h30\nLabs : 10h30 + 3h (graded lab)\nPresentations : 3h"
  },
  {
    "objectID": "index.html#lessons",
    "href": "index.html#lessons",
    "title": "Introduction to Big Data",
    "section": "Lessons",
    "text": "Lessons\n\n1.1 What is Big Data\n\nWhat is considered big ?\nVolume, Velocity, Variety, Veracity, Value, Variability\n\n\n\n1.2 Computer science survival kit\n\nProcessors, Memory, Storage, Network\n\n\n\n1.3 High-performance computing without distribution\n\nProfile code, Analyse code\nStore or process data in chunks, Take advantage of sparcity\nGo low-level\nUse cache\n\n\n\n1.4 What if ?\n\nWhat if data is too big to fit in memory ?\nWhat if your file is too big for your local file system ?\nWhat if data is too big to fit on disk ?\nWhat if computation takes ages ?\nWhat if computation / storage is too expensive ?\n\n\n\n1.5 Social issues\n\nEthical issues, Environmental issues, Political issues\n\n\n\n\n2.1 How to store big data\n\nFile system, Database, Distribution\nThe CAP theorem\n\n\n\n2.2 Hadoop file system (HDFS)\n\nHadoop Ecosystem, How to use HDFS\n\n\n\n2.3 Hadoop MapReduce\n\nKey Concepts\n\n\n\n2.4 Spark\n\nKey Concepts\nImporting/Exporting data\nHow to run Spark ?\n\n\n\n\n3 Cloud Computing\n\nTraditional IT, Virtualization, Containerization\nWhy cloud computing ?\ncategories of cloud services"
  },
  {
    "objectID": "index.html#labs",
    "href": "index.html#labs",
    "title": "Introduction to Big Data",
    "section": "Labs",
    "text": "Labs\n\nLab 0 : Discover Amazon Web Service (AWS)\nLab 1 : First steps with Spark\nLab 2 : Spark ML\nLab 3 : Stream processing with Spark"
  },
  {
    "objectID": "docs/lab/lab1/lab1-SSPCloud.html",
    "href": "docs/lab/lab1/lab1-SSPCloud.html",
    "title": "First steps with Spark",
    "section": "",
    "text": "Download this Jupyter Notebook\nFollow these instructions"
  },
  {
    "objectID": "docs/lab/lab1/lab1-SSPCloud.html#before-you-start",
    "href": "docs/lab/lab1/lab1-SSPCloud.html#before-you-start",
    "title": "First steps with Spark",
    "section": "",
    "text": "Download this Jupyter Notebook\nFollow these instructions"
  },
  {
    "objectID": "docs/lab/lab1/lab1-SSPCloud.html#create-a-spark-session",
    "href": "docs/lab/lab1/lab1-SSPCloud.html#create-a-spark-session",
    "title": "First steps with Spark",
    "section": "1 Create a Spark session",
    "text": "1 Create a Spark session\n\nExecute the following two cells to initialize a Spark session.\n\n\nimport os\nfrom pyspark.sql import SparkSession\n\nspark = (SparkSession\n         .builder\n         # default url of the internally accessed Kubernetes API\n         # (This Jupyter notebook service is itself a Kubernetes Pod)\n         .master(\"k8s://https://kubernetes.default.svc:443\")\n         # Executors spark docker image: for simplicity reasons, this jupyter notebook is reused\n         .config(\"spark.kubernetes.container.image\", os.environ['IMAGE_NAME'])\n         # Name of the Kubernetes namespace\n         .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\n         # Allocated memory to the JVM\n         # Stay careful, by default, the Kubernetes pods has a higher limit which depends on other parameters.\n         .config(\"spark.executor.memory\", \"4g\")\n         .config(\"spark.kubernetes.driver.pod.name\", os.environ['KUBERNETES_POD_NAME'])\n         # dynamic allocation configuration\n         .config(\"spark.dynamicAllocation.enabled\",\"true\")\n         .config(\"spark.dynamicAllocation.initialExecutors\",\"1\")\n         .config(\"spark.dynamicAllocation.minExecutors\",\"1\")\n         .config(\"spark.dynamicAllocation.maxExecutors\",\"60\")\n         # Ratio match the number of pods to create for a given number of parallel tasks\n         # (100 parallel, ratio of 1, one aims at 100 pods, with 0.5 it would be 50 pods)\n         .config(\"spark.dynamicAllocation.executorAllocationRatio\",\"1\")\n         .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",\"true\")\n         .getOrCreate()\n        )\n\n\nspark"
  },
  {
    "objectID": "docs/lab/lab1/lab1-SSPCloud.html#first-steps-with-spark---data-importation",
    "href": "docs/lab/lab1/lab1-SSPCloud.html#first-steps-with-spark---data-importation",
    "title": "First steps with Spark",
    "section": "2 First steps with Spark - Data importation",
    "text": "2 First steps with Spark - Data importation\nSpark’s main object class is the DataFrame, which is a distributed table. It is analogous to R’s or Python (Pandas)’s data frames:\n\none row represents an observation,\none column represents a variable.\n\nBut contrary to R or Python, Spark’s DataFrames can be distributed over hundred of nodes.\nSpark support multiple data formats, and multiple ways to load them.\n\ndata format : csv, json, parquet (an open source column oriented format)\ncan read archive files\nschema detection or user defined schema. For static data, like a json file, schema detection can be use with good results.\n\nSpark has multiple syntaxes to import data. Some are simple with no customisation, others are more complexes but you can specify options.\nThe simplest syntaxes to load a json or a csv file are :\n# JSON\njson_df = spark.read.json([location of the file])\n# csv\ncsv_df = spark.read.csv([location of the file])\nIn the future, you may consult the Data Source documentation to have the complete description of Spark’s reading abilities.\nThe data you will use in this lab are real data from the twitter sampled stream API and filtered stream API. The tweets folder contains more than 50 files and more than 2 million tweets. The tweets was collected between the 14/04/2021 and the 18/04/2021. The total collection time was less than 10 hours.\n\n\n2.1 ✍Hands-on 1 - Data importation\n\nLoad the json file stored here : s3a://ludo2ne/diffusion/tweets.jsonl.gz\n\nName you data frame df_tweet\nUse function cache() on the data frame. Caching is a performance optimization technique that allows you to persist an intermediate or final result of a computation in memory, reducing the need to recompute the data when it is accessed multiple time\n\n ⚙️ This file is an a JSONL (JSON-line) format, which means that each line of it is a JSON object. A JSON object is just a Python dictionary or a JavaScript object and looks like this: { key1: value1, key2: [\"array\", \"of\", \"many values]}). This file has been compressed into a GZ archive, hence the .jsonl.gz ending. Also this file is not magically appearing in your S3 storage. It is hosted on one of your teacher’s bucket and has been made public, so that you can access it.\n\nIt’s possible to load multiple file in a unique DataFrame. It’s useful when you have daily files and want to process them all. It’s the same syntax as the previous one, just specify a folder.\n\nIf you meet some issue to load this file, you can load and use your own file :\n\nIn Onyxia, mes fichiers\nLoad file tweets.jsonl.gz\nIn Jupyter, read it using s3a://&lt;user_name&gt;/tweets.jsonl.gz\n\n\n\n# DataFrame creation\n\n# caching"
  },
  {
    "objectID": "docs/lab/lab1/lab1-SSPCloud.html#data-frame-basic-manipulations",
    "href": "docs/lab/lab1/lab1-SSPCloud.html#data-frame-basic-manipulations",
    "title": "First steps with Spark",
    "section": "3 Data frame basic manipulations",
    "text": "3 Data frame basic manipulations\nIf DataFrames are immutable, they can however be transformed in other DataFrames, in the sense that a modified copy is returned. Such transformations include: filtering, sampling, dropping columns, selecting columns, adding new columns…\nFirst, you can get information about the columns with:\ndf.columns       # get the column names\ndf.schema        # get the column names and their respective type\ndf.printSchema() # same, but human-readable\nYou can select columns with the select() method. It takes as argument a list of column name. For example :\ndf_with_less_columns = df\\\n  .select(\"variable3\",\"variable_four\",\"variable-6\")\n\n# Yes, you do need the ugly \\ at the end of the line,\n# if you want to chain methods between lines in Python\nYou can get nested columns easily with :\ndf.select(\"parentField.nestedField\")\nTo filter data you could use the filter() method. It take as input an expression that gets evaluated for each observation and should return a boolean. Sampling is performed with the sample() method. For example :\ndf_with_less_rows = df\\\n  .sample(fraction=0.001)\\\n  .filter(df.variable1==\"value\")\\\n  .show(10)\nAs said before your data are distributed over multiple nodes (executors) and data inside a node are split into partitions. Then each transformations will be run in parallel. They are called narrow transformation For example, to sample a DataFrame, Spark sample every partitions in parallel because sample all partition produce the sample DataFrame. For some transformations, like groupBy() it’s impossible, and it’s cannot be run in parallel.\n\n\n\n\n3.1 Lazy evaluation\nThis is because Spark has what is known as lazy evaluation, in the sense that it will wait as much as it can before performing the actual computation. Said otherwise, when you run an instruction such as:\ntweet_author_hashtags = df_tweet_big.select(\"auteur\",\"hashtags\")\n… you are not executing anything! Rather, you are building an execution plan, to be realised later.\nSpark is quite extreme in its laziness, since only a handful of methods called actions, by opposition to transformations, will trigger an execution. The most notable are:\n\ncollect(), explicitly asking Spark to fetch the resulting rows instead of to lazily wait for more instructions,\ntake(n), asking for n first rows\nfirst(), an alias for take(1)\nshow() and show(n), human-friendly alternatives\ncount(), asking for the numbers of rows\nall the “write” methods (write on file, write to database), see here for the list\n\nThis has advantages: on huge data, you don’t want to accidently perform a computation that is not needed. Also, Spark can optimize each stage of the execution in regard to what comes next. For instance, filters will be executed as early as possible, since it diminishes the number of rows on which to perform later operations. On the contrary, joins are very computation-intense and will be executed as late as possible. The resulting execution plan consists in a directed acyclic graph (DAG) that contains the tree of all required actions for a specific computation, ordered in the most effective fashion.\nThis has also drawbacks. Since the computation is optimized for the end result, the intermediate stages are discarded by default. So if you need a DataFrame multiple times, you have to cache it in memory because if you don’t Spark will recompute it every single time.\n\n\n\n3.2 ✍Hands-on 2 - Data frame basic manipulations\n\nHow many rows have your two DataFrame ?\n\n\n# Code Here\n\n\nDisplay columns names and then the schema\n\n\n# Code Here\n\n\n# Code Here\n\n\nDisplay 10 rows of df_tweet\n\n\n# Code Here\n\n\nSample df_tweet and keep only 10% of it.\n\nCreate a new DataFrame named df_tweet_sampled.\nIf computations take too long on the full DataFrame, use this one instead or add a sample transformation in your expression.\n\n\n\n# Code Here\n\n\nDefine a DataFrame tweet_author_hashtags with only the auteur and hashtags columns\n\nThen display 5 rows\n\n\n\n# Code Here\n\n\nPrint 5 lines of a df_tweet with only the auteur, mentions, and urls columns.\n\nmentions and urls are both nested columns in entities\n\n\n\n# Code Here\n\n\nFilter df_tweet and keep only tweets with more than 1 like.\n\nDisplay only auteur, contenu and like_count\nPrint 10 lines\n\n\n\n# Code Here"
  },
  {
    "objectID": "docs/lab/lab1/lab1-SSPCloud.html#basic-dataframe-column-manipulation",
    "href": "docs/lab/lab1/lab1-SSPCloud.html#basic-dataframe-column-manipulation",
    "title": "First steps with Spark",
    "section": "4 Basic DataFrame column manipulation",
    "text": "4 Basic DataFrame column manipulation\nYou can add/update/rename column of a DataFrame using spark :\n\nDrop : df.drop(columnName : str )\nRename : df.withColumnRenamed(oldName : str, newName : str)\nAdd/update : df.withColumn(columnName : str, columnExpression)\n\nFor example\ndf_tweet\\\n  .withColumn(                                        # computes new variable\n    \"like_rt_ratio\",                                  # like_rt_ratio \"OVERCONFIDENCE\"\n    df_tweet.like_count /df_tweet.retweet_count)\nSee here for the list of all functions available in an expression.\n\n4.1 ✍Hands-on 3 - Basic DataFrame column manipulation\n\nDefine a DataFrame with a column names interaction_count named df_tweet_interaction_count\n\nThis column is the sum of like_count, reply_count and retweet_count.\n\n\n\n# Code Here\n\n\nUpdate the DataFrame you imported at the beginning of this lab and drop the other column\n\n\n# Code Here"
  },
  {
    "objectID": "docs/lab/lab1/lab1-SSPCloud.html#advance-dataframe-column-manipulation",
    "href": "docs/lab/lab1/lab1-SSPCloud.html#advance-dataframe-column-manipulation",
    "title": "First steps with Spark",
    "section": "5 Advance DataFrame column manipulation",
    "text": "5 Advance DataFrame column manipulation\n\n5.1 Array manipulation\nSome columns often contain arrays (lists) of values instead of just one value. This may seem surprising but this actually quite natural. For instance, you may create an array of words from a text, or generate a list of random numbers for each observation, etc.\nYou may create array of values with:\n\nsplit(text : string, delimiter : string), turning a text into an array of strings\n\nYou may use array of values with:\n\nsize(array : Array), getting the number of elements\narray_contains(inputArray : Array, value : any), checking if some value appears\nexplode(array : Array), unnesting an array and duplicating other values. For instance if you use explode() over the hashtags value of this DataFrame:\n\n\n\nAuteur\nContenu\nHashtags\n\n\n\n\nBob\nI love #Spark and #bigdata\n[Spark, bigdata]\n\n\nAlice\nJust finished #MHrise, best MH ever\n[MHrise]\n\n\n\nYou will get :\n\n\n\n\n\n\n\n\n\nAuteur\nContenu\nHashtags\nHashtag\n\n\n\n\nBob\nI love #Spark and #bigdata\n[Spark, bigdata]\nSpark\n\n\nBob\nI love #Spark and #bigdata\n[Spark, bigdata]\nbigdata\n\n\nAlice\nJust finished #MHrise, best MH ever\n[MHrise]\nMHrise\n\n\n\n\nAll this functions must be imported first :\nfrom pyspark.sql.functions import split, explode, size, array_contains\nDo not forget, to create a new column, you should use withColumn(). For example :\ndf.withColumn(\"new column\", explode(\"array\"))\n\n✍Hands-on 4 - Array manipulation\n\nKeep all the tweets with hashtags and for each remaining line, split the hashtag text into an array of hashtags\n\n\n# Code Here\n\n\nCreate a new column with the number of words of the contenu column. (Use split() + size())\n\n\n# Code Here\n\n\nCount how many tweet contain the Ukraine hashtag (use the count() action)\n\n\n# Code Here\n\n\n\n\n5.2 User defined function\nFor more very specific column manipulation you will need Spark’s udf() function (User Defined Function). It can be useful if Spark does not provide a feature you want. But Spark is a popular and active project, so before coding an udf, go check the documentation. For instance for natural language processing, Spark already has some functions. Last things, python udf can lead to performance issues (see https://stackoverflow.com/a/38297050) and learning a little bit of scala or java can be a good idea.\nFor example :\n# !!!! DOES NOT WORK !!!!\ndef to_lower_case(string):\n    return string.lower()\n\ndf.withColumn(\"tweet_lower_case\", to_lower_case(df.contenu))\nwill just crash. Keep in mind that Spark is a distributed system, and that Python is only installed on the central node, as a convenience to let you execute instructions on the executor nodes. But by default, pure Python functions can only be executed where Python is installed! We need udf() to enable Spark to send Python instructions to the worker nodes.\nLet us see how it is done :\n# imports\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.functions import explode\nfrom pyspark.sql.types import StringType\n\n# pure python functions\ndef to_lower_case(string):\n    return string.lower()\n\n# user defined function(we use a lambda function to create the udf)\nto_lower_case_udf = udf(\n    lambda x: to_lower_case(x), StringType()\n)\n\n# df manipulation\ndf_tweet_small\\\n  .select(\"auteur\",\"hashtags\")\\\n  .filter(\"size(hashtags)!=0\")\\\n  .withColumn(\"hashtag\", explode(\"hashtags\"))\\\n  .withColumn(\"hashtag\", to_lower_case_udf(\"hashtag\")).show(10)\n\n\n✍Hands-on 5 - User defined function\n\nCreate an user defined function that counts how many words a tweet contains.\n\nyour function will return an IntegerType and not a StringType\n\n\n\n# Code Here"
  },
  {
    "objectID": "docs/lab/lab1/lab1-SSPCloud.html#aggregation-functions",
    "href": "docs/lab/lab1/lab1-SSPCloud.html#aggregation-functions",
    "title": "First steps with Spark",
    "section": "6 Aggregation functions",
    "text": "6 Aggregation functions\nSpark offer a variety of aggregation functions :\n\ncount(column : string) will count every not null value of the specify column. You cant use count(1) of count(\"*\") to count every line (even row with only null values)\ncountDisctinct(column : string) and approx_count_distinct(column : string, percent_error: float). If the exact number is irrelevant, approx_count_distinct()should be preferred.\nCounting distinct elements cannot be done in parallel, and need a lot data transfer. But if you only need an approximation, there is a algorithm, named hyper-log-log (more info here) that can be parallelized.\nfrom pyspark.sql.functions import count, countDistinct, approx_count_distinct\n\ndf.select(count(\"col1\")).show()\ndf.select(countDistinct(\"col1\")).show()\ndf.select(approx_count_distinct(\"col1\"), 0.1).show()\nYou have access to all other common functions min(), max(), first(), last(), sum(), sumDistinct(), avg() etc (you should import them first from pyspark.sql.functions import min, max, avg, first, last, sum, sumDistinct)\n\n\n\n6.1 ✍Hands-on 6 - Aggregation functions\n\nWhat are the min, max, average of interaction_count (use df_tweet_interaction_count created earlier)\n\ndon’t forget to import the required functions\n\n\n\n# Code Here\n\n\nHow many tweets have hashtags ?\n\nDistinct hashtags ?\nTry the approximative count with 0.1 and 0.01 as maximum estimation error allowed.\n\n\n\n# Code Here"
  },
  {
    "objectID": "docs/lab/lab1/lab1-SSPCloud.html#grouping-functions",
    "href": "docs/lab/lab1/lab1-SSPCloud.html#grouping-functions",
    "title": "First steps with Spark",
    "section": "7 Grouping functions",
    "text": "7 Grouping functions\nLike SQL you can group row by a criteria with Spark. Just use the groupBy(column : string) method. Then you can compute some aggregation over those groups.\ndf\\\n  .groupBy(\"col1\")\\\n  .agg(count(\"col2\").alias(\"quantity\"))\\           # alias is use to specify the name of the new column\n  .show()\nThe agg() method can take multiples argument to compute multiple aggregation at once.\ndf\\\n  .groupBy(\"col1\")\\\n  .agg(count(\"col2\").alias(\"quantity\"),\n       min(\"col2\").alias(\"min\"),\n       avg(\"col3\").alias(\"avg3\"))\\\n  .show()\nAggregation and grouping transformations work differently than the previous method like filter(), select(), withColumn() etc. Those transformations cannot be run over each partitions in parallel, and need to transfer data between partitions and executors. They are called “wide transformations”\n\n\n7.1 ✍Hands-on 7 - Grouping functions\n\nCompute a daframe with the min, max and average retweet of each auteur.\n\nThen order it by the max number of retweet in descending order by .\nTo do that you can use the following syntax\n\nfrom pyspark.sql.functions import desc\ndf.orderBy(desc(\"col\"))\n\n\n# Code Here"
  },
  {
    "objectID": "docs/lab/lab1/lab1-SSPCloud.html#spark-sql",
    "href": "docs/lab/lab1/lab1-SSPCloud.html#spark-sql",
    "title": "First steps with Spark",
    "section": "8 Spark SQL",
    "text": "8 Spark SQL\nSpark understand SQL statement. It’s not a hack nor a workaround to use SQL in Spark, it’s one a the more powerful feature in Spark. To use SQL you will need :\n\nRegister a view pointing to your DataFrame\nmy_df.createOrReplaceTempView(viewName : str)\nUse the sql function\nspark.sql(\"\"\"\nYour SQL statement\n\"\"\")\nYou could manipulate every registered DataFrame by their view name with plain SQL.\n\nIn fact you can do most of this tutorial without any knowledge in PySpark nor Spark. Many things can only be done in Spark if you know SQL and how to use it in Spark.\n\n8.1 ✍Hands-on 8 - Spark SQL\n\nHow many tweets have hashtags ?\n\nDistinct hashtags ?\n\n\n\n# Code Here\n\n\nCompute a dataframe with the min, max and average retweet of each auteur using Spark SQL\n\n\n# Code Here"
  },
  {
    "objectID": "docs/lab/lab1/lab1-SSPCloud.html#delete-the-jupyter-pyspark-service",
    "href": "docs/lab/lab1/lab1-SSPCloud.html#delete-the-jupyter-pyspark-service",
    "title": "First steps with Spark",
    "section": "9 Delete the Jupyter-pyspark service",
    "text": "9 Delete the Jupyter-pyspark service\n\nExport your notebook\n\nRight clic and Download (.ipynb)\nFile &gt; Save and Export Notebook &gt; HTML\n\nhttps://datalab.sspcloud.fr/my-services"
  },
  {
    "objectID": "docs/lab/lab0/lab0.html",
    "href": "docs/lab/lab0/lab0.html",
    "title": "Discover Amazon Web Service (AWS)",
    "section": "",
    "text": "The lab’s goal is to make you familiar with the AWS’s interface. In this lab, you will :\n\nCreate an account on AWS academy if you haven’t done yet\nStore some data on Amazon Simple Storage Service (AWS S3)\nCreate an Elastic Compute Cloud instance (= a virtual machine) and connect to it with SSH\nRun some basic shell commands\n\nls to list files in a directory\ncd to change the current directory\nyum to install a package\naws s3 cp to copy file from S3\nchmod to change some file persmision\ntime [commande] to compute some execution time\n\nShut down your EC2 instance"
  },
  {
    "objectID": "docs/lab/lab0/lab0.html#objectives",
    "href": "docs/lab/lab0/lab0.html#objectives",
    "title": "Discover Amazon Web Service (AWS)",
    "section": "",
    "text": "The lab’s goal is to make you familiar with the AWS’s interface. In this lab, you will :\n\nCreate an account on AWS academy if you haven’t done yet\nStore some data on Amazon Simple Storage Service (AWS S3)\nCreate an Elastic Compute Cloud instance (= a virtual machine) and connect to it with SSH\nRun some basic shell commands\n\nls to list files in a directory\ncd to change the current directory\nyum to install a package\naws s3 cp to copy file from S3\nchmod to change some file persmision\ntime [commande] to compute some execution time\n\nShut down your EC2 instance"
  },
  {
    "objectID": "docs/lab/lab0/lab0.html#log-on-aws",
    "href": "docs/lab/lab0/lab0.html#log-on-aws",
    "title": "Discover Amazon Web Service (AWS)",
    "section": "1. Log on AWS",
    "text": "1. Log on AWS\n\nFollow the instructions you get from the AWS academy’s e-mail, then those of the AWS Academy Learner Lab - Student Guide to access to the AWS console.\n\nYour AWS account is located in North Virginia, do not change that.\nBecause your account is for academic purpose, your don’t have access to all the AWS services.\nYour account is managed by AWS academy, so you have to use the AWS academy portal to access to your AWS account.\n\nAWS Login\nDashboard\nSelect AWS Academy Learner Lab - Big Data\nSelect Modules and then Lancer l'atelier pour étudiants de l'AWS Academy\nStart Lab\n\nWait 2 minutes\nWhen the dot next to AWS turns green, your lab environment is ready to use\nClick AWS"
  },
  {
    "objectID": "docs/lab/lab0/lab0.html#exploration",
    "href": "docs/lab/lab0/lab0.html#exploration",
    "title": "Discover Amazon Web Service (AWS)",
    "section": "2. Exploration",
    "text": "2. Exploration\nIn the Services tab, you should find for instance :\n\nEC2 in computation\nS3 in storage\nA section for databases\nA section for machine-learning\n\nA section for data analysis"
  },
  {
    "objectID": "docs/lab/lab0/lab0.html#create-a-s3-bucket",
    "href": "docs/lab/lab0/lab0.html#create-a-s3-bucket",
    "title": "Discover Amazon Web Service (AWS)",
    "section": "3. Create a S3 bucket",
    "text": "3. Create a S3 bucket\nAmazon Simple Storage Service (S3) is the standard solution to store data on AWS. Amazon assured a 99,999999999 % durability for your data. This mean, is you store 10 000 000 files on S3, you can on average expect to incur a loss of a single file every 10 000 years.\nThe storage is quite expensive (around ~0,02 $/Go/month), knowing that you pay for read operations (around ~0,09 \\$/Go). 1 To cost around 240$ a year. For instance, a 1To SSD cost less than 100$ (for a smaller durability), and a cloud storage solution (like drop box) for private individual cost 10$/month for 2To (for the same durability). But the use case are different. S3 is for data access frequently by other AWS services. There is other storage solution (like S3 glacier) for archive, or databases.\nAll AWS services can natively read from and write to S3, if they have some access right. So, every application you deploy on AWS can import/export data from/to S3. A file stored in S3 is called an “object”, and can be access by a unique URL. You can limit access right to a specific file.\n\nIn the search bar type S3\nClick on Créer un compartiment (“bucket” in English)\n\nChose a unique name for your bucket (ex: ensai-firstName-lastName-currentDate)\nKeep all the default value\nCreate your bucket"
  },
  {
    "objectID": "docs/lab/lab0/lab0.html#import-some-files-to-your-bucket",
    "href": "docs/lab/lab0/lab0.html#import-some-files-to-your-bucket",
    "title": "Discover Amazon Web Service (AWS)",
    "section": "4. Import some files to your bucket",
    "text": "4. Import some files to your bucket\n\nSelect your bucket to go to its dedicated page\n\nClick on Charger, and upload the file Lab0_files.zip\n\n\n\nOnce the upload finished, click on your file\nYou will land on the file page, and found to link to access to your file. One is the URL of the file, the other is the S3 URI\nCopy in a text file your file URI, you will need it later"
  },
  {
    "objectID": "docs/lab/lab0/lab0.html#ssh-key-creation",
    "href": "docs/lab/lab0/lab0.html#ssh-key-creation",
    "title": "Discover Amazon Web Service (AWS)",
    "section": "5. SSH key creation",
    "text": "5. SSH key creation\nSSH (Secure SHell) protocol allow a secure remote connection to a machine. Moreover, with SSH you can control the remote machine. For more details, you can read this page page web. But first you need a private key to authenticate yourself.\n\nIn the search bar, search paire de clés\nClick on Créerune paire de clés\nGive it a name (par ex: labsuser), select the ppk format if you use windows, and pem if you use Linux / macOs, then click on créer\nThis will download your key, do not lost it !\nIf you are on Linux / macOs open a terminal and type :\ncd ~/Downloads\nchmod 400 labsuser.pem\nDo not close this terminal"
  },
  {
    "objectID": "docs/lab/lab0/lab0.html#create-your-first-virtual-machine",
    "href": "docs/lab/lab0/lab0.html#create-your-first-virtual-machine",
    "title": "Discover Amazon Web Service (AWS)",
    "section": "6. Create your first virtual machine",
    "text": "6. Create your first virtual machine\n\nIn the search bar, search EC2\n\nClick on Lancer une instance\nFirst, you must choose an image for your machine (called AMI pour Amazon Machine Image). This image contain an OS and some default applications. Choose the first : Amazon Linux 2 AMI (HVM) - Kernel 5.10.\n\nThen you will select the hardware configuration. For instance, for a general usage machine you can choose a t2.micro for a cheap but weak machine (0.012$/hour, 1 core, 1Go Ram) or a more powerful and more expensive one like t2.xlarge (0.188$/hour, 8 core, 32Go Ram). Because you pay for how long your EC2 instance are up, turn off you machine at the end of the lab !\n\nOn the next screen, select the AIM role LabInstanceProfile \nThen click on Vérifier et lancer\nValidate your instance\nSelect the key pair\nEt voilà! Your VM is launching. Click on Affichez les instances and wait a minute!"
  },
  {
    "objectID": "docs/lab/lab0/lab0.html#connect-to-the-ec2-instance",
    "href": "docs/lab/lab0/lab0.html#connect-to-the-ec2-instance",
    "title": "Discover Amazon Web Service (AWS)",
    "section": "7. Connect to the EC2 instance",
    "text": "7. Connect to the EC2 instance\n\nOnce your VM is running, you can access to the its administration page by clinking on its id d'instance\n\n\nYou will find many information, but the most important it’s its IPv4 publique which is its’s IP address (Internet Protocol) used to access to your machine from outside AWS.\n\nWindows user\n\nRun PuTTY\nIn Host Name type your public IP\nIn Saved Session enter AWS EC2 and Save\n\nThen go to SSH and find your .ppk file.\n\nFinally go back to the first screen, click on your session name, Save then Open the session.\n\nA similar windows will pop up, click on Oui\n\nA terminal will open with login as:. Type ec2-user then press Enter(documentation officielle)\n\n\n\n\nmacOS/Linux user\n\nIn the previously open terminal type\nssh -i labsuser.pem ec2-user@[public-ip]\nReplace [public-ip] by your public ip\n\nTada ! your are connected to your virtual machine. Although, this terminal is on your computer, every command you type are executed on the remote machine. This make it possible to run huge computation without altering your own performance. But, this machine do not have any graphical interface (GUI), so you need so basics of shell command."
  },
  {
    "objectID": "docs/lab/lab0/lab0.html#play-with-your-vm",
    "href": "docs/lab/lab0/lab0.html#play-with-your-vm",
    "title": "Discover Amazon Web Service (AWS)",
    "section": "8. Play with your VM",
    "text": "8. Play with your VM\nIn this section you will learn some basics of shell, and reproduce the benchmark of language made in the first lesson. You will :\n\nGet all the files for the benchmark\nInstall R and a python package on your machine\nRun the benchmark\n\nThis benchmark compare the time to compute the max temperature for some year based on the USA weather data. Each file contains all the weather data for one year, and each record isa weather observation. An observation looks like this :\n0029029070999991901010106004+64333+023450FM-12+000599999V0202701N015919999999N0000001N9-00781+99999102001ADDGF108991999999999999999999\nIt’s a positional file, with the temperature at the position [87, 91] and it’s quality at position 91. In the example the temperature is -0078 deca Celsius (so -7.8°C), with a quality of 1 (a good quality). Each program loads line by line the data and computes the max by updating the current max value (so a \\(\\mathcal{O}(n)\\) in computation time and \\(\\mathcal{O}(1)\\) in memory) (expect for R that load all the data in memory so \\(\\mathcal{O}(n)\\) in memory). But each language has its specificities :\n\npython : dynamic typing, just in time compilation to byte code then interpreted by python\njava : static typing, ahead of time compilation to byte code then interpreted by java\nC : statis typing, ahead of time compilation to machine code then executed by the OS\nscript bash : no type like python/java/C, interpeted by your OS.\n\nHere is some bash commands\n\n\n\n\n\n\n\nCommand\nUse\n\n\n\n\nsudo\nSuper user : basically you have all the rights\n\n\ncd [target_directory]\nChange directory : move in the file tree. To go to the parent directory docd ../\n\n\nls\nList : list all the file in the current directory\n\n\nmkdir [directory_name]\nMake directory : create a directory\n\n\nrm [file_name]\nRemove : remove a file. You cannot remove a none empty directory by default. You should use the rm [file_name] -r to do so. Note : NEVER execute the command rm -rf / because you will remove all the file on the current machine\n\n\nchmod 764 [file_name]\nChange mode : change the access right ofyour files. Without more detail, the access right 764 gives read, write and execution rights to the owner of the file.\n\n\nunzip [file_name]\nUnzip : unzip files\n\n\nyum install [package]\nPackage manager for some linux distribution. It’s like pip but for Linux.\n\n\naws s3 cp [s3://URI]\nSpecific AWS command. Copy file from s3 to the current machine\n\n\namazon-linux-extras [package]\nLike yum but for amazon machine.\n\n\n\n\nBenchmark setup\n\nDownload the file stored on S3. Use the command aws s3 cp [s3://object/uri] [output/folder]. The S3 URI is on S3 file page. For output/folder use the current directory with .. You should get a command and an output like this\n[ec2-user@ip-172-31-85-99 ~]$ aws s3 cp s3://remi-pepin-21032022/fichiersTP0.zip .\ndownload: s3://remi-pepin-21032022/fichier TP.zip to ./fichier TP.zip\nWith ls (list) check you downloaded the file\nNow you will unzip your file with unzip [nom de votre fichier]. Check the result with ls\nFor security reasons, you cannot execute your files. Do chmod 764 get_data.sh awk.sh GetMaxTempC to change that. For more details, the chmod wikipedia page provides more details..\nNow you will execute the get_data.sh script. Just type./get_data.sh. This script download some data from the NOAA (~ météo france in the USA) server\n\n\n\nInstall R, java, C compiler and a python package\nYour VM doesn’t have all the require packages for the benchmark.\n\npython-dev installation : python-devel is require to create extension for python. To install it use yum, a package manager for Linux. The exact command issudo yum install -y python3-devel.x86_64 (sudo to have super user right, yum to use the package manager, install to install a package, -y to validate the installation, and python3-devel.x86_64 for the package name)\nJava and GCC installation : in the same way, insatall java and GCC with sudo yum install java gcc -y\nCython installation : with pip3 install Cython and compile the Cython code:\n\ncd cython_code forchange directory and go in the cython_code directory\npython3 setup.py to run the compilation\ncd ../ to go back to the parent folder\n\nR installation : to install R, you will use the amazon custome package manager amazon-linux-extras, with the following command : sudo amazon-linux-extras install R4 -y. Just wait 1-2 minutes.\n\n\n\nBenchmark\nNow it’s time to run the benchmark. To do so, you will use the time command. time compute the execution time of the following command. For example : time ./get_data.sh . Write all the results, and compare then with the course. If results are different, try to understand why.\n\nTo run compiled C or bash : time ./[file]\nTo run compiled java :time java -jar [file.jar]\nTo run python :time python3 [file.py]\nTo run R script : time Rscript [filename.R]\n\n\n\nA shell in your web browser\n\nClose your terminal and go back to your EC2 instance page. Click onSe connecter then Se connecter  Vous allez arriver sur une page similaire à celle ci-dessous. Cliquez sur Se connecter  After 30 second you can use a cloud shell. You can do the same thing with your cloud shell as the previous SSH shell.\n\n\nType ls to check if your file are still there."
  },
  {
    "objectID": "docs/lab/lab0/lab0.html#shut-down-your-machine",
    "href": "docs/lab/lab0/lab0.html#shut-down-your-machine",
    "title": "Discover Amazon Web Service (AWS)",
    "section": "9. Shut down your machine",
    "text": "9. Shut down your machine\nBecause your EC2 instance is billed by time and not usage, once your work is done, shut down it. Although the cost is small, a small cost*24*7 for 1 week or running instance can cost 10$. And for bigger machine in a company maybe 100$ or 1000$.\nTo shut down your machine, go to EC2 &gt; Instances and find all running instances. Click on état de instance then on resilier l'instance."
  },
  {
    "objectID": "docs/lab/lab-setup.html",
    "href": "docs/lab/lab-setup.html",
    "title": "Lab setup",
    "section": "",
    "text": "You can choose to use :"
  },
  {
    "objectID": "docs/lab/lab-setup.html#before-starting-the-lab",
    "href": "docs/lab/lab-setup.html#before-starting-the-lab",
    "title": "Lab setup",
    "section": "Before starting the lab",
    "text": "Before starting the lab\n\nSSPCloud\n\nconnect to SSPCloud\n\nif necessary, create an account with your ENSAI e-mail address\n\nGo to Mes services\n\nCreate a new service : Jupyter-pyspark\nWait for a few seconds\nClic on Copier le mot de passe and then Ouvrir le service\nOn the next page, paste it in the appropriate place and Log in\nYou are now in your Jupyter service\n\nImport file named lab_-SSPCloud.ipynb\n\nopen it and follow the instructions\n\n\n\n\nAWS\n🚧"
  },
  {
    "objectID": "docs/lab/lab-setup.html#at-this-end-of-this-lab",
    "href": "docs/lab/lab-setup.html#at-this-end-of-this-lab",
    "title": "Lab setup",
    "section": "At this end of this lab",
    "text": "At this end of this lab\n\nExport your notebook\n\nFile &gt; Export Notebook As … &gt; Export Notebook To HTML\nRight clic &gt; Download the .ipynbfile\n\nDelete your service (SSPCloud)\nTurn your cluster off (AWS)"
  },
  {
    "objectID": "docs/lab/lab1/lab1-SSPCloud-correction.html",
    "href": "docs/lab/lab1/lab1-SSPCloud-correction.html",
    "title": "First steps with Spark - Correction",
    "section": "",
    "text": "Download this Jupyter Notebook\nFollow these instructions"
  },
  {
    "objectID": "docs/lab/lab1/lab1-SSPCloud-correction.html#before-you-start",
    "href": "docs/lab/lab1/lab1-SSPCloud-correction.html#before-you-start",
    "title": "First steps with Spark - Correction",
    "section": "",
    "text": "Download this Jupyter Notebook\nFollow these instructions"
  },
  {
    "objectID": "docs/lab/lab1/lab1-SSPCloud-correction.html#create-a-spark-session",
    "href": "docs/lab/lab1/lab1-SSPCloud-correction.html#create-a-spark-session",
    "title": "First steps with Spark - Correction",
    "section": "1 Create a Spark session",
    "text": "1 Create a Spark session\n\nExecute the following two cells to initialize a Spark session.\n\n\nimport os\nfrom pyspark.sql import SparkSession\n\nspark = (SparkSession\n         .builder\n         # default url of the internally accessed Kubernetes API\n         # (This Jupyter notebook service is itself a Kubernetes Pod)\n         .master(\"k8s://https://kubernetes.default.svc:443\")\n         # Executors spark docker image: for simplicity reasons, this jupyter notebook is reused\n         .config(\"spark.kubernetes.container.image\", os.environ['IMAGE_NAME'])\n         # Name of the Kubernetes namespace\n         .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\n         # Allocated memory to the JVM\n         # Stay careful, by default, the Kubernetes pods has a higher limit which depends on other parameters.\n         .config(\"spark.executor.memory\", \"4g\")\n         .config(\"spark.kubernetes.driver.pod.name\", os.environ['KUBERNETES_POD_NAME'])\n         # dynamic allocation configuration\n         .config(\"spark.dynamicAllocation.enabled\",\"true\")\n         .config(\"spark.dynamicAllocation.initialExecutors\",\"1\")\n         .config(\"spark.dynamicAllocation.minExecutors\",\"1\")\n         .config(\"spark.dynamicAllocation.maxExecutors\",\"60\")\n         # Ratio match the number of pods to create for a given number of parallel tasks\n         # (100 parallel, ratio of 1, one aims at 100 pods, with 0.5 it would be 50 pods)\n         .config(\"spark.dynamicAllocation.executorAllocationRatio\",\"1\")\n         .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",\"true\")\n         .getOrCreate()\n        )\n\n\nspark"
  },
  {
    "objectID": "docs/lab/lab1/lab1-SSPCloud-correction.html#first-steps-with-spark---data-importation",
    "href": "docs/lab/lab1/lab1-SSPCloud-correction.html#first-steps-with-spark---data-importation",
    "title": "First steps with Spark - Correction",
    "section": "2 First steps with Spark - Data importation",
    "text": "2 First steps with Spark - Data importation\nSpark’s main object class is the DataFrame, which is a distributed table. It is analogous to R’s or Python (Pandas)’s data frames:\n\none row represents an observation,\none column represents a variable.\n\nBut contrary to R or Python, Spark’s DataFrames can be distributed over hundred of nodes.\nSpark support multiple data formats, and multiple ways to load them.\n\ndata format : csv, json, parquet (an open source column oriented format)\ncan read archive files\nschema detection or user defined schema. For static data, like a json file, schema detection can be use with good results.\n\nSpark has multiple syntaxes to import data. Some are simple with no customisation, others are more complexes but you can specify options.\nThe simplest syntaxes to load a json or a csv file are :\n# JSON\njson_df = spark.read.json([location of the file])\n# csv\ncsv_df = spark.read.csv([location of the file])\nIn the future, you may consult the Data Source documentation to have the complete description of Spark’s reading abilities.\nThe data you will use in this lab are real data from the twitter sampled stream API and filtered stream API. The tweets folder contains more than 50 files and more than 2 million tweets. The tweets was collected between the 14/04/2021 and the 18/04/2021. The total collection time was less than 10 hours.\n\n\n2.1 ✍Hands-on 1 - Data importation\n\nLoad the json file stored here : s3a://ludo2ne/diffusion/tweets.jsonl.gz\n\nName you data frame df_tweet\nUse function cache() on the data frame. Caching is a performance optimization technique that allows you to persist an intermediate or final result of a computation in memory, reducing the need to recompute the data when it is accessed multiple time\n\n ⚙️ This file is an a JSONL (JSON-line) format, which means that each line of it is a JSON object. A JSON object is just a Python dictionary or a JavaScript object and looks like this: { key1: value1, key2: [\"array\", \"of\", \"many values]}). This file has been compressed into a GZ archive, hence the .jsonl.gz ending. Also this file is not magically appearing in your S3 storage. It is hosted on one of your teacher’s bucket and has been made public, so that you can access it.\n\nIt’s possible to load multiple file in a unique DataFrame. It’s useful when you have daily files and want to process them all. It’s the same syntax as the previous one, just specify a folder.\n\nIf you meet some issue to load this file, you can load and use your own file :\n\nIn Onyxia, mes fichiers\nLoad file tweets.jsonl.gz\nIn Jupyter, read it using s3a://&lt;user_name&gt;/tweets.jsonl.gz\n\n\n\n# DataFrame creation\ndf_tweet = spark.read.json(\"s3a://ludo2ne/diffusion/tweets.jsonl.gz\")\n\n# caching\ndf_tweet.cache()"
  },
  {
    "objectID": "docs/lab/lab1/lab1-SSPCloud-correction.html#data-frame-basic-manipulations",
    "href": "docs/lab/lab1/lab1-SSPCloud-correction.html#data-frame-basic-manipulations",
    "title": "First steps with Spark - Correction",
    "section": "3 Data frame basic manipulations",
    "text": "3 Data frame basic manipulations\nIf DataFrames are immutable, they can however be transformed in other DataFrames, in the sense that a modified copy is returned. Such transformations include: filtering, sampling, dropping columns, selecting columns, adding new columns…\nFirst, you can get information about the columns with:\ndf.columns       # get the column names\ndf.schema        # get the column names and their respective type\ndf.printSchema() # same, but human-readable\nYou can select columns with the select() method. It takes as argument a list of column name. For example :\ndf_with_less_columns = df\\\n  .select(\"variable3\",\"variable_four\",\"variable-6\")\n\n# Yes, you do need the ugly \\ at the end of the line,\n# if you want to chain methods between lines in Python\nYou can get nested columns easily with :\ndf.select(\"parentField.nestedField\")\nTo filter data you could use the filter() method. It take as input an expression that gets evaluated for each observation and should return a boolean. Sampling is performed with the sample() method. For example :\ndf_with_less_rows = df\\\n  .sample(fraction=0.001)\\\n  .filter(df.variable1==\"value\")\\\n  .show(10)\nAs said before your data are distributed over multiple nodes (executors) and data inside a node are split into partitions. Then each transformations will be run in parallel. They are called narrow transformation For example, to sample a DataFrame, Spark sample every partitions in parallel because sample all partition produce the sample DataFrame. For some transformations, like groupBy() it’s impossible, and it’s cannot be run in parallel.\n\n\n\n\n3.1 Lazy evaluation\nThis is because Spark has what is known as lazy evaluation, in the sense that it will wait as much as it can before performing the actual computation. Said otherwise, when you run an instruction such as:\ntweet_author_hashtags = df_tweet_big.select(\"auteur\",\"hashtags\")\n… you are not executing anything! Rather, you are building an execution plan, to be realised later.\nSpark is quite extreme in its laziness, since only a handful of methods called actions, by opposition to transformations, will trigger an execution. The most notable are:\n\ncollect(), explicitly asking Spark to fetch the resulting rows instead of to lazily wait for more instructions,\ntake(n), asking for n first rows\nfirst(), an alias for take(1)\nshow() and show(n), human-friendly alternatives\ncount(), asking for the numbers of rows\nall the “write” methods (write on file, write to database), see here for the list\n\nThis has advantages: on huge data, you don’t want to accidently perform a computation that is not needed. Also, Spark can optimize each stage of the execution in regard to what comes next. For instance, filters will be executed as early as possible, since it diminishes the number of rows on which to perform later operations. On the contrary, joins are very computation-intense and will be executed as late as possible. The resulting execution plan consists in a directed acyclic graph (DAG) that contains the tree of all required actions for a specific computation, ordered in the most effective fashion.\nThis has also drawbacks. Since the computation is optimized for the end result, the intermediate stages are discarded by default. So if you need a DataFrame multiple times, you have to cache it in memory because if you don’t Spark will recompute it every single time.\n\n\n\n3.2 ✍Hands-on 2 - Data frame basic manipulations\n\nHow many rows have your two DataFrame ?\n\n\ndf_tweet.count()\n\n\nDisplay columns names and then the schema\n\n\ndf_tweet.columns\n\n\ndf_tweet.printSchema()\n\n\nDisplay 10 rows of df_tweet\n\n\ndf_tweet.show(10)\n\n\nSample df_tweet and keep only 10% of it.\n\nCreate a new DataFrame named df_tweet_sampled.\nIf computations take too long on the full DataFrame, use this one instead or add a sample transformation in your expression.\n\n\n\ndf_tweet_sampled = df_tweet\\\n  .sample(fraction=0.1)\n\ndf_tweet_sampled.count()\n\n\nDefine a DataFrame tweet_author_hashtags with only the auteur and hashtags columns\n\nThen display 5 rows\n\n\n\ntweet_author_hashtags = df_tweet.select(\"auteur\", \"hashtags\")\n\ntweet_author_hashtags.show(5)\n\n\nPrint 5 lines of a df_tweet with only the auteur, mentions, and urls columns.\n\nmentions and urls are both nested columns in entities\n\n\n\ndf_tweet\\\n  .select(\"auteur\",\n          \"entities.mentions\",\n          \"entities.urls\")\\\n  .show(5)\n\n\nFilter df_tweet and keep only tweets with more than 1 like.\n\nDisplay only auteur, contenu and like_count\nPrint 10 lines\n\n\n\ndf_tweet\\\n  .filter(df_tweet.like_count &gt;= 1)\\\n  .show(10)"
  },
  {
    "objectID": "docs/lab/lab1/lab1-SSPCloud-correction.html#basic-dataframe-column-manipulation",
    "href": "docs/lab/lab1/lab1-SSPCloud-correction.html#basic-dataframe-column-manipulation",
    "title": "First steps with Spark - Correction",
    "section": "4 Basic DataFrame column manipulation",
    "text": "4 Basic DataFrame column manipulation\nYou can add/update/rename column of a DataFrame using spark :\n\nDrop : df.drop(columnName : str )\nRename : df.withColumnRenamed(oldName : str, newName : str)\nAdd/update : df.withColumn(columnName : str, columnExpression)\n\nFor example\n\ndf_tweet\\\n  .withColumn(                                        # computes new variable\n    \"like_rt_ratio\",                                  # like_rt_ratio \"OVERCONFIDENCE\"\n    df_tweet.like_count /df_tweet.retweet_count)\n\nSee here for the list of all functions available in an expression.\n\n4.1 ✍Hands-on 3 - Basic DataFrame column manipulation\n\nDefine a DataFrame with a column names interaction_count named df_tweet_interaction_count\n\nThis column is the sum of like_count, reply_count and retweet_count.\n\n\n\ndf_tweet_interaction_count = df_tweet\\\n  .drop(\"other\")\\\n  .filter(df_tweet.like_count &gt;= 1)\\\n  .filter(df_tweet.retweet_count &gt;= 1)\\\n  .withColumn(\n    \"interaction_count\",\n    df_tweet.like_count + df_tweet.reply_count + df_tweet.retweet_count)\n\ndf_tweet_interaction_count\\\n  .show(10)\n\n\nUpdate the DataFrame you imported at the beginning of this lab and drop the other column\n\n\ndf_tweet\\\n  .drop(\"other\")\\\n  .show(10)"
  },
  {
    "objectID": "docs/lab/lab1/lab1-SSPCloud-correction.html#advance-dataframe-column-manipulation",
    "href": "docs/lab/lab1/lab1-SSPCloud-correction.html#advance-dataframe-column-manipulation",
    "title": "First steps with Spark - Correction",
    "section": "5 Advance DataFrame column manipulation",
    "text": "5 Advance DataFrame column manipulation\n\n5.1 Array manipulation\nSome columns often contain arrays (lists) of values instead of just one value. This may seem surprising but this actually quite natural. For instance, you may create an array of words from a text, or generate a list of random numbers for each observation, etc.\nYou may create array of values with:\n\nsplit(text : string, delimiter : string), turning a text into an array of strings\n\nYou may use array of values with:\n\nsize(array : Array), getting the number of elements\narray_contains(inputArray : Array, value : any), checking if some value appears\nexplode(array : Array), unnesting an array and duplicating other values. For instance if you use explode() over the hashtags value of this DataFrame:\n\n\n\nAuteur\nContenu\nHashtags\n\n\n\n\nBob\nI love #Spark and #bigdata\n[Spark, bigdata]\n\n\nAlice\nJust finished #MHrise, best MH ever\n[MHrise]\n\n\n\nYou will get :\n\n\n\n\n\n\n\n\n\nAuteur\nContenu\nHashtags\nHashtag\n\n\n\n\nBob\nI love #Spark and #bigdata\n[Spark, bigdata]\nSpark\n\n\nBob\nI love #Spark and #bigdata\n[Spark, bigdata]\nbigdata\n\n\nAlice\nJust finished #MHrise, best MH ever\n[MHrise]\nMHrise\n\n\n\n\nAll this functions must be imported first :\nfrom pyspark.sql.functions import split, explode, size, array_contains\nDo not forget, to create a new column, you should use withColumn(). For example :\ndf.withColumn(\"new column\", explode(\"array\"))\n\n✍Hands-on 4 - Array manipulation\n\nKeep all the tweets with hashtags and for each remaining line, split the hashtag text into an array of hashtags\n\n\nfrom pyspark.sql.functions import split, explode, size, array_contains\n\ndf_tweet\\\n  .filter(size(\"hashtags\") &gt; 0)\\\n  .withColumn(\n    \"hashtag_exploded\",\n    explode(\"hashtags\"))\\\n  .show(10)\n\n\nCreate a new column with the number of words of the contenu column. (Use split() + size())\n\n\ndf_tweet\\\n  .withColumn(\"word_count\",\n              size(split(\"contenu\", \" \")))\\\n  .show(5)\n\n\nCount how many tweet contain the Ukraine hashtag (use the count() action)\n\n\ndf_tweet\\\n  .filter(array_contains(\"hashtags\", \"Ukraine\"))\\\n  .count()\n\n\n\n\n5.2 User defined function\nFor more very specific column manipulation you will need Spark’s udf() function (User Defined Function). It can be useful if Spark does not provide a feature you want. But Spark is a popular and active project, so before coding an udf, go check the documentation. For instance for natural language processing, Spark already has some functions. Last things, python udf can lead to performance issues (see https://stackoverflow.com/a/38297050) and learning a little bit of scala or java can be a good idea.\nFor example :\n# !!!! DOES NOT WORK !!!!\ndef to_lower_case(string):\n    return string.lower()\n\ndf.withColumn(\"tweet_lower_case\", to_lower_case(df.contenu))\nwill just crash. Keep in mind that Spark is a distributed system, and that Python is only installed on the central node, as a convenience to let you execute instructions on the executor nodes. But by default, pure Python functions can only be executed where Python is installed! We need udf() to enable Spark to send Python instructions to the worker nodes.\nLet us see how it is done :\n# imports\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.functions import explode\nfrom pyspark.sql.types import StringType\n\n# pure python functions\ndef to_lower_case(string):\n    return string.lower()\n\n# user defined function(we use a lambda function to create the udf)\nto_lower_case_udf = udf(\n    lambda x: to_lower_case(x), StringType()\n)\n\n# df manipulation\ndf_tweet_small\\\n  .select(\"auteur\",\"hashtags\")\\\n  .filter(\"size(hashtags)!=0\")\\\n  .withColumn(\"hashtag\", explode(\"hashtags\"))\\\n  .withColumn(\"hashtag\", to_lower_case_udf(\"hashtag\")).show(10)\n\n\n✍Hands-on 5 - User defined function\n\nCreate an user defined function that counts how many words a tweet contains.\n\nyour function will return an IntegerType and not a StringType\n\n\n\n# imports\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import IntegerType\n\n# pure python functions\ndef word_count(string):\n    return len(string.split(\" \"))\n\n# user definid function\nword_count_udf = udf(\n    lambda x: word_count(x), IntegerType()\n)\n\n# df manipulation\ndf_tweet\\\n  .withColumn(\"word_count\",\n              word_count_udf(\"contenu\"))\\\n  .show(10)"
  },
  {
    "objectID": "docs/lab/lab1/lab1-SSPCloud-correction.html#aggregation-functions",
    "href": "docs/lab/lab1/lab1-SSPCloud-correction.html#aggregation-functions",
    "title": "First steps with Spark - Correction",
    "section": "6 Aggregation functions",
    "text": "6 Aggregation functions\nSpark offer a variety of aggregation functions :\n\ncount(column : string) will count every not null value of the specify column. You cant use count(1) of count(\"*\") to count every line (even row with only null values)\ncountDisctinct(column : string) and approx_count_distinct(column : string, percent_error: float). If the exact number is irrelevant, approx_count_distinct()should be preferred.\nCounting distinct elements cannot be done in parallel, and need a lot data transfer. But if you only need an approximation, there is a algorithm, named hyper-log-log (more info here) that can be parallelized.\nfrom pyspark.sql.functions import count, countDistinct, approx_count_distinct\n\ndf.select(count(\"col1\")).show()\ndf.select(countDistinct(\"col1\")).show()\ndf.select(approx_count_distinct(\"col1\"), 0.1).show()\nYou have access to all other common functions min(), max(), first(), last(), sum(), sumDistinct(), avg() etc (you should import them first from pyspark.sql.functions import min, max, avg, first, last, sum, sumDistinct)\n\n\n\n6.1 ✍Hands-on 6 - Aggregation functions\n\nWhat are the min, max, average of interaction_count (use df_tweet_interaction_count created earlier)\n\ndon’t forget to import the required functions\n\n\n\nfrom pyspark.sql.functions import min, max, avg, first, last, sum, sumDistinct\n\ndf_tweet_interaction_count\\\n  .select(min(\"interaction_count\"),\n          max(\"interaction_count\"),\n          avg(\"interaction_count\"))\\\n  .first()\n\n\nHow many tweets have hashtags ?\n\nDistinct hashtags ?\nTry the approximative count with 0.1 and 0.01 as maximum estimation error allowed.\n\n\n\nfrom pyspark.sql.functions import count, countDistinct, approx_count_distinct\n\ndf_tweet\\\n  .select(count(\"hashtags\"),\n          countDistinct(\"hashtags\"),\n          approx_count_distinct(\"hashtags\", 0.1),\n          approx_count_distinct(\"hashtags\",0.01))\\\n  .show()"
  },
  {
    "objectID": "docs/lab/lab1/lab1-SSPCloud-correction.html#grouping-functions",
    "href": "docs/lab/lab1/lab1-SSPCloud-correction.html#grouping-functions",
    "title": "First steps with Spark - Correction",
    "section": "7 Grouping functions",
    "text": "7 Grouping functions\nLike SQL you can group row by a criteria with Spark. Just use the groupBy(column : string) method. Then you can compute some aggregation over those groups.\ndf\\\n  .groupBy(\"col1\")\\\n  .agg(count(\"col2\").alias(\"quantity\"))\\           # alias is use to specify the name of the new column\n  .show()\nThe agg() method can take multiples argument to compute multiple aggregation at once.\ndf\\\n  .groupBy(\"col1\")\\\n  .agg(count(\"col2\").alias(\"quantity\"),\n       min(\"col2\").alias(\"min\"),\n       avg(\"col3\").alias(\"avg3\"))\\\n  .show()\nAggregation and grouping transformations work differently than the previous method like filter(), select(), withColumn() etc. Those transformations cannot be run over each partitions in parallel, and need to transfer data between partitions and executors. They are called “wide transformations”\n\n\n7.1 ✍Hands-on 7 - Grouping functions\n\nCompute a daframe with the min, max and average retweet of each auteur.\n\nThen order it by the max number of retweet in descending order by .\nTo do that you can use the following syntax\n\nfrom pyspark.sql.functions import desc\ndf.orderBy(desc(\"col\"))\n\n\nfrom pyspark.sql.functions import desc\n\ndf_tweet\\\n  .groupBy(\"auteur\")\\\n  .agg(min(\"retweet_count\").alias(\"min_RT\"),\n       max(\"retweet_count\").alias(\"max_RT\"),\n       avg(\"retweet_count\").alias(\"avg_RT\"))\\\n  .orderBy(desc(\"max_RT\"))\\\n  .show(5)"
  },
  {
    "objectID": "docs/lab/lab1/lab1-SSPCloud-correction.html#spark-sql",
    "href": "docs/lab/lab1/lab1-SSPCloud-correction.html#spark-sql",
    "title": "First steps with Spark - Correction",
    "section": "8 Spark SQL",
    "text": "8 Spark SQL\nSpark understand SQL statement. It’s not a hack nor a workaround to use SQL in Spark, it’s one a the more powerful feature in Spark. To use SQL you will need :\n\nRegister a view pointing to your DataFrame\nmy_df.createOrReplaceTempView(viewName : str)\nUse the sql function\nspark.sql(\"\"\"\nYour SQL statement\n\"\"\")\nYou could manipulate every registered DataFrame by their view name with plain SQL.\n\nIn fact you can do most of this tutorial without any knowledge in PySpark nor Spark. Many things can only be done in Spark if you know SQL and how to use it in Spark.\n\n8.1 ✍Hands-on 8 - Spark SQL\n\nHow many tweets have hashtags ?\n\nDistinct hashtags ?\n\n\n\ndf_tweet\\\n  .select(\"contenu\", \"hashtags\")\\\n  .createOrReplaceTempView(\"view_hashtag_content\")\n\nspark.sql(\"\"\"\nSELECT COUNT(*),\n       COUNT(DISTINCT(contenu))\n  FROM view_hashtag_content\n WHERE size(hashtags) &gt; 0\n\"\"\")\\\n  .show(5)\n\n\nCompute a dataframe with the min, max and average retweet of each auteur using Spark SQL\n\n\ndf_tweet.createOrReplaceTempView(\"view_tweet\")\n\nspark.sql(\"\"\"\nSELECT min(retweet_count),\n       max(retweet_count),\n       avg(retweet_count)\n  FROM view_tweet\n GROUP BY auteur\n ORDER BY MAX(retweet_count) DESC\n\"\"\")\\\n  .show(5)"
  },
  {
    "objectID": "docs/lab/lab1/lab1-SSPCloud-correction.html#delete-the-jupyter-pyspark-service",
    "href": "docs/lab/lab1/lab1-SSPCloud-correction.html#delete-the-jupyter-pyspark-service",
    "title": "First steps with Spark - Correction",
    "section": "9 Delete the Jupyter-pyspark service",
    "text": "9 Delete the Jupyter-pyspark service\n\nExport your notebook\n\nRight clic and Download (.ipynb)\nFile &gt; Save and Export Notebook &gt; HTML\n\nhttps://datalab.sspcloud.fr/my-services"
  },
  {
    "objectID": "docs/lab/lab3/lab3.html",
    "href": "docs/lab/lab3/lab3.html",
    "title": "Stream processing with Spark",
    "section": "",
    "text": "This tutorial will teach you the basic of stream processing with Spark. As soon as an application compute something with business value (for instance customer activity), and new inputs arrive continuously, companies will want to compute this result continuously too. Spark makes possible to process stream with the Structured Streaming API. This lab will teach you the basics of this Spark’s API. Because the Structured Streaming API is based on the DataFrame API most syntaxes of tutorial 1 are still relevant."
  },
  {
    "objectID": "docs/lab/lab3/lab3.html#outline",
    "href": "docs/lab/lab3/lab3.html#outline",
    "title": "Stream processing with Spark",
    "section": "",
    "text": "This tutorial will teach you the basic of stream processing with Spark. As soon as an application compute something with business value (for instance customer activity), and new inputs arrive continuously, companies will want to compute this result continuously too. Spark makes possible to process stream with the Structured Streaming API. This lab will teach you the basics of this Spark’s API. Because the Structured Streaming API is based on the DataFrame API most syntaxes of tutorial 1 are still relevant."
  },
  {
    "objectID": "docs/lab/lab3/lab3.html#spark-cluster-creation-in-aws",
    "href": "docs/lab/lab3/lab3.html#spark-cluster-creation-in-aws",
    "title": "Stream processing with Spark",
    "section": "1. Spark cluster creation in AWS",
    "text": "1. Spark cluster creation in AWS\n⚠️ DO NOT FORGET TO TURN YOUR CLUSTER OFF A THE END OF THIS TUTORIAL!\nInstructions are at the beginning of lab 2. Or you can just clone your cluster"
  },
  {
    "objectID": "docs/lab/lab3/lab3.html#notebook-configuration",
    "href": "docs/lab/lab3/lab3.html#notebook-configuration",
    "title": "Stream processing with Spark",
    "section": "2. Notebook configuration",
    "text": "2. Notebook configuration\nBecause Spark Streaming need to write some data on the underlying HDFS cluster, we need to configure our cluster. The cluster configuration will be a little bit more complexe than the previous ones. 1. You need to select a SSH key to your cluster 2. Once you cluster is running, go to the EC2 dashboard like in lab 0 3. Click on the instance with the security groups : ElasticMapReduceEditors-Livy,ElasticMapReduce-master 4. Go to the security tab 5. Click on the security group ElasticMapReduce-master 6. Clink on edit inbound rules (Modifier les règles entrantes en français) 7. Add a rule SSH, to all IPv4 8. Save your rules 9. Go bask to the EC2 instance with the security group ElasticMapReduce-master 10. Click on Connect ( Se connecter en français) 11. Again clik on Connect 12. In the cloud shell copy/paste : sudo usermod -a -G hdfsadmingroup livy. It will give to the the user “livy” admin right.\n# Configuraion\n# The user pay the data transfer\nspark._jsc.hadoopConfiguration().set(\"fs.s3.useRequesterPaysHeader\",\"true\")\n\n# Set the number of shufflre partitions\nspark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n\n# Import all the needed library\nfrom time import sleep\nfrom pyspark.sql.functions import from_json, window, col, expr, size, explode, avg, min, max\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType, ArrayType, TimestampType, BooleanType, LongType, DoubleType\nExplanation:\n\nspark._jsc.hadoopConfiguration().set(\"fs.s3.useRequesterPaysHeader\",\"true\") : likes in lab2, you will be charged for the data transfer. without this configuration you can’t access the data.\nspark.conf.set(\"spark.sql.shuffle.partitions\", 5) : set the number of partitions for the shuffle phase. A partition is in Spark the name of a bloc of data. By default Spark use 200 partitions to shuffle data. But in this lab, our mini-batch will be small, and to many partitions will lead to performance issues.\nspark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n\n🤔 The shuffle dispatches data according to their key between a map and a reduce phase. For instance, if you are counting how many records have each g group, the map phase involve counting each group member in each Spark partition : {g1:5, g2:10, g4:1, g5:3} for one partition, {g1:1, g2:2, g3:23, g5:12} for another. The shuffle phase dispatch those first results and group them by key in the same partition, one partition gets {g1:5, g1:1, g2:10, g2:2}, the other gets : {g4:1, g5:3, g3:23, g5:12} Then each reduce can be done efficiently.\n\n📦 Import all needed library\nfrom time import sleep\nfrom pyspark.sql.functions import from_json, window, col, expr\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType, ArrayType, TimestampType, BooleanType, LongType, DoubleType"
  },
  {
    "objectID": "docs/lab/lab3/lab3.html#stream-processing",
    "href": "docs/lab/lab3/lab3.html#stream-processing",
    "title": "Stream processing with Spark",
    "section": "3. Stream processing",
    "text": "3. Stream processing\nStream processing is the act to process data in real-time. When a new record is available, it is processed. There is no real beginning nor end to the process, and there is no “result”. The result is updated in real time, hence multiple versions of the results exist. For instance, you want to count how many tweet about cat are posted in twitter every hour. Until the end of an hour, you do not have you final result. And even at this moment, your result can change. Maybe some technical problems created some latency and you will get some tweets later. And you will need to update your previous count.\nSome commons use cases of stream processing are :\n\nNotifications and alerting : real-time bank fraud detection ; electric grid monitoring with smart meters ; medical monitoring with smart meters, etc.\nReal time reporting: traffic in a website updated every minute; impact of a publicity campaign ; stock option portfolio, etc.\nIncremental ELT (extract transform load): new unstructured data are always available and they need to be processed (cleaned, filtered, put in a structured format) before their integration in the company IT system.\nOnline machine learning : new data are always available and used by a ML algorithm to improve its performance dynamically.\n\nUnfortunately, stream processing has some issues. First because there is no end to the process, you cannot keep all the data in memory. Second, process a chain of event can be complex. How do you raise an alert when you receive the value 5, 6 and 3 consecutively ? Don’t forget you are in a distributed environment, and there is latency. Hence, the received order can be different from the emitted order."
  },
  {
    "objectID": "docs/lab/lab3/lab3.html#spark-and-stream-processing",
    "href": "docs/lab/lab3/lab3.html#spark-and-stream-processing",
    "title": "Stream processing with Spark",
    "section": "4. Spark and stream processing",
    "text": "4. Spark and stream processing\nStream processing was gradually incorporated in Spark. In 2012 Spark Streaming and it’s DStreams API was added to Spark (it was before an external project). This made it possible use high-level operator like map and reduce to process stream of data. Because of its implementation, this API has some limitations, and its syntax was different from the DataFrame one. Thus, in 2016 a new API was added, the Structured Streaming API. This API is directly build built on DataFrame, unlike DStreams. This has an advantage, you can process your stream like static data. Of course there are some limitations, but the core syntaxes is the same. You will chain transformations, because each transformation takes a DataFrame as input and produces a DataFrame as output. The big change is there is no action at the end, but an output sink.\n\n\n\ndata stream\n\n\nFigure 1 : data stream representation (source structured streaming programming guide)\nSpark offer two ways to process stream, one record at a time, or processing micro batching (processing a small amount of line at once).\n\none record at a time every time a new record is available it’s processed. This has a big advantage, it achieves very low latency . But there is a drawback, the system can not handle too much data at the same time (low throughput). It’s the default mode. Because in this lab, you will process files with record, even if you will process one file at a time, you will process mini batch of records\nas for micro batching it process new records every t seconds. Hence records are not process really in “real-time”, but periodically, the latency will be higher, and so the throughput. Unless you really need low latency, make it you first choice option.\n\n\n🧐 To get the best ratio latency/throughput, a good practice is to decrease the micro-batch size until the mini-batch throughput is the same as the input throughput. Then increase the size to have some margin\n\n\nFigure 2 : which Spark solution suit best giving latency requirement (source : Learning Spark, O’Reilly)\n\nTo understand why processing one record at a time has lower latency and throughput than batch processing, imagine a restaurant. Every time a client order something the chef cooks its order independently of the other current orders. So if two clients order pizza, the chief makes two small doughs, and cook them individually. If clients there is only a few clients, the chief can finish each order before a new client comes. The latency is the lowest possible when the chief is idle when a client come. Know imagine a restaurant were the chief process the orders by batch. He waits some minutes to gather all the orders than he mutualizes the cooking. If there are 5 pizza orders, he only does one big dough, divides it in five, add the toppings then cook all five at once. The latency is higher because the chief waits before cooking, but so the throughput because he can cook multiple things at once."
  },
  {
    "objectID": "docs/lab/lab3/lab3.html#the-basics-of-sparks-structured-streaming",
    "href": "docs/lab/lab3/lab3.html#the-basics-of-sparks-structured-streaming",
    "title": "Stream processing with Spark",
    "section": "5. The basics of Spark’s Structured Streaming",
    "text": "5. The basics of Spark’s Structured Streaming\n\n5.1. The different sources for stream processing in Spark\nIn lab 2 you discovered Spark DataFrame, in this lab you will learn about Structured Streaming. It’s a stream processing framework built on the Spark SQL engine, and it uses the existing structured APIs in Spark. So one you define a way to read a stream, you will get a DataFrame. Like in lab2 ! So except state otherwise, all transformations presented in lab2 are still relevant in this lab.\nSpark Streaming supports several input source for reading in a streaming fashion :\n\nApache Kafka an open-source distributed event streaming platform (not show in this lab)\nFiles on distributed file system like HDFS or S3 (Spark will continuously read new files in a directory)\nA network socket : an end-point in a communication across a network (sort of very simple webservice). It’s not recommend for production application, because a socket connection doesn’t provide any mechanism to check the consistency of data.\n\nDefining an input source is like loading a DataFrame but, you have to replace spark.read by spark.readStream. For instance, if I want to open a stream to a folder located in S3 you have to read every new files put in it, just write\nmy_first_stream = spark\\\n.readStream\\\n.schema(schema_tweet)\\\n.json(\"s3://my-awesome-bucket/my-awesome-folder\")\nThe major difference with lab2, it is Spark cannot infer the schema of the stream. You have to pass it to Spark. There is two ways :\n\nA reliable way : you define the schema by yourself and gave it to Spark\nA quick way : you load one file of the folder in a DataFrame, extract the schema and use it. It works, but the schema can be incomplete. It’s a better solution to create the schema by hand and use it.\n\nFor Apache Kafka, or socket , it’s a slightly more complex, (not used today, it’s jute for you personal knowledge) :\nmy_first_stream = spark\\\n.readStream\\\n.format(\"kafka\")\n.option(\"kafka.bootstrat.servers\", \"host1:port1, host2:port2 etc\")\n.option(\"subscribePattern\", \"topic name\")\n.load()\n\n5.1.1 Why is a folder a relevant source in stream processing ?\nPreviously, in lab 1, you loaded all the files in a folder stored in S3 with Spark. And it worked pretty well. But this folder was static, in other words, Its content didn’t change. But in some cases, new data are constantly written into a folder. For instance, in this lab you will process a stream of tweets. A python script is running in a EC2 machine reading tweets from the Twitter’s web service and writing them in a S3 buckets. Every 2 seconds or so, a new file is added to the bucket with 1000 tweets. If you use DataFrame like in lab 1, your process cannot proceed those new files. You should relaunch your process every time. But with Structured Streaming Spark will dynamically load new files.\n\nFigure 3 : Complete lab architecture to stream process tweets\n\nThe remaining question is, why don’t we connect Spark to the twitter webservice directly ? And the answer is : we can’t. Spark cannot be connected to a webservice directly. You need a middle-man between Spark and a webservice. There are multiple solutions, but an easy and reliable one is to write tweet to s3 (because we use AWS services, if you use Microsoft Azure, Google Cloud Platform or OVH cloud replace S3 by their storage service).\n\n\n\n\n✍ Open a stream\nLike in lab 1, you will use tweets in this lab. The tweets are stored in jsonl file (json line every line of the file is a complete json). Here is an example. The schema changed a little, because this time tweets aren’t pre-processed.\n{\n    \"data\": {\n        \"public_metrics\": {\n            \"retweet_count\": 0,\n            \"reply_count\": 0,\n            \"like_count\": 0,\n            \"quote_count\": 0\n        },\n        \"text\": \"Day 93. Tweeting every day until Colby cheez its come back #bringcolbyback @cheezit\",\n        \"possibly_sensitive\": false,\n        \"created_at\": \"2021-05-03T07:55:46.000Z\",\n        \"id\": \"1389126523853148162\",\n        \"entities\": {\n            \"annotations\": [\n                {\n                    \"start\": 33,\n                    \"end\": 43,\n                    \"probability\": 0.5895,\n                    \"type\": \"Person\",\n                    \"normalized_text\": \"Colby cheez\"\n                }\n            ],\n            \"mentions\": [\n                {\n                    \"start\": 75,\n                    \"end\": 83,\n                    \"username\": \"cheezit\"\n                }\n            ],\n            \"hashtags\": [\n                {\n                    \"start\": 59,\n                    \"end\": 74,\n                    \"tag\": \"bringcolbyback\"\n                }\n            ]\n        },\n        \"lang\": \"en\",\n        \"source\": \"Twitter for iPhone\",\n        \"author_id\": \"606856313\"\n    },\n    \"includes\": {\n        \"users\": [\n            {\n                \"created_at\": \"2012-06-13T03:36:00.000Z\",\n                \"username\": \"DivinedHavoc\",\n                \"verified\": false,\n                \"name\": \"Justin\",\n                \"id\": \"606856313\"\n            }\n        ]\n    }\n}\n\nDefine a variable with this schema (you will find a file schema pyspark tweet on moodle with the schema to copy /aste)\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType, ArrayType, TimestampType, BooleanType, LongType, DoubleType\n\nStructType([\n  StructField(\"data\", StructType([\n      StructField(\"author_id\",StringType(),True),\n      StructField(\"text\",StringType(),True),\n      StructField(\"source\",StringType(),True),\n      StructField(\"lang\",StringType(),True),\n      StructField(\"created_at\",TimestampType(),True),\n      StructField(\"entities\",StructType([\n          StructField(\"annotations\", ArrayType(StructType([\n              StructField(\"end\", LongType(), True),\n              StructField(\"normalized_text\", StringType(), True),\n              StructField(\"probability\", DoubleType(), True),\n              StructField(\"start\", LongType(), True),\n              StructField(\"type\", StringType(), True)\n          ]),True),True),\n          StructField(\"cashtags\", ArrayType(StructType([\n              StructField(\"end\", LongType(), True),\n              StructField(\"start\", LongType(), True),\n              StructField(\"tag\", StringType(), True)\n          ]),True),True),\n           StructField(\"hashtags\", ArrayType(StructType([\n              StructField(\"end\", LongType(), True),\n              StructField(\"start\", LongType(), True),\n              StructField(\"tag\", StringType(), True)\n          ]),True),True),\n          StructField(\"mentions\", ArrayType(StructType([\n              StructField(\"end\", LongType(), True),\n              StructField(\"start\", LongType(), True),\n              StructField(\"username\", StringType(), True)\n          ]),True),True),\n          StructField(\"urls\", ArrayType(StructType([\n              StructField(\"description\", StringType(), True),\n              StructField(\"display_url\", StringType(), True),\n              StructField(\"end\", LongType(), True),\n              StructField(\"expanded_url\", StringType(), True),\n              StructField(\"images\", ArrayType(StructType([\n                      StructField(\"height\", LongType(), True),\n                      StructField(\"url\", StringType(), True),\n                      StructField(\"width\", LongType(), True)\n                  ]),True),True),\n              StructField(\"start\", LongType(), True),\n              StructField(\"status\", LongType(), True),\n              StructField(\"title\", StringType(), True),\n              StructField(\"unwound_url\", StringType(), True),\n              StructField(\"url\", StringType(), True),\n          ]),True),True),\n      ]),True),\n      StructField(\"public_metrics\", StructType([\n          StructField(\"like_count\", LongType(), True),\n          StructField(\"reply_count\", LongType(), True),\n          StructField(\"retweet_count\", LongType(), True),\n          StructField(\"quote_count\", LongType(), True),\n      ]),True)\n    ]),True),\n  StructField(\"includes\", StructType([\n      StructField(\"users\", ArrayType(StructType([\n          StructField(\"created_at\", TimestampType(), True),\n          StructField(\"id\", StringType(), True),\n          StructField(\"name\", StringType(), True),\n          StructField(\"username\", StringType(), True),\n          StructField(\"verified\", BooleanType(), True)\n      ]),True),True)\n  ]),True)\n  ])\nCrate a stream to this s3 bucket : s3://spark-lab-input-data-ensai20212022/stream_tweet/. Name it tweet_stream\n\n🤔 Nothing happen ? It’s normal ! Do not forget, Spark use lazy evaluation. It doesn’t use data if you don’t define an action. For now Spark only know how to get the stream, that’s all.\n\nIn a cell just execute tweet_stream. It should print the type of tweet_stream and the associated schema. You can see you created a DataFrame like in lab2 !\nTo print the size of your DataFrame with this piece of code :\nstream_size_query= tweet_stream\\\n.writeStream\\\n.queryName(\"stream_size\")\\\n.format(\"memory\")\\\n.start()\n\nfor _ in range(10): # we use an _ because the variable isn't used. You can use i if you prefere\n    sleep(3)\n    spark.sql(\"\"\"\n      SELECT count(1) FROM stream_size\n    \"\"\").show()\nstream_size_query.stop() #needed to close the query\n\n\n\n5.2 How to output a stream ?\nRemember, Spark has two types of methods to process DataFrame:\n\nTransformations which take a DataFrame has input and produce an other Dataframe\nAnd actions, which effectively run computation and produce something, like a file, or a output in you notebook/console.\n\nStream processing looks the same as DataFrame processing. Hence, you still have transformations, the exact same one that can be apply on classic DataFrame (with some restriction, for example you can not sample a stream with the sample() transformation). The action part is a little different. Because a stream runs continuously, you cannot just print the data or run a count at the end of the process. In fact actions will nor work on stream. To tackle this issue, Spark proposes different outputs sinks. An output sink is a possible output for your stream. The different output sink are (this part came from the official Spark documentation) :\n\nFile sink - Stores the output to a file. The file can be stored locally (on the cluster), remotely (on S3). The file format can be json, csv etc\n\nwriteStream\\\n.format('json')\\\n.option(\"checkpointLocation\", \"output_folder/history\") \\\n.option(\"path\", \"output_folder\")\\\n.start()\n\nKafka sink - Stores the output to one or more topics in Kafka.\nForeach sink - Runs arbitrary computation on the records in the output. It does not produce an DataFrame. Each processed lines lost\n\nwriteStream\n    .foreach(...)\n    .start()\n\nConsole sink (for debugging) - Prints the output to the console standard output (stdout) every time there is a trigger. This should be used for debugging purposes on low data volumes as the entire output is collected and stored in the driver’s memory after every trigger. Sadly console sink does not work with jupyter notebook.\n\nwriteStream\n    .format(\"console\")\n    .start()\n\nMemory sink (for debugging) - The output is stored in memory as an in-memory table. This should be used for debugging purposes on low data volumes as the entire output is collected and stored in the driver’s memory. Hence, use it with caution. Because we are in a simple lab, you will use this solution. But keep in mind it’s a very bad idea because data must fit in the the ram of the driver node. And in a big data context it’s impossible. Because it’s not a big data problem if one computer can tackle it.\n\nwriteStream\n    .format(\"memory\")\n    .queryName(\"tableName\") # to resquest the table with spark.sql()\n    .start()\nWe just talked where we can output a stream, but there is another question, how ?\nTo understand why it’s a issue, let’s talk about two things that spark can do with streams : filter data and group by + aggregation\n\nFilter : your process is really simple. Every time you get a new data you just compute a score and drop records with a score less than a threshold. Then you write into a file every kept record. In a nutshell, you just append new data to a file. Spark does not have to read an already written row, it just add new data.\nGroup by + aggregation : in this case you want to group by your data by key than compute a simple count. Then you want to write the result in a file. But now there is an issue, Spark needs to update some existing rows in your file every time it writes somethings. But is your file is stored in HDFS of S3, it’s impossible to update in a none append way a file. In a nutshell, it’s impossible to output in a file your operation.\n\nTo deal with this issue, Spark proposes 3 mode. And you cannot use every mode with every output sink, with every transformation. The 3 modes are (more info here) :\n\nAppend mode (default) - This is the default mode, where only the new rows added to the Result Table since the last trigger will be outputted to the sink. This is supported for only those queries where rows added to the Result Table is never going to change. Hence, this mode guarantees that each row will be output only once (assuming fault-tolerant sink). For example, queries with only select, where, map, flatMap, filter, join, etc. will support Append mode.\nComplete mode - The whole Result Table will be outputted to the sink after every trigger. This is supported for aggregation queries.\nUpdate mode - (Available since Spark 2.1.1) Only the rows in the Result Table that were updated since the last trigger will be outputted to the sink. More information to be added in future releases.\n\n\n\n\nSink\nSupported Output Modes\n\n\n\n\nFile Sink\nAppend\n\n\nKafka Sink\nAppend, Update, Complete\n\n\nForeach Sink\nAppend, Update, Complete\n\n\nForeachBatch Sink\nAppend, Update, Complete\n\n\nConsole Sink\nAppend, Update, Complete\n\n\nMemory Sink\nAppend, Complete\n\n\n\n\n\n5.3 How to output a stream : summary\nTo sum up to output a stream you need\n\nDataFrame (because once load a stream is a DataFrame)\nA format for your output, like console to print in console, memory to keep the Result Table in memory, json to write it to a file etc\nA mode to specify how the Result Table will be updated.\n\nFor instance for the memory sink\nmemory_sink = df\\\n.writeStream\\\n.queryName(\"my_awesome_name\")\\\n.format('memory')\\\n.outputMode(\"complete\" or \"append\")\\\n.start() #needed to start the stream\n\nFigure 4 : The different phases of stream processing in Spark\n\n\n✍ Output a stream\n\nLang count\n\nCompute a DataFrame that group and count data by the lang column. Name your DataFrame lang_count\nUse this DataFrame to create a output stream with the following configuration :\n\nNames the variable lang_query\nMemory sink\nComplete mode (because we are doing an agregation)\nName you query lang_count\n\nThen past this code\nfor _ in range(10): # we use an _ because the variable isn't use. You can use i if you prefere\n    sleep(3)\n    spark.sql(\"\"\"\n    SELECT * FROM lang_count\"\"\").show()\nlang_query.stop() #needed to close the stream\nAfter 30 seconds, 10 tables will appeared in your notebook. Each table represents the contain of lang_count at a certain time. The .stop() method close the stream.\nIn the rest of this tutorial, to will need two steps to print data :\n\nDefine a stream query with a memory sink\nRequest this stream with the spark.sql() function\n\nInstead of a for loop, you can just write you spark.sql() statement in a cell and rerun it. In this case you will need a third cell with a stop() method to close your stream.\nFor instance:\nCell 1 python       my_query = my_df\\           .writeStream\\           .format(\"memory\")\\           .queryName(\"query_table\")\\           .start()\nCell 2 python       spark.sql(\"SELECT * FROM query_table\").show()\nCell 3 python       my_query.stop()\n\n#### ❌ Count tweets with and without hashtag\n\nAdd a column has_hashtag to your DataFrame. This column equals True if data.entities.hashtags is not null. Else it’s false. Use the withColumn transformation to add a column. You can count the size of data.entities.hashtags to check if it’s empty or not.\nGroup and count by the has_hashtag column\nPrint some results\n\n\n\n\nDebugging tip\nIf at any moment of this lab you encounter an error like this one :\n'Cannot start query with name has_hashtag as a query with that name is already active'\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py\", line 1109, in start\n    return self._sq(self._jwrite.start())\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 79, in deco\n    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)\npyspark.sql.utils.IllegalArgumentException: 'Cannot start query with name has_hashtag as a query with that name is already active'\nRun in a cell the following code :\nfor stream in spark.streams.active:\n    stream.stop()\nspark.streams.active returns an array with all the active stream, and the code loops over all the active stream and closes them."
  },
  {
    "objectID": "docs/lab/lab3/lab3.html#stream-processing-basics",
    "href": "docs/lab/lab3/lab3.html#stream-processing-basics",
    "title": "Stream processing with Spark",
    "section": "6. Stream processing basics",
    "text": "6. Stream processing basics\n\n✍ Transformations on stream\n\n🔪 Filter all records with missing / null value then count how many records you keep\n\nFor this filter, you will use the na.drop(\"any\") transformation. The na.drop(\"any\") drop every line with a null value in at least one column. It’s simpler than using a filter() transformation because you don’t have to specify all the column. For more precise filter you can use na.drop(\"any\" or \"all\", subset=list of col) (all will drop rows with only null value in all columns or in the specified list).\nUse the SQL COUNT(1) function in the sql request to get the count\nBecause you don’t perform aggregation the outputMode() must be append\n\nYou will notice no record are dropped.\n🕵️‍♂️ Drop all records with unverified (includes.users.verified == True)user then group the remaining records by hashtag.\n\nincludes.users is an array with only one element. You will need to extract it.\ndata.entities.hashtags is an array too ! To group by tag (the hashtag content) you will need to explode it too.\n\n🔻 Find ukraine related tweet (or any other topic like cat, dog, spring, batman, dogecoin etc) :\n\nDefine a new column, name ukraine_related. This column is equal to True if data.text contains “ukraine”, else it’s equal toFalse.\nUse the withColumn() transformation, and the expr() function to define the column. expr() takes as input an SQL expression. You do not need a full SQL statement (SELECT ... FROM ... WHERE ...) but just an SQL expression that return True or False if data.text contains “ukraine”. To help you :\nLOWER() put in lower case a string\ninput_string LIKE wanted_string return True if input_string is equal to wanted_string\nYou can use % as wildcards\n\nFor more help\n\nOnly keep data.text, data.lang, data.public_metrics and data.created_at\n\n\n\n\n✍ Aggregation and grouping on stream\n\nCount the number of different hashtag.\nGroup by hashtag and compute the average, min and max of like_count\n\nUse the groupBy() and agg() transformations\n\nCompute the average of like_count, retweet_count and quote_count :\n\nacross all hashtag and lang\nfor each lang across all hashtag\nfor each hashtag across all lang\nfor each hashtag and each lang\n\nTo do so, replace the groupBy() transformation by the cube() one. cube() group compute all possible cross between dimensions passed as parameter. You will get something like this\n\n\n\n\n\n\n\n\n\n\nhashtag\nlang\navg(like_count)\navg(retweet_count)\navg(quote_count)\n\n\n\n\ncat\nnull\n1\n2\n3\n\n\ndog\nnull\n4\n5\n6\n\n\n…\n…\n…\n…\n…\n\n\nbird\nfr\n7\n8\n9\n\n\nnull\nen\n10\n11\n12\n\n\nnull\nnull\n13\n14\n15\n\n\n\nA null value mean this dimension wasn’t use for this row. For instance, the first row gives the averages when hashtag==cat independently of the lang. The before last row gives averages when lang==en independently of the hashtag. And the last row gives the averages for the full DataFrame."
  },
  {
    "objectID": "docs/lab/lab3/lab3.html#event-time-processing",
    "href": "docs/lab/lab3/lab3.html#event-time-processing",
    "title": "Stream processing with Spark",
    "section": "7. Event-time processing",
    "text": "7. Event-time processing\nEvent-time processing consists in processing information with respect to the time that it was created, not received. It’s a hot topic because sometime you will receive data in an order different from the creation order. For example, you are monitoring servers distributed across the globe. Your main datacentre is located in Paris. Something append in New York, and a few milliseconds after something append in Toulouse. Due to location, the event in Toulouse is likely to show up in your datacentre before the New York one. If you analyse data bases on the received time the order will be different than the event time. Computers and network are unreliable. Hence, when temporality is important, you must consider the creation time of the event and not it’s received time.\nHopefully, Spark will handle all this complexity for you ! If you have a timestamp column with the event creation spark can update data accordingly to the event time.\nFor instance is you process some data with a time window, Spark will update the result based on the event-time not the received time. So previous windows can be updated in the future.\n\nFigure 5 : Time-event processing, event grouped by time windows\nTo work with time windows, Spark offers two type of windows\n\nNormal windows. You only consider event in a given windows. All windows are disjoint, and a event is only in one window.\nSliding windows. You have a fix window size (for example 1 hour) and a trigger time (for example 10 minute). Every 10 minute, you will process the data with an event time less than 1h.\n\n\nFigure 6 : Time-event processing, event grouped by sliding time windows\nTo create time windows, you need :\n\nto define a time window : window(column_with_time_event : str or col, your_time_window : str, timer_for_sliding_window) : str\ngrouping row by event-time using your window : df.groupeBy(window(...))\n\nTo produce the above processes :\n# Need some import\nfrom pyspark.sql.functions import window, col\n\n# word count + classic time window\ndf_with_event_time.groupBy(\n    window(df_with_event_time.event_time, \"5 minutes\"),\n    df_with_event_time.word).count()\n\n# word count + sliding time window\ndf_with_event_time.groupBy(\n    window(df_with_event_time.event_time, \"10 minutes\", \"5 minutes\"),\n    df_with_event_time.word).count()\n\n✍ Event-time processing ⌛\n\nCount the number of event with a 10 seconds time window (use the created_at column)\nCount the number of event by verified / unverified user with a 10 seconds time window (use the Creation_Time column)\nCount the number of event with a 10 seconds time window sliding every 5 seconds\n\n\n\n7.1 ​Handling late data with watermarks\nProcessing accordingly to time-event is great, but currently there is one flaw. We never specified how late we expect to see data. This means, Spark will keep some data in memory forever. Because streams never end, Spark will keep in memory every time windows, to be able to update some previous results. But in some cases, you know that after some time, you don’t expect new data, or very late data aren’t relevant any more. In other words, after a certain amount of time you want to freeze old results.\nOnce again, Spark can handle such process, with watermarks.\n\nFigure 7 : Time-event processing with watermark\nTo do so, you have to define column as watermark and a the max delay. You have to use the withWatermark(column, max_delay) method.\n# Need some import\nfrom pyspark.sql.functions import window, col\n\n# word count + classic time window\ndf_with_event_time.withWatermark(df_with_event_time.event_time, \"4 minutes\")\\\n.groupBy(\n    window(df_with_event_time.event_time, \"5 minutes\"),\n    df_with_event_time.word).count()\n\n# word count + sliding time window\ndf_with_event_time.withWatermark(df_with_event_time.event_time, \"4 minutes\")\\\n.groupBy(\n    window(df_with_event_time.event_time, \"10 minutes\", \"5 minutes\"),\n    df_with_event_time.word).count()\nBe careful, the watermark field cannot be a nested field (link)\n\n✍ Handling late data with watermarks ⏳\n\nCount the number of event with a 10 seconds time window (use the created_at column) with a 5 seconds watermark\nCount the number of event by hashtag with a 30 seconds time window with a 1 minute watermark\nCount the number of event post by verified user with a 10 seconds time window sliding every 5 seconds with 25 seconds watermark. Write the the result in a file sorted in S3."
  },
  {
    "objectID": "docs/lab/lab3/lab3.html#for-more-details",
    "href": "docs/lab/lab3/lab3.html#for-more-details",
    "title": "Stream processing with Spark",
    "section": "For more details",
    "text": "For more details\n\nSpark official documentation\nZAHARIA, B. C. M. (2018). Spark: the Definitive Guide. , O’Reilly Media, Inc. https://proquest.safaribooksonline.com/9781491912201\nhttps://databricks.com/blog/2018/03/13/introducing-stream-stream-joins-in-apache-spark-2-3.html\nhttps://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\nhttps://databricks.com/blog/2015/07/30/diving-into-apache-spark-streamings-execution-model.html\n\n\nDO NOT FORGET TO TURN YOUR CLUSTER OFF A THE END OF THIS TUTORIAL!"
  }
]