[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Big Data",
    "section": "",
    "text": "Lessons and Labs created by Rémi Pépin and Arthur Katossky"
  },
  {
    "objectID": "index.html#objectives",
    "href": "index.html#objectives",
    "title": "Introduction to Big Data",
    "section": "Objectives",
    "text": "Objectives\n\nUnderstand the basics of computation in the real world, the bottlenecks and how to solve them\nUnderstand the basics of cloud computing and how to use AWS (or SSP Cloud)\nGet familiar with big data technologies and the most common paradigm\nLearn how to use Spark for data exploration on data at rest or streamed data, and how to use some basics ML algorithm on big data"
  },
  {
    "objectID": "index.html#organisation",
    "href": "index.html#organisation",
    "title": "Introduction to Big Data",
    "section": "Organisation",
    "text": "Organisation\n\nLessons : 7h30\nLabs : 10h30 + 3h (graded lab)\nPresentations : 3h"
  },
  {
    "objectID": "index.html#lessons",
    "href": "index.html#lessons",
    "title": "Introduction to Big Data",
    "section": "Lessons",
    "text": "Lessons\n\n1.1 What is Big Data\n\nWhat is considered big ?\nVolume, Velocity, Variety, Veracity, Value, Variability\n\n\n\n1.2 Computer science survival kit\n\nProcessors, Memory, Storage, Network\n\n\n\n1.3 High-performance computing without distribution\n\nProfile code, Analyse code\nStore or process data in chunks, Take advantage of sparcity\nGo low-level\nUse cache\n\n\n\n1.4 What if ?\n\nWhat if data is too big to fit in memory ?\nWhat if your file is too big for your local file system ?\nWhat if data is too big to fit on disk ?\nWhat if computation takes ages ?\nWhat if computation / storage is too expensive ?\n\n\n\n1.5 Social issues\n\nEthical issues, Environmental issues, Political issues\n\n\n\n\n2.1 How to store big data\n\nFile system, Database, Distribution\nThe CAP theorem\n\n\n\n2.2 Hadoop file system (HDFS)\n\nHadoop Ecosystem, How to use HDFS\n\n\n\n2.3 Hadoop MapReduce\n\nKey Concepts\n\n\n\n2.4 Spark\n\nKey Concepts\nImporting/Exporting data\nHow to run Spark ?\n\n\n\n\n3 Cloud Computing\n\nTraditional IT, Virtualization, Containerization\nWhy cloud computing ?\ncategories of cloud services"
  },
  {
    "objectID": "index.html#labs",
    "href": "index.html#labs",
    "title": "Introduction to Big Data",
    "section": "Labs",
    "text": "Labs\n\nLab 0 : Discover Amazon Web Service (AWS)\nLab 1 : First steps with Spark\nLab 2 : Spark ML\nLab 3 : Stream processing with Spark"
  },
  {
    "objectID": "docs/lab/lab2/lab2.html",
    "href": "docs/lab/lab2/lab2.html",
    "title": "Spark ML",
    "section": "",
    "text": "Download this Jupyter Notebook\nFollow these instructions\nSolution"
  },
  {
    "objectID": "docs/lab/lab2/lab2.html#before-you-start",
    "href": "docs/lab/lab2/lab2.html#before-you-start",
    "title": "Spark ML",
    "section": "",
    "text": "Download this Jupyter Notebook\nFollow these instructions\nSolution"
  },
  {
    "objectID": "docs/lab/lab2/lab2.html#outline",
    "href": "docs/lab/lab2/lab2.html#outline",
    "title": "Spark ML",
    "section": "Outline",
    "text": "Outline\nIn this tutorial, we are going to perform exploratory and explanatory analyses of a massive dataset consisting in hundreds of thousands of AirBnB listings, as made available by the Inside AirBnB project.\nRémi Pépin has loaded a lot these listings on AWS at this address:\n\ns3://spark‑lab‑input‑data‑ensai20222023/airbnbb/ on AWS\ns3a://remipepin/diffusion/ensai/airbnb on SSPCloud"
  },
  {
    "objectID": "docs/lab/lab2/lab2.html#how-to-distribute-elementary-statistical-tasks",
    "href": "docs/lab/lab2/lab2.html#how-to-distribute-elementary-statistical-tasks",
    "title": "Spark ML",
    "section": "1 How to distribute elementary statistical tasks?",
    "text": "1 How to distribute elementary statistical tasks?\nThe map and reduce principle\nWhen your data is distributed, i.e is spread out across multiple hard disks / memories on different logical or physical machines, it is clearly not possible to load everything in memory to perform some computation. (No computer from the cluster would have enough storage space / memory space to load the full data set, and the exchange of information between the nodes of the cluster would take considerable amounts of time.) What can you do then?\nA surprisingly satisfying situation is when your algorithm can be expressed in a map-and-reduce model. A map step, in computer science, is the equivalent a function in mathematics: from a given entry, return an output. Examples include counting the number of occurrences of a word in a text, squaring some number, subtracting some number, etc. A reduce step takes two inputs and produces one input, and can be called recursively onto its own outputs, progressively yielding the final result through a pyramid of accumulators (see diagram here under). Popular reduce functions include (pairwise) concatenation of character strings, (pairwise) product, (pairwise) minimum and (pairwise) maximum. But pairwise addition is probably the most used reduce function, with the aim goal of performing a complete addition:\nHadoop’s MapReduce is the name of what was to become today Apache Spark. The persons behind this framework were among the first to advocate for the map-and-reduce mode in order to achieve efficient parallelisation. Unfortunately, the similarity of the names causes a lot of confusion between the map-and-reduce theoretical model and the concrete Hadoop implementation. I will use “map-and-reduce” to help distinguish the algorithmic concept from the MapReduce program, but this is not standard in the literature.\n\nWhy is the map-and-reduce scheme so interesting?\nWell, say you have \\(n\\) entries and \\(k\\) worker nodes at your disposal. The map operation can always be performed locally on each node, since the transformation does not depend on the rest of the data set. This is an embarrassingly parallel problem and we roughly divide the execution time by \\(k\\). Then, most of the reduce steps can also happen on the worker nodes, until the local data has been completely summarized. This also an \\(k\\)_fold acceleration! Then, there remains only \\(k\\) reduce steps, and since \\(k \\ll n\\), this is usually quite negligible, even though the (potentially high) networking costs happen at this step. There is still some cost of task coordination and data exchange, but this usually small compared to the costs of parallelisation.\n\nThe reduce step\nA reduce function is an associative function \\(f: E \\times E \\mapsto E\\), where associativity means \\(\\forall (a,b,c) \\in E^3, f(a,f(b,c))=f(f(a,b),c)\\). This is required because the distribution of data blocks across the nodes is random, and that we want to minimize data transmission between the nodes.\nMoreover, \\(f\\) may or may not be commutative, in the sense that \\(f(a,b)=f(b,a)\\). If it is the case, such as with addition and multiplication, then the computing may happen in no particular order. This means that the central node need not wait for some partial results to be returned by a belated node. On the contrary, if \\(f\\) is not commutative, (a) the worker nodes must apply the function in a defined order, (b) the central node needs to reduce the intermediate outputs in a defined order, (c) it may have to delay the final reduce steps because of a lingering node.\nThe reduce function must not be defined on \\(E=\\mathbb{R}\\). For instance, in the context where data is a collection of text documents, a word-count function may return accumulator objects looking like: ((word1,count1), (word2,count2)). Also, the accumulators — that is, the outputs of the each intermediate reduce step — are not necessarily exactly the cumulative version of the final statistic our algorithm outputs! Rather, accumulators are information-dense, fast-to-compute summary statistics from which the required final statistics can be obtained.\nImagine you want to count the frequency of the vocal E in English, given a collection of texts. It is faster to count the number of Es as well as the total number of characters than to accumulate directly the frequencies, as shown in this diagram:\n\nOnline algorithms\nAn online algorithm is an algorithm with an inner state that can be actualized at low cost for any new arrival of data. A good metaphor is track-keeping of the number of people on a bus: every time a person enters or leaves, you apply ±1 to the count, without the need to systematically recount everyone. Said otherwise, an online algorithm is any algorithm whose last result can be actualized from new data, at a smaller cost than an alternative algorithm that uses both old and new data from scratch.\nIt turns out that respecting the map-and-reduce model gives us online algorithms for free, where the inner state of the algorithm is the output from the last reduce call. Indeed, writing \\(s_\\text{old}\\) and \\(s_\\text{new}\\) the old and new states (the old and new summary statistics), and \\(x_new\\) the latest data point, we have:\n\\[s_\\text{new}=\\text{reduce}(s_\\text{old}, \\text{map}(x_\\text{new}))\\]\nThus, writing an algorithm following the map-and-reduce model gives you both a parallelized batch algorithm and a stream algorithm at once.\nNumber of passes\nSo far we have discussed algorithms that require only one map and one reduce functions. But for some statistics, it is not sufficient. For instance, if we want to count the number of texts where the letter E is more common than average, we first have to compute the average frequency in a first pass, then to count the texts where the frequency exceed this number with a second one. We can NOT do this in only one run, since the global average frequency is not known !\nEach run is called a pass and some algorithms require several passes.\nLimits\n\nNot all statistical algorithms can be expressed according to the map-and-reduce algorithm, and when they can, it may require a significant re-writing compared to the standard algorithms.\nThere may be a trade-off between the number of passes, the speed of each map / reduce steps and the volume of data transferred between each reduce step.\n\n\n1.1 ✍ Hands-on 1\n\nYou are given errors, a distributed vector of prediction errors errors = [1, 2, 5, 10, 3, 4, 6, 8, 9]\nWrite a map-and-reduce algorithm for computing the total sum of squares.\n\nYou may want to create a Python version of this algorithm, using the map(function, vector) and reduce(function, vector) functions or you may use lambda-functions\nYou have to import reduce from the functools module\n\n\n\n# Code Here\n\n\nWrite two different map-and-reduce algorithm for computing the mean sum of squares. (One may include a final \\(O(1)\\) step.)\n\n\n# Code Here\n\n\nIs the median easy to write as a map-and-reduce algorithm? Why?\n\n\n# Code Here\n\n\nGiven a (distributed) series of numbers, the variance can be straightforwardly expressed as a two-pass algorithm: (a) in a first pass, compute the mean, then (b) in a second pass, compute the mean of the errors to the mean. Can it be expressed as a one-pass only algorithm? Is it more expensive to compute variance and mean instead of the variance alone?\n\n\n# Code Here"
  },
  {
    "objectID": "docs/lab/lab2/lab2.html#create-a-spark-session",
    "href": "docs/lab/lab2/lab2.html#create-a-spark-session",
    "title": "Spark ML",
    "section": "2 Create a Spark session",
    "text": "2 Create a Spark session\n\nDepending on the chosen platform, initialize the Spark session\n\n\n2.1 Only on SSPCloud\n\nimport os\nfrom pyspark.sql import SparkSession\n\nspark = (SparkSession\n         .builder\n         # default url of the internally accessed Kubernetes API\n         # (This Jupyter notebook service is itself a Kubernetes Pod)\n         .master(\"k8s://https://kubernetes.default.svc:443\")\n         # Executors spark docker image: for simplicity reasons, this jupyter notebook is reused\n         .config(\"spark.kubernetes.container.image\", os.environ['IMAGE_NAME'])\n         # Name of the Kubernetes namespace\n         .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\n         # Allocated memory to the JVM\n         # Stay careful, by default, the Kubernetes pods has a higher limit which depends on other parameters.\n         .config(\"spark.executor.memory\", \"4g\")\n         .config(\"spark.kubernetes.driver.pod.name\", os.environ['KUBERNETES_POD_NAME'])\n         # dynamic allocation configuration\n         .config(\"spark.dynamicAllocation.enabled\",\"true\")\n         .config(\"spark.dynamicAllocation.initialExecutors\",\"1\")\n         .config(\"spark.dynamicAllocation.minExecutors\",\"1\")\n         .config(\"spark.dynamicAllocation.maxExecutors\",\"60\")\n         # Ratio match the number of pods to create for a given number of parallel tasks\n         # (100 parallel, ratio of 1, one aims at 100 pods, with 0.5 it would be 50 pods)\n         .config(\"spark.dynamicAllocation.executorAllocationRatio\",\"1\")\n         .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",\"true\")\n         .getOrCreate()\n        )\n\n\n\n2.2 Only on AWS\n\n#Spark session\nspark\n\n# Configuraion\nspark._jsc.hadoopConfiguration().set(\"fs.s3.useRequesterPaysHeader\",\"true\")\n\n\n\n2.3 Check spark session\n\nspark"
  },
  {
    "objectID": "docs/lab/lab2/lab2.html#application-on-airbnb-data",
    "href": "docs/lab/lab2/lab2.html#application-on-airbnb-data",
    "title": "Spark ML",
    "section": "3 Application on Airbnb Data",
    "text": "3 Application on Airbnb Data\n\nfrom pyspark.sql.types import FloatType, IntegerType, DateType\nfrom pyspark.sql.functions import regexp_replace, col\n\n# Uncomment one of the following lines\n# listings_raw = spark.read.csv(\"s3a://remipepin/diffusion/ensai/airbnb\", header=True, multiLine=True, escape='\"')\n# listings_raw = spark.read.csv(\"s3://spark‑lab‑input‑data‑ensai20222023/airbnbb/\", header=True, multiLine=True, escape='\"')\n\nlistings = (listings_raw\n  .withColumn(\"beds\",     listings_raw[\"beds\"    ].cast(IntegerType()))\n  .withColumn(\"bedrooms\", listings_raw[\"bedrooms\"].cast(IntegerType()))\n  .withColumn(\"time\", listings_raw[\"last_scraped\"].cast(DateType()))\n  .withColumn(\"price\", regexp_replace('price', '[$\\\\,]', '').cast(FloatType()))\n  .select(\"id\", \"beds\", \"bedrooms\", \"price\", \"city\", \"time\")\n  .dropna() # remove lines with missing values\n)\n\nlistings_raw.cache()\nlistings.cache()\n\n\n3.1 ✍ Hands-on 2\n\nHow many lines do the raw and the formatted datasets have?\n\n\n# Code Here\n\n\nHow many columns are there?\n\nCan you list all the available columns?\n\n\n\n# Code Here\n\nSpark SQL’s summary() method\nIn Spark SQL, elementary univariate summary statistics can also be obtained through the summary() method. The summary() method takes either the names of the statistics to compute, or nothing, in which case it computes every possible statistics:\nlistings.summary(\"count\", \"min\", \"max\").show() # computes the selection of statistics\nlistings.summary().show() # computes every possible statistics\nThis is a way to incite you to compute all the statistics you want at the same moment : it avoids an extra pass on the data set because all accumulators can be computed simultaneously. You can fin a list of all supported statistics here in PySpark documentation: count, mean, standard-deviation, minimum, maximum, approximate median, approximate first and last quartiles. Null (missing) values will be ignored in numerical columns before calculation.\nSpark ML\nSpark ML is a Spark module that allow us to execute parallelised versions of most popular machine-learning algorithms, such as linear or logistic regression. However, we can also use Spark ML to compute elementaty univariate summary statistics. However the philosophy is quite different, and is worth explaining.\nThe syntax of Spark ML may feel artificially convoluted ; this not only an impression, it is convoluted. However, there are grounds for this situation :\n\nSpark ML has been built on top of Spark years into the project, and the core of Spark is not well adapted to machine-learning ;\nSpark ML is intended for much more advanced treatments than unviariate statistics, and we will see linear regression as an exemple at the end of this tutorial\n\nStep 1: vectorisation. A little counter-intuitively, spark ML operates on a single column of your data frame, typically called features. (Features is the word used in the machine-learning community for “variables”, see “Vocabulary” section hereunder.) This features column has the Vector type: each element contains an array of floating-point numbers, representing a subst of the variables from your dataset. The key is that this features column is usually redundant with the rest of the data frame: it just ensures the proper conversion from any type we wish (string, integer…) to a standardized numeric format. Indeed, it is often derived from the other columns, as this image illustrates:\n\nUnfortunately for us, the construction the features column is not performed automatically under the hood by Spark, like when doing statistics in R. On the contrary, we have to construct the column explicitly. The VectorAssembler() constructor is here for that:\nfrom pyspark.ml.feature import VectorAssembler\n\nvectorizer = VectorAssembler(\n    inputCols     = [\"price\", \"beds\", \"bedrooms\"], # the columns we want to put in the features column\n    outputCol     = \"features\",                    # the name of the column (\"features\")\n    handleInvalid = 'skip'                         # skip rows with missing / invalid values\n)\n\nlistings_vec = vectorizer.transform(listings)\n\n# Reminders:\n# Spark data sets are immutable: a copy is returned, and the original is unchanged.\n# Spark operations are lazy: listings_vec just contains the recipe for building vector column\n# but no item of the column is computed unless explicitly asked to.\n\nlistings_vec.show(5) # The first 5 values of the features column are computed.\nStep 2: summarization. Now that we have a vector column, we can use a Summarizer object to declare all the statistics we want to compute, in a similar fashion than with the Spark SQL summary() method. The following statistics are known: mean*, sum*, variance*, standard-deviation*, count*, number of non-zero entries, maximum*, minimum*, L2-norm, L1-norm, as can be read in the documentation. (Stars (*) denote statistics that could also be computed with the summary() method. Approximate quartiles are not computed.) Summarizers are created with the Summarizer.metrics() constructor. Here again, you are incited to declare all the summaries at once, so that they can all be computed in one pass:\nfrom pyspark.ml.stat    import Summarizer\n\nsummarizer = Summarizer.metrics(\"count\", \"min\", \"max\")\n\nlistings_vec.select( summarizer.summary(listings_vec.features), ).show(truncate=False)\n# By default, the output of columns is capped to a maximum width.\n# truncate=False prevents this behaviour.\nThis produces the output:\n\n\n\n3.2 ✍ Hands-on 3\n\nIs listings.summary() slower to run than listings.summary(\"count\", \"min\", \"max\") ? Why?\n\nYou can measure time in Python with this simple template:\n\nfrom timeit import default_timer as t\nstart = t()\n# the thing you want to measure\nprint(\"Time:\", t()-start)\n\n\n# Code Here\n\n\nCompute the average number of beds per property in Barcelona in four different ways:\n\nWhich method is the fastest?\n\ndirectly with the Spark SQL mean function,\nusing summary(),\nusing a Sumarizer object\nlocally after you collected the bed columns. Despite the operation being very common, Spark does not provide a simple syntax to collect a column as a local array. A work-around is to use the Pandas package and the asPanda() method (documentation). First install Pandas with !pip install pandas. Then you can collect a local copy of a dataframe called df with: df_local = df.toPandas(). A Pandas data frame possesses a mean() method, that compute the mean of each column of the data frame: more details are in Pandas’ documentation.\n\n\n\n\n# summary()\n\n\n# Sumarizer\n\n\n# locally after you collected the bed columns\n\n# !pip install pandas\n\nThe most simple model is often surprisingly difficult to beat!\n\nCompute the mean price on the data set as a predictor for an AirBnB listing’s price and the total sum of squares. (We will elaborate in the next section.)\n\n\n# Code here"
  },
  {
    "objectID": "docs/lab/lab2/lab2.html#regression-with-spark-ml",
    "href": "docs/lab/lab2/lab2.html#regression-with-spark-ml",
    "title": "Spark ML",
    "section": "4 Regression with Spark ML",
    "text": "4 Regression with Spark ML\nA better way to predict prices is to build a regression mode, which in Spark falls under the broad category of machine-learning problems. Regressions thus belong the the ml module, often called Spark ML, like the summarizer that we saw just before.\nThere is an old module called mllib that is also called “Spark ML”. That can cause confusion.\nThe ml module is built in a distinctive fashion than the rest of Spark. Firstly we have seen with Summarizer that we can not readily use the columns and that instead columns have to be first converted to a Vector format with the VectorAssembler function.\nSecondly, we need to distinguish between two different types of object classes: transformers and estimators classes. Transformers are a class of objects representing any process that modifies the dataset, and returns the modified version. It has a transform() method. Estimators on the other hand are classes of objects representing any process that produces a transformer based on some computed parameters from the data set. It has a fit() method. It is easier with an example. In the following example, regressor is an estimator, and we compute the regression coefficients with the fit() method. This produces model, the regression model itself, which is of class transformer. Indeed, we can use its transform() method to add predictions to the initial dataset.\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nvectorizer = VectorAssembler( # copy-pasted from previous section...\n    inputCols     = [\"beds\", \"bedrooms\"], # ... but without price\n    outputCol     = \"features\",\n    handleInvalid = 'skip'\n)\n\nlistings_vec = vectorizer.transform(listings)\n\nregressor = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\nmodel     = regressor.fit(listings_vec)\n\nmodel.coefficients\nmodel.intercept\n\nlistings_pred = model.transform(listings_vec)\nlistings_pred.show() # model and predictions from the regression\nVocabulary\nThe machine-learning community leaves at the border between computer science and mathematics. They borrow vocabulary from both sides, and it can sometimes be confusing when reading software documentation. Spark’s lib module uses conventions from this community :\n\nlabel, rather than “independent variable”. This comes from the fact that historically, machine-learning has originated from problems such as image labeling (for instance digit recognition). Even for continuous variables, machine-learners may use “label”\nfeatures, rather than “dependent variables” ; the number of features is often dubbed \\(d\\) like dimension (instead of \\(p\\) in statistics)\nmachine-learners don’t use the word “observation” or “unit” and prefer row\n\nPipelines\nIf you come to repeat several times the same series of transformations, you may take advantage of the pipeline objects. A pipeline is just a collections of steps applied to the same dataset. This helpful when you:\n\nrepeat the same analysis for different regions / periods\nwant to control predictions on a new, unseen test set, and ant to apply exactly the same process\n\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml import Pipeline\n\nvectorizer = VectorAssembler( # same vectorizer as before\n    inputCols     = [\"beds\", \"bedrooms\"],\n    outputCol     = \"features\",\n    handleInvalid = 'skip'\n)\nregressor = LinearRegression(featuresCol=\"features\", labelCol=\"price\") # same regressor\npipeline  = Pipeline(stages = [vectorizer, regressor]) # ... but now we pack them into a pipeline\n\nlistings_beij = listings.filter(listings.city==\"Beijing\")\nlistings_barc = listings.filter(listings.city==\"Barcelona\")\n\nmodel_beij = pipeline.fit(listings_beij) # vectorizer AND regressor are applied\nmodel_barc = pipeline.fit(listings_barc)\n\nprint(model_beij.stages[1].coefficients) # model.stages[0] is the first step, model.stages[1] the second...\nprint(model_beij.stages[1].intercept)\n\nprint(model_barc.stages[1].coefficients)\nprint(model_barc.stages[1].intercept)\n\n4.1 ✍ Hands-on 4\n\nInterpret the results of the general regression.\n\n\n# Code here\n\n\nCollect the model’s \\(R^2\\). How good is our model?\n\nModels have a summary property, that you can explore with dir(model.summary).\n\n\n\n# Code here\n\n\nRepeat the estimation separately for barcelona, brussels and rome.\n\nAre the coefficients stable? You will build a pipeline object.\n\n\n\n# Code here\n\n\nAre the fit() and transform() methods called eagerly or lazily?\n\nCheck the execution plan with the explain() method for lazy evaluations.\n\n\n\n# Code here"
  },
  {
    "objectID": "docs/lab/lab2/lab2.html#diving-deeper",
    "href": "docs/lab/lab2/lab2.html#diving-deeper",
    "title": "Spark ML",
    "section": "5 Diving deeper",
    "text": "5 Diving deeper\nYou are in autonomy for this section. You will find helpful:\n\nThe general Spark documentation for the ml module\nThe PySpark documentation\n\n\n5.1 ✍ Hands-on 5\n\nAdd a categorical variable to the regression.\n\n\n# Code here\n\n\nCompute the p-values of your model as well as confidence intervals for the predictions.\n\n\n# Code here\n\n\nTime the regression in different settings and report the results on this shared spreadsheet. How does it scale with the number of listings (\\(n\\)) ? the number of regressors (\\(p\\)) ? the number of nodes in your cluster (\\(k\\)) ? You will only try a couple of configurations that have not been tested by others. Remember that you can order and revoke nodes from your cluster at any time from the AWS’s cluster view, in the hardware tab, on on the CORE line, “resize”.\n\n\n# Code here\n\n\nDown-sample your data set to \\(n=100000\\), while still keeping a few variables. Save it on S3, then download it on your computer. Run the regression locally on your computer in R. In your opinion, is the extra precision (in term of \\(R^2\\)) is worth the extra computation time?\n\n\n# Code here"
  },
  {
    "objectID": "docs/lab/lab2/lab2.html#end-of-the-lab",
    "href": "docs/lab/lab2/lab2.html#end-of-the-lab",
    "title": "Spark ML",
    "section": "End of the Lab",
    "text": "End of the Lab\n\nExport your notebook\n\nRight clic and Download (.ipynb)\nFile &gt; Save and Export Notebook &gt; HTML\n\nDelete the Jupyter-pyspark service\n\nSSPCloud &gt; My services &gt; Delete"
  },
  {
    "objectID": "docs/lab/lab1/lab1.html",
    "href": "docs/lab/lab1/lab1.html",
    "title": "First steps with Spark",
    "section": "",
    "text": "Download this Jupyter Notebook\nFollow these instructions\nSolution"
  },
  {
    "objectID": "docs/lab/lab1/lab1.html#before-you-start",
    "href": "docs/lab/lab1/lab1.html#before-you-start",
    "title": "First steps with Spark",
    "section": "",
    "text": "Download this Jupyter Notebook\nFollow these instructions\nSolution"
  },
  {
    "objectID": "docs/lab/lab1/lab1.html#what-is-spark",
    "href": "docs/lab/lab1/lab1.html#what-is-spark",
    "title": "First steps with Spark",
    "section": "1 What is Spark ?",
    "text": "1 What is Spark ?\nLet’s keep it simple.\nThe intuitive way: Spark distributes tasks to different Worker nodes (machine executing the tasks).\nThe Kubernetes way: Spark distributes tasks to different containers. The location of containers among the different worker nodes is then handled by Kubernetes. The more computation power you need, the more containers get created. It’s a smooth way to save Worker nodes resources. With Kubernetes, we just speak in terms of Pods rathen than containers.\nPretty handy right ? If you want to have a deeper look on how it’s done, you can by clicking on the link bellow but you still have some work to do so don’t waste your time ! You can keep it to the end.\n\n\nIf you want more ! \n\n\nSpark Context\nThe Spark Context is an object that hides the complexity of the underlying infrastructure to perform computation from the Data Scientist.\nThis Spark context is a JVM process that gives access to a Spark driver which schedules the tasks and spans tasks across Worker nodes through Executors. In brief, the Spark driver communicates with all the Worker nodes.\nEach Worker node consists of one or more Executor(s) who are responsible for running the Task. Executors register themselves with Driver. The Driver has all the information about the Executors at all the time.\nThis working combination of Driver and Workers is known as Spark Application.\nJVM: Java virtual machine that load, verifies and executes Java bytecode.\n\nThe Spark Application is launched with the help of the Cluster Manager. Spark is dependent on the Cluster Manager to launch the Executors and also the Driver (in Cluster mode).\nSpark can be run with any of the 5 following Cluster Manager :\n\nlocal : Driver use CPU threads on your local machine\nSpark Standalone Mode : A basic resource manager provided by Spark\nYARN : the historical resource manager commonly used in traditional Big Data infrastructures (Courdera or Hortonworks cluster for instance)\nMesos : the resource manager from Berkeley University\nKubernetes : the game changing containers orchestrator\n\n\n\nSpark on a Kubernetes cluster\n\nThe Spark driver runs on a Kubernetes Pod and creates executors running within Kubernetes pods. In simple terms, Spark driver is in a Pod and it distributes tasks to different Pods. The location of Pods among the different worker nodes is then handled by Kubernetes.\nThe Kubernetes Scheduler checks if pods are assigned to nodes. For every Pod that the scheduler discovers, the scheduler is responsible of finding the best node for that Pod to run on.\nThe Kubernetes API Server allows to interact with Pods. It schedules executor Pods by creating or deleting Pods."
  },
  {
    "objectID": "docs/lab/lab1/lab1.html#create-a-spark-session",
    "href": "docs/lab/lab1/lab1.html#create-a-spark-session",
    "title": "First steps with Spark",
    "section": "2 Create a Spark session",
    "text": "2 Create a Spark session\n\nDepending on the chosen platform, initialize the Spark session\n\n\n2.1 Only on SSPCloud\n\nimport os\nfrom pyspark.sql import SparkSession\n\nspark = (SparkSession\n         .builder\n         # default url of the internally accessed Kubernetes API\n         # (This Jupyter notebook service is itself a Kubernetes Pod)\n         .master(\"k8s://https://kubernetes.default.svc:443\")\n         # Executors spark docker image: for simplicity reasons, this jupyter notebook is reused\n         .config(\"spark.kubernetes.container.image\", os.environ['IMAGE_NAME'])\n         # Name of the Kubernetes namespace\n         .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\n         # Allocated memory to the JVM\n         # Stay careful, by default, the Kubernetes pods has a higher limit which depends on other parameters.\n         .config(\"spark.executor.memory\", \"4g\")\n         .config(\"spark.kubernetes.driver.pod.name\", os.environ['KUBERNETES_POD_NAME'])\n         # dynamic allocation configuration\n         .config(\"spark.dynamicAllocation.enabled\",\"true\")\n         .config(\"spark.dynamicAllocation.initialExecutors\",\"1\")\n         .config(\"spark.dynamicAllocation.minExecutors\",\"1\")\n         .config(\"spark.dynamicAllocation.maxExecutors\",\"60\")\n         # Ratio match the number of pods to create for a given number of parallel tasks\n         # (100 parallel, ratio of 1, one aims at 100 pods, with 0.5 it would be 50 pods)\n         .config(\"spark.dynamicAllocation.executorAllocationRatio\",\"1\")\n         .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",\"true\")\n         .getOrCreate()\n        )\n\n\n\n2.2 Only on AWS\n\n#Spark session\nspark\n\n# Configuraion\nspark._jsc.hadoopConfiguration().set(\"fs.s3.useRequesterPaysHeader\",\"true\")\n\n\n\n2.3 Check spark session\n\nspark"
  },
  {
    "objectID": "docs/lab/lab1/lab1.html#first-steps-with-spark---data-importation",
    "href": "docs/lab/lab1/lab1.html#first-steps-with-spark---data-importation",
    "title": "First steps with Spark",
    "section": "3 First steps with Spark - Data importation",
    "text": "3 First steps with Spark - Data importation\nSpark’s main object class is the DataFrame, which is a distributed table. It is analogous to R’s or Python (Pandas)’s data frames:\n\none row represents an observation,\none column represents a variable.\n\nBut contrary to R or Python, Spark’s DataFrames can be distributed over hundred of nodes.\nSpark support multiple data formats, and multiple ways to load them.\n\ndata format : csv, json, parquet (an open source column oriented format)\ncan read archive files\nschema detection or user defined schema. For static data, like a json file, schema detection can be use with good results.\n\nSpark has multiple syntaxes to import data. Some are simple with no customisation, others are more complexes but you can specify options.\nThe simplest syntaxes to load a json or a csv file are :\n# JSON\njson_df = spark.read.json([location of the file])\n# csv\ncsv_df = spark.read.csv([location of the file])\nIn the future, you may consult the Data Source documentation to have the complete description of Spark’s reading abilities.\nThe data you will use in this lab are real data from the twitter sampled stream API and filtered stream API. The tweets folder contains more than 50 files and more than 2 million tweets. The tweets was collected between the 14/04/2021 and the 18/04/2021. The total collection time was less than 10 hours.\n\n\n3.1 ✍Hands-on 1 - Data importation\n\nLoad the json file stored here :\n\ns3a://ludo2ne/diffusion/tweets.jsonl.gz for SSPCloud\ns3://spark-lab-input-data-ensai20222023/tweets/tweets20220324-155940.jsonl.gz for AWS\n\nName you data frame df_tweet\nUse function cache() on the data frame. Caching is a performance optimization technique that allows you to persist an intermediate or final result of a computation in memory, reducing the need to recompute the data when it is accessed multiple time\n ⚙️ This file is an a JSONL (JSON-line) format, which means that each line of it is a JSON object. A JSON object is just a Python dictionary or a JavaScript object and looks like this: { key1: value1, key2: [\"array\", \"of\", \"many values]}. This file has been compressed into a GZ archive, hence the .jsonl.gz ending. Also this file is not magically appearing in your S3 storage. It is hosted on one of your teacher’s bucket and has been made public, so that you can access it.\n\nIt’s possible to load multiple file in a unique DataFrame. It’s useful when you have daily files and want to process them all. It’s the same syntax as the previous one, just specify a folder.\n\nSSPCloud users : if you meet some issue to load this file, you can load and use your own file :\n\nIn Onyxia, mes fichiers\nLoad file tweets.jsonl.gz\nIn Jupyter, read it using s3a://&lt;user_name&gt;/tweets.jsonl.gz\n\n\n\n# DataFrame creation\n\n# caching"
  },
  {
    "objectID": "docs/lab/lab1/lab1.html#data-frame-basic-manipulations",
    "href": "docs/lab/lab1/lab1.html#data-frame-basic-manipulations",
    "title": "First steps with Spark",
    "section": "4 Data frame basic manipulations",
    "text": "4 Data frame basic manipulations\nIf DataFrames are immutable, they can however be transformed in other DataFrames, in the sense that a modified copy is returned. Such transformations include: filtering, sampling, dropping columns, selecting columns, adding new columns…\nFirst, you can get information about the columns with:\ndf.columns       # get the column names\ndf.schema        # get the column names and their respective type\ndf.printSchema() # same, but human-readable\nYou can select columns with the select() method. It takes as argument a list of column name. For example :\ndf_with_less_columns = df\\\n  .select(\"variable3\",\"variable_four\",\"variable-6\")\n\n# Yes, you do need the ugly \\ at the end of the line,\n# if you want to chain methods between lines in Python\nYou can get nested columns easily with :\ndf.select(\"parentField.nestedField\")\nTo filter data you could use the filter() method. It take as input an expression that gets evaluated for each observation and should return a boolean. Sampling is performed with the sample() method. For example :\ndf_with_less_rows = df\\\n  .sample(fraction=0.001)\\\n  .filter(df.variable1==\"value\")\\\n  .show(10)\nAs said before your data are distributed over multiple nodes (executors) and data inside a node are split into partitions. Then each transformations will be run in parallel. They are called narrow transformation For example, to sample a DataFrame, Spark sample every partitions in parallel because sample all partition produce the sample DataFrame. For some transformations, like groupBy() it’s impossible, and it’s cannot be run in parallel.\n\n\n4.1 Lazy evaluation\nThis is because Spark has what is known as lazy evaluation, in the sense that it will wait as much as it can before performing the actual computation. Said otherwise, when you run an instruction such as:\ntweet_author_hashtags = df_tweet_big.select(\"auteur\",\"hashtags\")\n… you are not executing anything! Rather, you are building an execution plan, to be realised later.\nSpark is quite extreme in its laziness, since only a handful of methods called actions, by opposition to transformations, will trigger an execution. The most notable are:\n\ncollect(), explicitly asking Spark to fetch the resulting rows instead of to lazily wait for more instructions,\ntake(n), asking for n first rows\nfirst(), an alias for take(1)\nshow() and show(n), human-friendly alternatives\ncount(), asking for the numbers of rows\nall the “write” methods (write on file, write to database), see here for the list\n\nThis has advantages: on huge data, you don’t want to accidently perform a computation that is not needed. Also, Spark can optimize each stage of the execution in regard to what comes next. For instance, filters will be executed as early as possible, since it diminishes the number of rows on which to perform later operations. On the contrary, joins are very computation-intense and will be executed as late as possible. The resulting execution plan consists in a directed acyclic graph (DAG) that contains the tree of all required actions for a specific computation, ordered in the most effective fashion.\nThis has also drawbacks. Since the computation is optimized for the end result, the intermediate stages are discarded by default. So if you need a DataFrame multiple times, you have to cache it in memory because if you don’t Spark will recompute it every single time.\n\n\n\n4.2 ✍Hands-on 2 - Data frame basic manipulations\n\nHow many rows have your two DataFrame ?\n\n\n# Code Here\n\n\nDisplay columns names and then the schema\n\n\n# Code Here\n\n\n# Code Here\n\n\nDisplay 10 rows of df_tweet\n\n\n# Code Here\n\n\nSample df_tweet and keep only 10% of it.\n\nCreate a new DataFrame named df_tweet_sampled.\nIf computations take too long on the full DataFrame, use this one instead or add a sample transformation in your expression.\n\n\n\n# Code Here\n\n\nDefine a DataFrame tweet_author_hashtags with only the auteur and hashtags columns\n\nThen display 5 rows\n\n\n\n# Code Here\n\n\nPrint 5 lines of a df_tweet with only the auteur, mentions, and urls columns.\n\nmentions and urls are both nested columns in entities\n\n\n\n# Code Here\n\n\nFilter df_tweet and keep only tweets with more than 1 like.\n\nDisplay only auteur, contenu and like_count\nPrint 10 lines\n\n\n\n# Code Here"
  },
  {
    "objectID": "docs/lab/lab1/lab1.html#basic-dataframe-column-manipulation",
    "href": "docs/lab/lab1/lab1.html#basic-dataframe-column-manipulation",
    "title": "First steps with Spark",
    "section": "5 Basic DataFrame column manipulation",
    "text": "5 Basic DataFrame column manipulation\nYou can add/update/rename column of a DataFrame using spark :\n\nDrop : df.drop(columnName : str )\nRename : df.withColumnRenamed(oldName : str, newName : str)\nAdd/update : df.withColumn(columnName : str, columnExpression)\n\nFor example\ndf_tweet\\\n  .withColumn(                                        # computes new variable\n    \"like_rt_ratio\",                                  # like_rt_ratio \"OVERCONFIDENCE\"\n    df_tweet.like_count /df_tweet.retweet_count)\nSee here for the list of all functions available in an expression.\n\n5.1 ✍Hands-on 3 - Basic DataFrame column manipulation\n\nDefine a DataFrame with a column names interaction_count named df_tweet_interaction_count\n\nThis column is the sum of like_count, reply_count and retweet_count.\n\n\n\n# Code Here\n\n\nUpdate the DataFrame you imported at the beginning of this lab and drop the other column\n\n\n# Code Here"
  },
  {
    "objectID": "docs/lab/lab1/lab1.html#advance-dataframe-column-manipulation",
    "href": "docs/lab/lab1/lab1.html#advance-dataframe-column-manipulation",
    "title": "First steps with Spark",
    "section": "6 Advance DataFrame column manipulation",
    "text": "6 Advance DataFrame column manipulation\n\n6.1 Array manipulation\nSome columns often contain arrays (lists) of values instead of just one value. This may seem surprising but this actually quite natural. For instance, you may create an array of words from a text, or generate a list of random numbers for each observation, etc.\nYou may create array of values with:\n\nsplit(text : string, delimiter : string), turning a text into an array of strings\n\nYou may use array of values with:\n\nsize(array : Array), getting the number of elements\narray_contains(inputArray : Array, value : any), checking if some value appears\nexplode(array : Array), unnesting an array and duplicating other values. For instance if you use explode() over the hashtags value of this DataFrame:\n\n\n\nAuteur\nContenu\nHashtags\n\n\n\n\nBob\nI love #Spark and #bigdata\n[Spark, bigdata]\n\n\nAlice\nJust finished #MHrise, best MH ever\n[MHrise]\n\n\n\nYou will get :\n\n\n\n\n\n\n\n\n\nAuteur\nContenu\nHashtags\nHashtag\n\n\n\n\nBob\nI love #Spark and #bigdata\n[Spark, bigdata]\nSpark\n\n\nBob\nI love #Spark and #bigdata\n[Spark, bigdata]\nbigdata\n\n\nAlice\nJust finished #MHrise, best MH ever\n[MHrise]\nMHrise\n\n\n\n\nAll this functions must be imported first :\nfrom pyspark.sql.functions import split, explode, size, array_contains\nDo not forget, to create a new column, you should use withColumn(). For example :\ndf.withColumn(\"new column\", explode(\"array\"))\n\n✍Hands-on 4 - Array manipulation\n\nKeep all the tweets with hashtags and for each remaining line, split the hashtag text into an array of hashtags\n\n\n# Code Here\n\n\nCreate a new column with the number of words of the contenu column. (Use split() + size())\n\n\n# Code Here\n\n\nCount how many tweet contain the Ukraine hashtag (use the count() action)\n\n\n# Code Here\n\n\n\n\n6.2 User defined function\nFor more very specific column manipulation you will need Spark’s udf() function (User Defined Function). It can be useful if Spark does not provide a feature you want. But Spark is a popular and active project, so before coding an udf, go check the documentation. For instance for natural language processing, Spark already has some functions. Last things, python udf can lead to performance issues (see https://stackoverflow.com/a/38297050) and learning a little bit of scala or java can be a good idea.\nFor example :\n# !!!! DOES NOT WORK !!!!\ndef to_lower_case(string):\n    return string.lower()\n\ndf.withColumn(\"tweet_lower_case\", to_lower_case(df.contenu))\nwill just crash. Keep in mind that Spark is a distributed system, and that Python is only installed on the central node, as a convenience to let you execute instructions on the executor nodes. But by default, pure Python functions can only be executed where Python is installed! We need udf() to enable Spark to send Python instructions to the worker nodes.\nLet us see how it is done :\n# imports\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.functions import explode\nfrom pyspark.sql.types import StringType\n\n# pure python functions\ndef to_lower_case(string):\n    return string.lower()\n\n# user defined function(we use a lambda function to create the udf)\nto_lower_case_udf = udf(\n    lambda x: to_lower_case(x), StringType()\n)\n\n# df manipulation\ndf_tweet_small\\\n  .select(\"auteur\",\"hashtags\")\\\n  .filter(\"size(hashtags)!=0\")\\\n  .withColumn(\"hashtag\", explode(\"hashtags\"))\\\n  .withColumn(\"hashtag\", to_lower_case_udf(\"hashtag\")).show(10)\n\n\n✍Hands-on 5 - User defined function\n\nCreate an user defined function that counts how many words a tweet contains.\n\nyour function will return an IntegerType and not a StringType\n\n\n\n# Code Here"
  },
  {
    "objectID": "docs/lab/lab1/lab1.html#aggregation-functions",
    "href": "docs/lab/lab1/lab1.html#aggregation-functions",
    "title": "First steps with Spark",
    "section": "7 Aggregation functions",
    "text": "7 Aggregation functions\nSpark offer a variety of aggregation functions :\n\ncount(column : string) will count every not null value of the specify column. You cant use count(1) of count(\"*\") to count every line (even row with only null values)\ncountDisctinct(column : string) and approx_count_distinct(column : string, percent_error: float). If the exact number is irrelevant, approx_count_distinct()should be preferred.\nCounting distinct elements cannot be done in parallel, and need a lot data transfer. But if you only need an approximation, there is a algorithm, named hyper-log-log (more info here) that can be parallelized.\nfrom pyspark.sql.functions import count, countDistinct, approx_count_distinct\n\ndf.select(count(\"col1\")).show()\ndf.select(countDistinct(\"col1\")).show()\ndf.select(approx_count_distinct(\"col1\"), 0.1).show()\nYou have access to all other common functions min(), max(), first(), last(), sum(), sumDistinct(), avg() etc (you should import them first from pyspark.sql.functions import min, max, avg, first, last, sum, sumDistinct)\n\n\n\n7.1 ✍Hands-on 6 - Aggregation functions\n\nWhat are the min, max, average of interaction_count (use df_tweet_interaction_count created earlier)\n\ndon’t forget to import the required functions\n\n\n\n# Code Here\n\n\nHow many tweets have hashtags ?\n\nDistinct hashtags ?\nTry the approximative count with 0.1 and 0.01 as maximum estimation error allowed.\n\n\n\n# Code Here"
  },
  {
    "objectID": "docs/lab/lab1/lab1.html#grouping-functions",
    "href": "docs/lab/lab1/lab1.html#grouping-functions",
    "title": "First steps with Spark",
    "section": "8 Grouping functions",
    "text": "8 Grouping functions\nLike SQL you can group row by a criteria with Spark. Just use the groupBy(column : string) method. Then you can compute some aggregation over those groups.\ndf\\\n  .groupBy(\"col1\")\\\n  .agg(count(\"col2\").alias(\"quantity\"))\\           # alias is use to specify the name of the new column\n  .show()\nThe agg() method can take multiples argument to compute multiple aggregation at once.\ndf\\\n  .groupBy(\"col1\")\\\n  .agg(count(\"col2\").alias(\"quantity\"),\n       min(\"col2\").alias(\"min\"),\n       avg(\"col3\").alias(\"avg3\"))\\\n  .show()\nAggregation and grouping transformations work differently than the previous method like filter(), select(), withColumn() etc. Those transformations cannot be run over each partitions in parallel, and need to transfer data between partitions and executors. They are called “wide transformations”\n\n\n8.1 ✍Hands-on 7 - Grouping functions\n\nCompute a daframe with the min, max and average retweet of each auteur.\n\nThen order it by the max number of retweet in descending order by .\nTo do that you can use the following syntax\n\nfrom pyspark.sql.functions import desc\ndf.orderBy(desc(\"col\"))\n\n\n# Code Here"
  },
  {
    "objectID": "docs/lab/lab1/lab1.html#spark-sql",
    "href": "docs/lab/lab1/lab1.html#spark-sql",
    "title": "First steps with Spark",
    "section": "9 Spark SQL",
    "text": "9 Spark SQL\nSpark understand SQL statement. It’s not a hack nor a workaround to use SQL in Spark, it’s one a the more powerful feature in Spark. To use SQL you will need :\n\nRegister a view pointing to your DataFrame\nmy_df.createOrReplaceTempView(viewName : str)\nUse the sql function\nspark.sql(\"\"\"\nYour SQL statement\n\"\"\")\nYou could manipulate every registered DataFrame by their view name with plain SQL.\n\nIn fact you can do most of this tutorial without any knowledge in PySpark nor Spark. Many things can only be done in Spark if you know SQL and how to use it in Spark.\n\n9.1 ✍Hands-on 8 - Spark SQL\n\nHow many tweets have hashtags ?\n\nDistinct hashtags ?\n\n\n\n# Code Here\n\n\nCompute a dataframe with the min, max and average retweet of each auteur using Spark SQL\n\n\n# Code Here"
  },
  {
    "objectID": "docs/lab/lab1/lab1.html#end-of-the-lab",
    "href": "docs/lab/lab1/lab1.html#end-of-the-lab",
    "title": "First steps with Spark",
    "section": "End of the Lab",
    "text": "End of the Lab\n\nExport your notebook\n\nRight clic and Download (.ipynb)\nFile &gt; Save and Export Notebook &gt; HTML\n\nDelete the Jupyter-pyspark service\n\nSSPCloud &gt; My services &gt; Delete"
  },
  {
    "objectID": "docs/lab/lab0/lab0.html",
    "href": "docs/lab/lab0/lab0.html",
    "title": "Discover Amazon Web Service (AWS)",
    "section": "",
    "text": "The lab’s goal is to make you familiar with the AWS’s interface. In this lab, you will :\n\nCreate an account on AWS academy if you haven’t done yet\nStore some data on Amazon Simple Storage Service (AWS S3)\nCreate an Elastic Compute Cloud instance (= a virtual machine) and connect to it with SSH\nRun some basic shell commands\n\nls to list files in a directory\ncd to change the current directory\nyum to install a package\naws s3 cp to copy file from S3\nchmod to change some file persmision\ntime [commande] to compute some execution time\n\nShut down your EC2 instance"
  },
  {
    "objectID": "docs/lab/lab0/lab0.html#objectives",
    "href": "docs/lab/lab0/lab0.html#objectives",
    "title": "Discover Amazon Web Service (AWS)",
    "section": "",
    "text": "The lab’s goal is to make you familiar with the AWS’s interface. In this lab, you will :\n\nCreate an account on AWS academy if you haven’t done yet\nStore some data on Amazon Simple Storage Service (AWS S3)\nCreate an Elastic Compute Cloud instance (= a virtual machine) and connect to it with SSH\nRun some basic shell commands\n\nls to list files in a directory\ncd to change the current directory\nyum to install a package\naws s3 cp to copy file from S3\nchmod to change some file persmision\ntime [commande] to compute some execution time\n\nShut down your EC2 instance"
  },
  {
    "objectID": "docs/lab/lab0/lab0.html#log-on-aws",
    "href": "docs/lab/lab0/lab0.html#log-on-aws",
    "title": "Discover Amazon Web Service (AWS)",
    "section": "1 Log on AWS",
    "text": "1 Log on AWS\n\nFollow the instructions you get from the AWS academy’s e-mail, then those of the AWS Academy Learner Lab - Student Guide to access to the AWS console.\n\nYour AWS account is located in North Virginia, do not change that.\nBecause your account is for academic purpose, your don’t have access to all the AWS services.\nYour account is managed by AWS academy, so you have to use the AWS academy portal to access to your AWS account.\n\nAWS Login\nDashboard\nSelect AWS Academy Learner Lab - Big Data\nSelect Modules and then Lancer l'atelier pour étudiants de l'AWS Academy\nStart Lab\n\nWait 2 minutes\nWhen the dot next to AWS turns green, your lab environment is ready to use\nClick AWS"
  },
  {
    "objectID": "docs/lab/lab0/lab0.html#exploration",
    "href": "docs/lab/lab0/lab0.html#exploration",
    "title": "Discover Amazon Web Service (AWS)",
    "section": "2 Exploration",
    "text": "2 Exploration\nIn the Services tab, you should find for instance :\n\nEC2 in computation\nS3 in storage\nA section for databases\nA section for machine-learning\n\nA section for data analysis"
  },
  {
    "objectID": "docs/lab/lab0/lab0.html#create-a-s3-bucket",
    "href": "docs/lab/lab0/lab0.html#create-a-s3-bucket",
    "title": "Discover Amazon Web Service (AWS)",
    "section": "3 Create a S3 bucket",
    "text": "3 Create a S3 bucket\nAmazon Simple Storage Service (S3) is the standard solution to store data on AWS. Amazon assured a 99,999999999 % durability for your data. This mean, is you store 10 000 000 files on S3, you can on average expect to incur a loss of a single file every 10 000 years.\nThe storage is quite expensive (around ~0,02 $/Go/month), knowing that you pay for read operations (around ~0,09 \\$/Go). 1 To cost around 240$ a year. For instance, a 1To SSD cost less than 100$ (for a smaller durability), and a cloud storage solution (like drop box) for private individual cost 10$/month for 2To (for the same durability). But the use case are different. S3 is for data access frequently by other AWS services. There is other storage solution (like S3 glacier) for archive, or databases.\nAll AWS services can natively read from and write to S3, if they have some access right. So, every application you deploy on AWS can import/export data from/to S3. A file stored in S3 is called an “object”, and can be access by a unique URL. You can limit access right to a specific file.\n\nIn the search bar type S3\nClick on Créer un compartiment (“bucket” in English)\n\nChose a unique name for your bucket (ex: ensai-firstName-lastName-currentDate)\nKeep all the default value\nCreate your bucket"
  },
  {
    "objectID": "docs/lab/lab0/lab0.html#import-some-files-to-your-bucket",
    "href": "docs/lab/lab0/lab0.html#import-some-files-to-your-bucket",
    "title": "Discover Amazon Web Service (AWS)",
    "section": "4 Import some files to your bucket",
    "text": "4 Import some files to your bucket\n\nSelect your bucket to go to its dedicated page\n\nClick on Charger, and upload the file Lab0_files.zip\n\n\n\nOnce the upload finished, click on your file\nYou will land on the file page, and found to link to access to your file. One is the URL of the file, the other is the S3 URI\nCopy in a text file your file URI, you will need it later"
  },
  {
    "objectID": "docs/lab/lab0/lab0.html#ssh-key-creation",
    "href": "docs/lab/lab0/lab0.html#ssh-key-creation",
    "title": "Discover Amazon Web Service (AWS)",
    "section": "5 SSH key creation",
    "text": "5 SSH key creation\nSSH (Secure SHell) protocol allow a secure remote connection to a machine. Moreover, with SSH you can control the remote machine. For more details, you can read this page. But first you need a private key to authenticate yourself.\n\nIn the search bar, search paire de clés\nClick on Créer une paire de clés\nGive it a name (par ex: labsuser), select the ppk format if you use windows, and pem if you use Linux / macOs, then click on créer\nThis will download your key, do not lost it !\nIf you are on Linux / macOs open a terminal and type :\ncd ~/Downloads\nchmod 400 labsuser.pem\nDo not close this terminal"
  },
  {
    "objectID": "docs/lab/lab0/lab0.html#create-your-first-virtual-machine",
    "href": "docs/lab/lab0/lab0.html#create-your-first-virtual-machine",
    "title": "Discover Amazon Web Service (AWS)",
    "section": "6 Create your first virtual machine",
    "text": "6 Create your first virtual machine\n\nIn the search bar, search EC2\nClick on Lancer une instance\n\nFirst, you must choose an image for your machine (called AMI pour Amazon Machine Image). This image contain an OS and some default applications.\nChoose Amazon Linux 2 AMI (HVM) - Kernel 5.10\nThen you will select the hardware configuration. For instance, for a general usage machine you can choose a t2.micro for a cheap but weak machine (0.012$/hour, 1 core, 1Go Ram) or a more powerful and more expensive one like t2.xlarge (0.188$/hour, 8 core, 32Go Ram). Because you pay for how long your EC2 instance are up, turn off you machine at the end of the lab !\n\nSelect the key pair\nClic on Détails avancés\n\nProfil d’instance IAM : select LabInstanceProfile\n\nThen click on Lancer l'instance\n\nCongrats! Your VM is launching.\n\nClick on Affichez les instances and wait a minute!"
  },
  {
    "objectID": "docs/lab/lab0/lab0.html#connect-to-the-ec2-instance",
    "href": "docs/lab/lab0/lab0.html#connect-to-the-ec2-instance",
    "title": "Discover Amazon Web Service (AWS)",
    "section": "7 Connect to the EC2 instance",
    "text": "7 Connect to the EC2 instance\n\nOnce your VM is running, you can access to the its administration page by clinking on its id d'instance\n\n\nYou will find many information, but the most important it’s its IPv4 publique which is its’s IP address (Internet Protocol) used to access to your machine from outside AWS.\n\n7.1 Windows user\n\nRun PuTTY\nIn Host Name type your public IP\nIn Saved Session enter AWS EC2 and Save \nThen go to SSH &gt; Auth &gt; Credentials\n\nPrivate key file for authentification : your .ppk file \n\nFinally go back to the first screen, click on your session name, Save then Open the session. \nA similar windows will pop up, click on Oui \nA terminal will open\n\nLogin as: ec2-user\nthen press Enter(documentation) \n\n\n\n\n7.2 macOS/Linux user\n\nIn the previously open terminal type\nssh -i labsuser.pem ec2-user@[public-ip]\nReplace [public-ip] by your public ip\n\nTada ! your are connected to your virtual machine. Although, this terminal is on your computer, every command you type are executed on the remote machine. This make it possible to run huge computation without altering your own performance. But, this machine do not have any graphical interface (GUI), so you need so basics of shell command."
  },
  {
    "objectID": "docs/lab/lab0/lab0.html#play-with-your-vm",
    "href": "docs/lab/lab0/lab0.html#play-with-your-vm",
    "title": "Discover Amazon Web Service (AWS)",
    "section": "8 Play with your VM",
    "text": "8 Play with your VM\nIn this section you will learn some basics of shell, and reproduce the benchmark of language made in the first lesson. You will :\n\nGet all the files for the benchmark\nInstall R and a python package on your machine\nRun the benchmark\n\nThis benchmark compare the time to compute the max temperature for some year based on the USA weather data. Each file contains all the weather data for one year, and each record isa weather observation. An observation looks like this :\n0029029070999991901010106004+64333+023450FM-12+000599999V0202701N015919999999N0000001N9-00781+99999102001ADDGF108991999999999999999999\nIt’s a positional file, with the temperature at the position [87, 91] and it’s quality at position 91. In the example the temperature is -0078 deca Celsius (so -7.8°C), with a quality of 1 (a good quality). Each program loads line by line the data and computes the max by updating the current max value (so a \\(\\mathcal{O}(n)\\) in computation time and \\(\\mathcal{O}(1)\\) in memory) (expect for R that load all the data in memory so \\(\\mathcal{O}(n)\\) in memory). But each language has its specificities :\n\npython : dynamic typing, just in time compilation to byte code then interpreted by python\njava : static typing, ahead of time compilation to byte code then interpreted by java\nC : statis typing, ahead of time compilation to machine code then executed by the OS\nscript bash : no type like python/java/C, interpeted by your OS.\n\nHere is some bash commands\n\n\n\n\n\n\n\nCommand\nUse\n\n\n\n\nsudo\nSuper user : basically you have all the rights\n\n\ncd [target_directory]\nChange directory : move in the file tree. To go to the parent directory docd ../\n\n\nls\nList : list all the file in the current directory\n\n\nmkdir [directory_name]\nMake directory : create a directory\n\n\nrm [file_name]\nRemove : remove a file. You cannot remove a none empty directory by default. You should use the rm [file_name] -r to do so. Note : NEVER execute the command rm -rf / because you will remove all the file on the current machine\n\n\nchmod 764 [file_name]\nChange mode : change the access right ofyour files. Without more detail, the access right 764 gives read, write and execution rights to the owner of the file.\n\n\nunzip [file_name]\nUnzip : unzip files\n\n\nyum install [package]\nPackage manager for some linux distribution. It’s like pip but for Linux.\n\n\naws s3 cp [s3://URI]\nSpecific AWS command. Copy file from s3 to the current machine\n\n\namazon-linux-extras [package]\nLike yum but for amazon machine.\n\n\n\n\n8.1 Benchmark setup\n\nDownload the file stored on S3. Use the command aws s3 cp [s3://object/uri] [output/folder]. The S3 URI is on S3 file page. For output/folder use the current directory with .. You should get a command and an output like this\n[ec2-user@ip-172-31-85-99 ~]$ aws s3 cp s3://remi-pepin-21032022/fichiersTP0.zip .\ndownload: s3://remi-pepin-21032022/fichier TP.zip to ./fichier TP.zip\nWith ls (list) check you downloaded the file\nNow you will unzip your file with unzip [nom de votre fichier]. Check the result with ls\nFor security reasons, you cannot execute your files. Do chmod 764 get_data.sh awk.sh GetMaxTempC to change that. For more details, the chmod wikipedia page provides more details..\nNow you will execute the get_data.sh script. Just type./get_data.sh. This script download some data from the NOAA (~ météo france in the USA) server\n\n\n\n8.2 Install R, java, C compiler and a python package\nYour VM doesn’t have all the require packages for the benchmark.\n\npython-dev installation : python-devel is require to create extension for python. To install it use yum, a package manager for Linux. The exact command issudo yum install -y python3-devel.x86_64 (sudo to have super user right, yum to use the package manager, install to install a package, -y to validate the installation, and python3-devel.x86_64 for the package name)\nJava and GCC installation : in the same way, insatall java and GCC with sudo yum install java gcc -y\nCython installation : with pip3 install Cython and compile the Cython code:\n\ncd cython_code for*change directory- and go in the cython_code directory\npython3 setup.py to run the compilation\ncd ../ to go back to the parent folder\n\nR installation : to install R, you will use the amazon custome package manager amazon-linux-extras, with the following command : sudo amazon-linux-extras install R4 -y. Just wait 1-2 minutes.\n\n\n\n8.3 Benchmark\nNow it’s time to run the benchmark. To do so, you will use the time command. time compute the execution time of the following command. For example : time ./get_data.sh . Write all the results, and compare then with the course. If results are different, try to understand why.\n\nTo run compiled C or bash : time ./[file]\nTo run compiled java :time java -jar [file.jar]\nTo run python :time python3 [file.py]\nTo run R script : time Rscript [filename.R]\n\n\n\n8.4 A shell in your web browser\n\nClose your terminal and go back to your EC2 instance page. Click onSe connecter then Se connecter  Vous allez arriver sur une page similaire à celle ci-dessous. Cliquez sur Se connecter  After 30 second you can use a cloud shell. You can do the same thing with your cloud shell as the previous SSH shell.\n\n\nType ls to check if your file are still there."
  },
  {
    "objectID": "docs/lab/lab0/lab0.html#shut-down-your-machine",
    "href": "docs/lab/lab0/lab0.html#shut-down-your-machine",
    "title": "Discover Amazon Web Service (AWS)",
    "section": "Shut down your machine",
    "text": "Shut down your machine\nBecause your EC2 instance is billed by time and not usage, once your work is done, shut down it. Although the cost is small, a small cost*24*7 for 1 week or running instance can cost 10$. And for bigger machine in a company maybe 100$ or 1000$.\nTo shut down your machine : - [ ] go to EC2 &gt; Instances and find all running instances - [ ] Click on état de instance - [ ] then on resilier l'instance"
  },
  {
    "objectID": "docs/lab/lab-end.html",
    "href": "docs/lab/lab-end.html",
    "title": "Big Data",
    "section": "",
    "text": "Export your notebook\n\nRight clic and Download (.ipynb)\nFile &gt; Save and Export Notebook &gt; HTML\n\nDelete the Jupyter-pyspark service\n\nSSPCloud &gt; My services &gt; Delete"
  },
  {
    "objectID": "docs/lab/lab-end.html#end-of-the-lab",
    "href": "docs/lab/lab-end.html#end-of-the-lab",
    "title": "Big Data",
    "section": "",
    "text": "Export your notebook\n\nRight clic and Download (.ipynb)\nFile &gt; Save and Export Notebook &gt; HTML\n\nDelete the Jupyter-pyspark service\n\nSSPCloud &gt; My services &gt; Delete"
  },
  {
    "objectID": "docs/lab/aws-cluster-spark.html",
    "href": "docs/lab/aws-cluster-spark.html",
    "title": "Create a Spark cluster on AWS",
    "section": "",
    "text": "Important\n\n\n\nWe strongly recommend using the Google Chrome browser!"
  },
  {
    "objectID": "docs/lab/aws-cluster-spark.html#chrome-setup",
    "href": "docs/lab/aws-cluster-spark.html#chrome-setup",
    "title": "Create a Spark cluster on AWS",
    "section": "1 Chrome Setup",
    "text": "1 Chrome Setup\n\nOpen Chrome\nIn the top right, click on the 3 dots, then select Settings.\nClick on Privacy and security, then Site settings, then Pop-ups and redirects.\nWebsites can show pop-ups and use redirects"
  },
  {
    "objectID": "docs/lab/aws-cluster-spark.html#run-the-aws-learner-lab",
    "href": "docs/lab/aws-cluster-spark.html#run-the-aws-learner-lab",
    "title": "Create a Spark cluster on AWS",
    "section": "2 Run the AWS Learner Lab",
    "text": "2 Run the AWS Learner Lab\n\nLog in AWS academy\n\nuse your ENSAI mail\n\nGo to the Dashboard\n\nSelect course AWS Academy Learner Lab [78065]\nClick on Learner Lab\nClick on Start Lab ▶️\nWait for 2 minutes until the circle next to AWS turns green\nClick on AWS\n\n\nNow, you enter the lab."
  },
  {
    "objectID": "docs/lab/aws-cluster-spark.html#create-a-studio",
    "href": "docs/lab/aws-cluster-spark.html#create-a-studio",
    "title": "Create a Spark cluster on AWS",
    "section": "3 Create a Studio",
    "text": "3 Create a Studio\n\nIn the search bar, search for the EMR service\nClick on Studios, then Create Studio\nSetup options ➡️ select Custom\nStudio settings\n\nService role to let Studio access your AWS resources : LabRole\n\nNetworking and security\n\nVPC : select the default VPC\nSubnets : choose one and note its name for later\nClick on Create Studio"
  },
  {
    "objectID": "docs/lab/aws-cluster-spark.html#create-a-cluster",
    "href": "docs/lab/aws-cluster-spark.html#create-a-cluster",
    "title": "Create a Spark cluster on AWS",
    "section": "4 Create a Cluster",
    "text": "4 Create a Cluster\n\nClick on Clusters, then Create cluster\nGo to the Cluster Configuration section\n\nFor Primary, Core and Task 1 of 1, choose EC2 instance type : m4.large\n\nNetworking section\n\nUse the same subnet as before\n\nSecurity configuration and EC2 key pair section\n\nAmazon EC2 key pair for SSH to the cluster : vockey\n\nIdentity and Access Management (IAM) roles section\n\nService role : EMR_DefaultRole\nInstance profile : EMR_EC2_DefaultRole\n\nCreate Cluster\n\nThe cluster creation takes some time (10 min).\nGo back to EMR &gt; Cluster and wait until the cluster status is Waiting (refresh 🔄)"
  },
  {
    "objectID": "docs/lab/aws-cluster-spark.html#associate-the-cluster-with-a-workspace",
    "href": "docs/lab/aws-cluster-spark.html#associate-the-cluster-with-a-workspace",
    "title": "Create a Spark cluster on AWS",
    "section": "5 Associate the cluster with a workspace",
    "text": "5 Associate the cluster with a workspace\n\nClick on Workspaces (Notebooks)\nSelect the workspace\n\nclick on Actions &gt; Stop\nRefresh until the status turn to Idle\n\nSelect the workspace, then click on Attach cluster\n\nCheck *Launch in Jupyter`\nEMR cluster : your cluster\nAttach cluster and launch\n\n\nA new tab will open. Save the URL, for example by bookmarking it.\nNow you can load the lab notebooks (lab_-AWS.ipynb file) and then open it. Once it’s open, verify that the kernel is set to PySpark on the top right."
  },
  {
    "objectID": "docs/duckdb.html",
    "href": "docs/duckdb.html",
    "title": "DuckDB",
    "section": "",
    "text": "🚧"
  },
  {
    "objectID": "docs/duckdb.html#shell",
    "href": "docs/duckdb.html#shell",
    "title": "DuckDB",
    "section": "Shell",
    "text": "Shell\n\nhttps://shell.duckdb.org/\n\nExemple avec ces données - https://www.data.gouv.fr/fr/datasets/populations-legales-communales-2017-2021/\nFROM 'https://static.data.gouv.fr/resources/populations-legales-communales-2017-2021/20240122-151058/poplegales2017-2021.parquet'\nLIMIT 10;\nFROM 'https://static.data.gouv.fr/resources/populations-legales-communales-2017-2021/20240122-151058/poplegales2017-2021.parquet'\nWHERE codgeo = '28088';\nFROM 'https://static.data.gouv.fr/resources/populations-legales-communales-2017-2021/20240122-151058/poplegales2017-2021.parquet'\nWHERE annee_rp = 2021 \nAND LOWER(libgeo) NOT LIKE '%arrondissement%'\nORDER BY ptot DESC\nLIMIT 20;\nBonus :\n\nbouton SAVE URL\nurl un peu longue donc utiliser https://tinyurl.com/app pour raccourcir\ndiffuser l’url"
  },
  {
    "objectID": "docs/duckdb.html#exécutable",
    "href": "docs/duckdb.html#exécutable",
    "title": "DuckDB",
    "section": "Exécutable",
    "text": "Exécutable\nTélécharger duckdb.exe\nFROM 'https://static.data.gouv.fr/resources/populations-legales-communales-2017-2021/20240122-151058/poplegales2017-2021.parquet'\nSELECT SUM(pnum)\nGROUP BY annee_rp\nORDER BY 1 DESC;"
  },
  {
    "objectID": "docs/duckdb.html#dbeaver",
    "href": "docs/duckdb.html#dbeaver",
    "title": "DuckDB",
    "section": "DBeaver",
    "text": "DBeaver\nhttps://duckdb.org/docs/guides/sql_editors/dbeaver.html\n\nNouvelle connexion : DuckDB\n\nPath : :memory:\n\nOuvrir la connexion\nOuvrir un éditeur SQL\n\n FROM 'https://static.data.gouv.fr/resources/populations-legales-communales-2017-2021/20240122-151058/poplegales2017-2021.parquet'\nSELECT codreg,\n       SUM(pmun) pop\n WHERE annee_rp = 2021\n GROUP BY ALL\n ORDER BY ALL;\n\nAjouter une ligne de total\n\nFROM 'https://static.data.gouv.fr/resources/populations-legales-communales-2017-2021/20240122-151058/poplegales2017-2021.parquet'\nSELECT codreg,\n       SUM(pmun) pop\n WHERE annee_rp = 2021\n GROUP BY GROUPING SETS((codreg),())\n ORDER BY ALL; \nJointure avec les noms de régions\nFROM read_json_auto('https://geo.api.gouv.fr/regions');\n\nFROM 'https://static.data.gouv.fr/resources/populations-legales-communales-2017-2021/20240122-151058/poplegales2017-2021.parquet'\nJOIN read_json_auto('https://geo.api.gouv.fr/regions') ON (codreg=code)\nSELECT codreg,\n       nom,\n       SUM(pmun) pop\n WHERE annee_rp = 2021\n GROUP BY GROUPING SETS((codreg, nom),())\n ORDER BY ALL;\nRemplacer les NULL de la dernière ligne\nWITH popreg AS(\n    FROM 'https://static.data.gouv.fr/resources/populations-legales-communales-2017-2021/20240122-151058/poplegales2017-2021.parquet'\n    JOIN read_json_auto('https://geo.api.gouv.fr/regions') ON (codreg = code)\n    SELECT codreg,\n           nom,\n           SUM(pmun) pop\n     WHERE annee_rp = 2021\n     GROUP BY GROUPING SETS((codreg, nom),())\n     ORDER BY ALL\n )\nFROM popreg\nSELECT * REPLACE(IFNULL(codreg, 'FR') AS codreg, IFNULL(nom, 'France') AS nom);"
  },
  {
    "objectID": "docs/lab/lab-create-spark-session.html",
    "href": "docs/lab/lab-create-spark-session.html",
    "title": "Create a Spark session",
    "section": "",
    "text": "Depending on the chosen platform, initialize the Spark session\n\n\nOnly on SSPCloud\n\nimport os\nfrom pyspark.sql import SparkSession\n\nspark = (SparkSession\n         .builder\n         # default url of the internally accessed Kubernetes API\n         # (This Jupyter notebook service is itself a Kubernetes Pod)\n         .master(\"k8s://https://kubernetes.default.svc:443\")\n         # Executors spark docker image: for simplicity reasons, this jupyter notebook is reused\n         .config(\"spark.kubernetes.container.image\", os.environ['IMAGE_NAME'])\n         # Name of the Kubernetes namespace\n         .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\n         # Allocated memory to the JVM\n         # Stay careful, by default, the Kubernetes pods has a higher limit which depends on other parameters.\n         .config(\"spark.executor.memory\", \"4g\")\n         .config(\"spark.kubernetes.driver.pod.name\", os.environ['KUBERNETES_POD_NAME'])\n         # dynamic allocation configuration\n         .config(\"spark.dynamicAllocation.enabled\",\"true\")\n         .config(\"spark.dynamicAllocation.initialExecutors\",\"1\")\n         .config(\"spark.dynamicAllocation.minExecutors\",\"1\")\n         .config(\"spark.dynamicAllocation.maxExecutors\",\"60\")\n         # Ratio match the number of pods to create for a given number of parallel tasks\n         # (100 parallel, ratio of 1, one aims at 100 pods, with 0.5 it would be 50 pods)\n         .config(\"spark.dynamicAllocation.executorAllocationRatio\",\"1\")\n         .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",\"true\")\n         .getOrCreate()\n        )\n\n\n\nOnly on AWS\n\n#Spark session\nspark\n\n# Configuraion\nspark._jsc.hadoopConfiguration().set(\"fs.s3.useRequesterPaysHeader\",\"true\")\n\n\n\nCheck spark session\n\nspark"
  },
  {
    "objectID": "docs/lab/lab-setup.html",
    "href": "docs/lab/lab-setup.html",
    "title": "Lab setup",
    "section": "",
    "text": "You can choose to use :"
  },
  {
    "objectID": "docs/lab/lab-setup.html#before-starting-the-lab",
    "href": "docs/lab/lab-setup.html#before-starting-the-lab",
    "title": "Lab setup",
    "section": "Before starting the lab",
    "text": "Before starting the lab\n\nSSPCloud\n\nconnect to SSPCloud\n\nif necessary, create an account with your ENSAI e-mail address\n\nGo to Mes services\n\nCreate a new service : Jupyter-pyspark\nWait for a few seconds\nClic on Copier le mot de passe and then Ouvrir le service\nOn the next page, paste it in the appropriate place and Log in\nYou are now in your Jupyter service\n\nImport file named lab_-SSPCloud.ipynb\n\nopen it and follow the instructions\n\n\n\n\nAWS\nFor the first time, it will take some time to create your Spark cluster.\nFollow these instructions to create a Spark cluster on AWS"
  },
  {
    "objectID": "docs/lab/lab-setup.html#at-this-end-of-this-lab",
    "href": "docs/lab/lab-setup.html#at-this-end-of-this-lab",
    "title": "Lab setup",
    "section": "At this end of this lab",
    "text": "At this end of this lab\n\nExport your notebook\n\nFile &gt; Export Notebook As … &gt; Export Notebook To HTML\nRight clic &gt; Download the .ipynbfile\n\nDelete your service (SSPCloud)\nTurn your cluster off (AWS)"
  },
  {
    "objectID": "docs/lab/lab1/lab1-correction.html",
    "href": "docs/lab/lab1/lab1-correction.html",
    "title": "First steps with Spark - Correction",
    "section": "",
    "text": "Download this Jupyter Notebook\nFollow these instructions"
  },
  {
    "objectID": "docs/lab/lab1/lab1-correction.html#before-you-start",
    "href": "docs/lab/lab1/lab1-correction.html#before-you-start",
    "title": "First steps with Spark - Correction",
    "section": "",
    "text": "Download this Jupyter Notebook\nFollow these instructions"
  },
  {
    "objectID": "docs/lab/lab1/lab1-correction.html#what-is-spark",
    "href": "docs/lab/lab1/lab1-correction.html#what-is-spark",
    "title": "First steps with Spark - Correction",
    "section": "1 What is Spark ?",
    "text": "1 What is Spark ?\nLet’s keep it simple.\nThe intuitive way: Spark distributes tasks to different Worker nodes (machine executing the tasks).\nThe Kubernetes way: Spark distributes tasks to different containers. The location of containers among the different worker nodes is then handled by Kubernetes. The more computation power you need, the more containers get created. It’s a smooth way to save Worker nodes resources. With Kubernetes, we just speak in terms of Pods rathen than containers.\nPretty handy right ? If you want to have a deeper look on how it’s done, you can by clicking on the link bellow but you still have some work to do so don’t waste your time ! You can keep it to the end.\n\n\nIf you want more ! \n\n\nSpark Context\nThe Spark Context is an object that hides the complexity of the underlying infrastructure to perform computation from the Data Scientist.\nThis Spark context is a JVM process that gives access to a Spark driver which schedules the tasks and spans tasks across Worker nodes through Executors. In brief, the Spark driver communicates with all the Worker nodes.\nEach Worker node consists of one or more Executor(s) who are responsible for running the Task. Executors register themselves with Driver. The Driver has all the information about the Executors at all the time.\nThis working combination of Driver and Workers is known as Spark Application.\nJVM: Java virtual machine that load, verifies and executes Java bytecode.\n\nThe Spark Application is launched with the help of the Cluster Manager. Spark is dependent on the Cluster Manager to launch the Executors and also the Driver (in Cluster mode).\nSpark can be run with any of the 5 following Cluster Manager :\n\nlocal : Driver use CPU threads on your local machine\nSpark Standalone Mode : A basic resource manager provided by Spark\nYARN : the historical resource manager commonly used in traditional Big Data infrastructures (Courdera or Hortonworks cluster for instance)\nMesos : the resource manager from Berkeley University\nKubernetes : the game changing containers orchestrator\n\n\n\nSpark on a Kubernetes cluster\n\nThe Spark driver runs on a Kubernetes Pod and creates executors running within Kubernetes pods. In simple terms, Spark driver is in a Pod and it distributes tasks to different Pods. The location of Pods among the different worker nodes is then handled by Kubernetes.\nThe Kubernetes Scheduler checks if pods are assigned to nodes. For every Pod that the scheduler discovers, the scheduler is responsible of finding the best node for that Pod to run on.\nThe Kubernetes API Server allows to interact with Pods. It schedules executor Pods by creating or deleting Pods."
  },
  {
    "objectID": "docs/lab/lab1/lab1-correction.html#create-a-spark-session",
    "href": "docs/lab/lab1/lab1-correction.html#create-a-spark-session",
    "title": "First steps with Spark - Correction",
    "section": "2 Create a Spark session",
    "text": "2 Create a Spark session\n\nDepending on the chosen platform, initialize the Spark session\n\n\n2.1 Only on SSPCloud\n\nimport os\nfrom pyspark.sql import SparkSession\n\nspark = (SparkSession\n         .builder\n         # default url of the internally accessed Kubernetes API\n         # (This Jupyter notebook service is itself a Kubernetes Pod)\n         .master(\"k8s://https://kubernetes.default.svc:443\")\n         # Executors spark docker image: for simplicity reasons, this jupyter notebook is reused\n         .config(\"spark.kubernetes.container.image\", os.environ['IMAGE_NAME'])\n         # Name of the Kubernetes namespace\n         .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\n         # Allocated memory to the JVM\n         # Stay careful, by default, the Kubernetes pods has a higher limit which depends on other parameters.\n         .config(\"spark.executor.memory\", \"4g\")\n         .config(\"spark.kubernetes.driver.pod.name\", os.environ['KUBERNETES_POD_NAME'])\n         # dynamic allocation configuration\n         .config(\"spark.dynamicAllocation.enabled\",\"true\")\n         .config(\"spark.dynamicAllocation.initialExecutors\",\"1\")\n         .config(\"spark.dynamicAllocation.minExecutors\",\"1\")\n         .config(\"spark.dynamicAllocation.maxExecutors\",\"60\")\n         # Ratio match the number of pods to create for a given number of parallel tasks\n         # (100 parallel, ratio of 1, one aims at 100 pods, with 0.5 it would be 50 pods)\n         .config(\"spark.dynamicAllocation.executorAllocationRatio\",\"1\")\n         .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",\"true\")\n         .getOrCreate()\n        )\n\n\n\n2.2 Only on AWS\n\n#Spark session\nspark\n\n# Configuraion\nspark._jsc.hadoopConfiguration().set(\"fs.s3.useRequesterPaysHeader\",\"true\")\n\n\n\n2.3 Check spark session\n\nspark"
  },
  {
    "objectID": "docs/lab/lab1/lab1-correction.html#first-steps-with-spark---data-importation",
    "href": "docs/lab/lab1/lab1-correction.html#first-steps-with-spark---data-importation",
    "title": "First steps with Spark - Correction",
    "section": "3 First steps with Spark - Data importation",
    "text": "3 First steps with Spark - Data importation\nSpark’s main object class is the DataFrame, which is a distributed table. It is analogous to R’s or Python (Pandas)’s data frames:\n\none row represents an observation,\none column represents a variable.\n\nBut contrary to R or Python, Spark’s DataFrames can be distributed over hundred of nodes.\nSpark support multiple data formats, and multiple ways to load them.\n\ndata format : csv, json, parquet (an open source column oriented format)\ncan read archive files\nschema detection or user defined schema. For static data, like a json file, schema detection can be use with good results.\n\nSpark has multiple syntaxes to import data. Some are simple with no customisation, others are more complexes but you can specify options.\nThe simplest syntaxes to load a json or a csv file are :\n# JSON\njson_df = spark.read.json([location of the file])\n# csv\ncsv_df = spark.read.csv([location of the file])\nIn the future, you may consult the Data Source documentation to have the complete description of Spark’s reading abilities.\nThe data you will use in this lab are real data from the twitter sampled stream API and filtered stream API. The tweets folder contains more than 50 files and more than 2 million tweets. The tweets was collected between the 14/04/2021 and the 18/04/2021. The total collection time was less than 10 hours.\n\n\n3.1 ✍Hands-on 1 - Data importation\n\nLoad the json file stored here :\n\ns3a://ludo2ne/diffusion/tweets.jsonl.gz for SSPCloud\ns3://spark-lab-input-data-ensai20222023/tweets/tweets20220324-155940.jsonl.gz for AWS\nName you data frame df_tweet\nUse function cache() on the data frame. Caching is a performance optimization technique that allows you to persist an intermediate or final result of a computation in memory, reducing the need to recompute the data when it is accessed multiple time\n\n ⚙️ This file is an a JSONL (JSON-line) format, which means that each line of it is a JSON object. A JSON object is just a Python dictionary or a JavaScript object and looks like this: { key1: value1, key2: [\"array\", \"of\", \"many values]}). This file has been compressed into a GZ archive, hence the .jsonl.gz ending. Also this file is not magically appearing in your S3 storage. It is hosted on one of your teacher’s bucket and has been made public, so that you can access it.\n\nIt’s possible to load multiple file in a unique DataFrame. It’s useful when you have daily files and want to process them all. It’s the same syntax as the previous one, just specify a folder.\n\nSSPCloud users : if you meet some issue to load this file, you can load and use your own file :\n\nIn Onyxia, mes fichiers\nLoad file tweets.jsonl.gz\nIn Jupyter, read it using s3a://&lt;user_name&gt;/tweets.jsonl.gz\n\n\n\n# DataFrame creation\ndf_tweet = spark.read.json(\"s3a://ludo2ne/diffusion/tweets.jsonl.gz\")\n# or df_tweet = spark.read.json(\"s3://spark-lab-input-data-ensai20222023/tweets/tweets20220324-155940.jsonl.gz\")\n\n# caching\ndf_tweet.cache()"
  },
  {
    "objectID": "docs/lab/lab1/lab1-correction.html#data-frame-basic-manipulations",
    "href": "docs/lab/lab1/lab1-correction.html#data-frame-basic-manipulations",
    "title": "First steps with Spark - Correction",
    "section": "4 Data frame basic manipulations",
    "text": "4 Data frame basic manipulations\nIf DataFrames are immutable, they can however be transformed in other DataFrames, in the sense that a modified copy is returned. Such transformations include: filtering, sampling, dropping columns, selecting columns, adding new columns…\nFirst, you can get information about the columns with:\ndf.columns       # get the column names\ndf.schema        # get the column names and their respective type\ndf.printSchema() # same, but human-readable\nYou can select columns with the select() method. It takes as argument a list of column name. For example :\ndf_with_less_columns = df\\\n  .select(\"variable3\",\"variable_four\",\"variable-6\")\n\n# Yes, you do need the ugly \\ at the end of the line,\n# if you want to chain methods between lines in Python\nYou can get nested columns easily with :\ndf.select(\"parentField.nestedField\")\nTo filter data you could use the filter() method. It take as input an expression that gets evaluated for each observation and should return a boolean. Sampling is performed with the sample() method. For example :\ndf_with_less_rows = df\\\n  .sample(fraction=0.001)\\\n  .filter(df.variable1==\"value\")\\\n  .show(10)\nAs said before your data are distributed over multiple nodes (executors) and data inside a node are split into partitions. Then each transformations will be run in parallel. They are called narrow transformation For example, to sample a DataFrame, Spark sample every partitions in parallel because sample all partition produce the sample DataFrame. For some transformations, like groupBy() it’s impossible, and it’s cannot be run in parallel.\n\n\n4.1 Lazy evaluation\nThis is because Spark has what is known as lazy evaluation, in the sense that it will wait as much as it can before performing the actual computation. Said otherwise, when you run an instruction such as:\ntweet_author_hashtags = df_tweet_big.select(\"auteur\",\"hashtags\")\n… you are not executing anything! Rather, you are building an execution plan, to be realised later.\nSpark is quite extreme in its laziness, since only a handful of methods called actions, by opposition to transformations, will trigger an execution. The most notable are:\n\ncollect(), explicitly asking Spark to fetch the resulting rows instead of to lazily wait for more instructions,\ntake(n), asking for n first rows\nfirst(), an alias for take(1)\nshow() and show(n), human-friendly alternatives\ncount(), asking for the numbers of rows\nall the “write” methods (write on file, write to database), see here for the list\n\nThis has advantages: on huge data, you don’t want to accidently perform a computation that is not needed. Also, Spark can optimize each stage of the execution in regard to what comes next. For instance, filters will be executed as early as possible, since it diminishes the number of rows on which to perform later operations. On the contrary, joins are very computation-intense and will be executed as late as possible. The resulting execution plan consists in a directed acyclic graph (DAG) that contains the tree of all required actions for a specific computation, ordered in the most effective fashion.\nThis has also drawbacks. Since the computation is optimized for the end result, the intermediate stages are discarded by default. So if you need a DataFrame multiple times, you have to cache it in memory because if you don’t Spark will recompute it every single time.\n\n\n\n4.2 ✍Hands-on 2 - Data frame basic manipulations\n\nHow many rows have your two DataFrame ?\n\n\ndf_tweet.count()\n\n\nDisplay columns names and then the schema\n\n\ndf_tweet.columns\n\n\ndf_tweet.printSchema()\n\n\nDisplay 10 rows of df_tweet\n\n\ndf_tweet.show(10)\n\n\nSample df_tweet and keep only 10% of it.\n\nCreate a new DataFrame named df_tweet_sampled.\nIf computations take too long on the full DataFrame, use this one instead or add a sample transformation in your expression.\n\n\n\ndf_tweet_sampled = df_tweet\\\n  .sample(fraction=0.1)\n\ndf_tweet_sampled.count()\n\n\nDefine a DataFrame tweet_author_hashtags with only the auteur and hashtags columns\n\nThen display 5 rows\n\n\n\ntweet_author_hashtags = df_tweet.select(\"auteur\", \"hashtags\")\n\ntweet_author_hashtags.show(5)\n\n\nPrint 5 lines of a df_tweet with only the auteur, mentions, and urls columns.\n\nmentions and urls are both nested columns in entities\n\n\n\ndf_tweet\\\n  .select(\"auteur\",\n          \"entities.mentions\",\n          \"entities.urls\")\\\n  .show(5)\n\n\nFilter df_tweet and keep only tweets with more than 1 like.\n\nDisplay only auteur, contenu and like_count\nPrint 10 lines\n\n\n\ndf_tweet\\\n  .filter(df_tweet.like_count &gt;= 1)\\\n  .show(10)"
  },
  {
    "objectID": "docs/lab/lab1/lab1-correction.html#basic-dataframe-column-manipulation",
    "href": "docs/lab/lab1/lab1-correction.html#basic-dataframe-column-manipulation",
    "title": "First steps with Spark - Correction",
    "section": "5 Basic DataFrame column manipulation",
    "text": "5 Basic DataFrame column manipulation\nYou can add/update/rename column of a DataFrame using spark :\n\nDrop : df.drop(columnName : str )\nRename : df.withColumnRenamed(oldName : str, newName : str)\nAdd/update : df.withColumn(columnName : str, columnExpression)\n\nFor example\n\ndf_tweet\\\n  .withColumn(                                        # computes new variable\n    \"like_rt_ratio\",                                  # like_rt_ratio \"OVERCONFIDENCE\"\n    df_tweet.like_count /df_tweet.retweet_count)\n\nSee here for the list of all functions available in an expression.\n\n5.1 ✍Hands-on 3 - Basic DataFrame column manipulation\n\nDefine a DataFrame with a column names interaction_count named df_tweet_interaction_count\n\nThis column is the sum of like_count, reply_count and retweet_count.\n\n\n\ndf_tweet_interaction_count = df_tweet\\\n  .drop(\"other\")\\\n  .filter(df_tweet.like_count &gt;= 1)\\\n  .filter(df_tweet.retweet_count &gt;= 1)\\\n  .withColumn(\n    \"interaction_count\",\n    df_tweet.like_count + df_tweet.reply_count + df_tweet.retweet_count)\n\ndf_tweet_interaction_count\\\n  .show(10)\n\n\nUpdate the DataFrame you imported at the beginning of this lab and drop the other column\n\n\ndf_tweet\\\n  .drop(\"other\")\\\n  .show(10)"
  },
  {
    "objectID": "docs/lab/lab1/lab1-correction.html#advance-dataframe-column-manipulation",
    "href": "docs/lab/lab1/lab1-correction.html#advance-dataframe-column-manipulation",
    "title": "First steps with Spark - Correction",
    "section": "6 Advance DataFrame column manipulation",
    "text": "6 Advance DataFrame column manipulation\n\n6.1 Array manipulation\nSome columns often contain arrays (lists) of values instead of just one value. This may seem surprising but this actually quite natural. For instance, you may create an array of words from a text, or generate a list of random numbers for each observation, etc.\nYou may create array of values with:\n\nsplit(text : string, delimiter : string), turning a text into an array of strings\n\nYou may use array of values with:\n\nsize(array : Array), getting the number of elements\narray_contains(inputArray : Array, value : any), checking if some value appears\nexplode(array : Array), unnesting an array and duplicating other values. For instance if you use explode() over the hashtags value of this DataFrame:\n\n\n\nAuteur\nContenu\nHashtags\n\n\n\n\nBob\nI love #Spark and #bigdata\n[Spark, bigdata]\n\n\nAlice\nJust finished #MHrise, best MH ever\n[MHrise]\n\n\n\nYou will get :\n\n\n\n\n\n\n\n\n\nAuteur\nContenu\nHashtags\nHashtag\n\n\n\n\nBob\nI love #Spark and #bigdata\n[Spark, bigdata]\nSpark\n\n\nBob\nI love #Spark and #bigdata\n[Spark, bigdata]\nbigdata\n\n\nAlice\nJust finished #MHrise, best MH ever\n[MHrise]\nMHrise\n\n\n\n\nAll this functions must be imported first :\nfrom pyspark.sql.functions import split, explode, size, array_contains\nDo not forget, to create a new column, you should use withColumn(). For example :\ndf.withColumn(\"new column\", explode(\"array\"))\n\n✍Hands-on 4 - Array manipulation\n\nKeep all the tweets with hashtags and for each remaining line, split the hashtag text into an array of hashtags\n\n\nfrom pyspark.sql.functions import split, explode, size, array_contains\n\ndf_tweet\\\n  .filter(size(\"hashtags\") &gt; 0)\\\n  .withColumn(\n    \"hashtag_exploded\",\n    explode(\"hashtags\"))\\\n  .show(10)\n\n\nCreate a new column with the number of words of the contenu column. (Use split() + size())\n\n\ndf_tweet\\\n  .withColumn(\"word_count\",\n              size(split(\"contenu\", \" \")))\\\n  .show(5)\n\n\nCount how many tweet contain the Ukraine hashtag (use the count() action)\n\n\ndf_tweet\\\n  .filter(array_contains(\"hashtags\", \"Ukraine\"))\\\n  .count()\n\n\n\n\n6.2 User defined function\nFor more very specific column manipulation you will need Spark’s udf() function (User Defined Function). It can be useful if Spark does not provide a feature you want. But Spark is a popular and active project, so before coding an udf, go check the documentation. For instance for natural language processing, Spark already has some functions. Last things, python udf can lead to performance issues (see https://stackoverflow.com/a/38297050) and learning a little bit of scala or java can be a good idea.\nFor example :\n# !!!! DOES NOT WORK !!!!\ndef to_lower_case(string):\n    return string.lower()\n\ndf.withColumn(\"tweet_lower_case\", to_lower_case(df.contenu))\nwill just crash. Keep in mind that Spark is a distributed system, and that Python is only installed on the central node, as a convenience to let you execute instructions on the executor nodes. But by default, pure Python functions can only be executed where Python is installed! We need udf() to enable Spark to send Python instructions to the worker nodes.\nLet us see how it is done :\n# imports\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.functions import explode\nfrom pyspark.sql.types import StringType\n\n# pure python functions\ndef to_lower_case(string):\n    return string.lower()\n\n# user defined function(we use a lambda function to create the udf)\nto_lower_case_udf = udf(\n    lambda x: to_lower_case(x), StringType()\n)\n\n# df manipulation\ndf_tweet_small\\\n  .select(\"auteur\",\"hashtags\")\\\n  .filter(\"size(hashtags)!=0\")\\\n  .withColumn(\"hashtag\", explode(\"hashtags\"))\\\n  .withColumn(\"hashtag\", to_lower_case_udf(\"hashtag\")).show(10)\n\n\n✍Hands-on 5 - User defined function\n\nCreate an user defined function that counts how many words a tweet contains.\n\nyour function will return an IntegerType and not a StringType\n\n\n\n# imports\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import IntegerType\n\n# pure python functions\ndef word_count(string):\n    return len(string.split(\" \"))\n\n# user definid function\nword_count_udf = udf(\n    lambda x: word_count(x), IntegerType()\n)\n\n# df manipulation\ndf_tweet\\\n  .withColumn(\"word_count\",\n              word_count_udf(\"contenu\"))\\\n  .show(10)"
  },
  {
    "objectID": "docs/lab/lab1/lab1-correction.html#aggregation-functions",
    "href": "docs/lab/lab1/lab1-correction.html#aggregation-functions",
    "title": "First steps with Spark - Correction",
    "section": "7 Aggregation functions",
    "text": "7 Aggregation functions\nSpark offer a variety of aggregation functions :\n\ncount(column : string) will count every not null value of the specify column. You cant use count(1) of count(\"*\") to count every line (even row with only null values)\ncountDisctinct(column : string) and approx_count_distinct(column : string, percent_error: float). If the exact number is irrelevant, approx_count_distinct()should be preferred.\nCounting distinct elements cannot be done in parallel, and need a lot data transfer. But if you only need an approximation, there is a algorithm, named hyper-log-log (more info here) that can be parallelized.\nfrom pyspark.sql.functions import count, countDistinct, approx_count_distinct\n\ndf.select(count(\"col1\")).show()\ndf.select(countDistinct(\"col1\")).show()\ndf.select(approx_count_distinct(\"col1\"), 0.1).show()\nYou have access to all other common functions min(), max(), first(), last(), sum(), sumDistinct(), avg() etc (you should import them first from pyspark.sql.functions import min, max, avg, first, last, sum, sumDistinct)\n\n\n\n7.1 ✍Hands-on 6 - Aggregation functions\n\nWhat are the min, max, average of interaction_count (use df_tweet_interaction_count created earlier)\n\ndon’t forget to import the required functions\n\n\n\nfrom pyspark.sql.functions import min, max, avg, first, last, sum, sumDistinct\n\ndf_tweet_interaction_count\\\n  .select(min(\"interaction_count\"),\n          max(\"interaction_count\"),\n          avg(\"interaction_count\"))\\\n  .first()\n\n\nHow many tweets have hashtags ?\n\nDistinct hashtags ?\nTry the approximative count with 0.1 and 0.01 as maximum estimation error allowed.\n\n\n\nfrom pyspark.sql.functions import count, countDistinct, approx_count_distinct\n\ndf_tweet\\\n  .select(count(\"hashtags\"),\n          countDistinct(\"hashtags\"),\n          approx_count_distinct(\"hashtags\", 0.1),\n          approx_count_distinct(\"hashtags\",0.01))\\\n  .show()"
  },
  {
    "objectID": "docs/lab/lab1/lab1-correction.html#grouping-functions",
    "href": "docs/lab/lab1/lab1-correction.html#grouping-functions",
    "title": "First steps with Spark - Correction",
    "section": "8 Grouping functions",
    "text": "8 Grouping functions\nLike SQL you can group row by a criteria with Spark. Just use the groupBy(column : string) method. Then you can compute some aggregation over those groups.\ndf\\\n  .groupBy(\"col1\")\\\n  .agg(count(\"col2\").alias(\"quantity\"))\\           # alias is use to specify the name of the new column\n  .show()\nThe agg() method can take multiples argument to compute multiple aggregation at once.\ndf\\\n  .groupBy(\"col1\")\\\n  .agg(count(\"col2\").alias(\"quantity\"),\n       min(\"col2\").alias(\"min\"),\n       avg(\"col3\").alias(\"avg3\"))\\\n  .show()\nAggregation and grouping transformations work differently than the previous method like filter(), select(), withColumn() etc. Those transformations cannot be run over each partitions in parallel, and need to transfer data between partitions and executors. They are called “wide transformations”\n\n\n8.1 ✍Hands-on 7 - Grouping functions\n\nCompute a daframe with the min, max and average retweet of each auteur.\n\nThen order it by the max number of retweet in descending order by .\nTo do that you can use the following syntax\n\nfrom pyspark.sql.functions import desc\ndf.orderBy(desc(\"col\"))\n\n\nfrom pyspark.sql.functions import desc\n\ndf_tweet\\\n  .groupBy(\"auteur\")\\\n  .agg(min(\"retweet_count\").alias(\"min_RT\"),\n       max(\"retweet_count\").alias(\"max_RT\"),\n       avg(\"retweet_count\").alias(\"avg_RT\"))\\\n  .orderBy(desc(\"max_RT\"))\\\n  .show(5)"
  },
  {
    "objectID": "docs/lab/lab1/lab1-correction.html#spark-sql",
    "href": "docs/lab/lab1/lab1-correction.html#spark-sql",
    "title": "First steps with Spark - Correction",
    "section": "9 Spark SQL",
    "text": "9 Spark SQL\nSpark understand SQL statement. It’s not a hack nor a workaround to use SQL in Spark, it’s one a the more powerful feature in Spark. To use SQL you will need :\n\nRegister a view pointing to your DataFrame\nmy_df.createOrReplaceTempView(viewName : str)\nUse the sql function\nspark.sql(\"\"\"\nYour SQL statement\n\"\"\")\nYou could manipulate every registered DataFrame by their view name with plain SQL.\n\nIn fact you can do most of this tutorial without any knowledge in PySpark nor Spark. Many things can only be done in Spark if you know SQL and how to use it in Spark.\n\n9.1 ✍Hands-on 8 - Spark SQL\n\nHow many tweets have hashtags ?\n\nDistinct hashtags ?\n\n\n\ndf_tweet\\\n  .select(\"contenu\", \"hashtags\")\\\n  .createOrReplaceTempView(\"view_hashtag_content\")\n\nspark.sql(\"\"\"\nSELECT COUNT(*),\n       COUNT(DISTINCT(contenu))\n  FROM view_hashtag_content\n WHERE size(hashtags) &gt; 0\n\"\"\")\\\n  .show(5)\n\n\nCompute a dataframe with the min, max and average retweet of each auteur using Spark SQL\n\n\ndf_tweet.createOrReplaceTempView(\"view_tweet\")\n\nspark.sql(\"\"\"\nSELECT min(retweet_count),\n       max(retweet_count),\n       avg(retweet_count)\n  FROM view_tweet\n GROUP BY auteur\n ORDER BY MAX(retweet_count) DESC\n\"\"\")\\\n  .show(5)"
  },
  {
    "objectID": "docs/lab/lab1/lab1-correction.html#end-of-the-lab",
    "href": "docs/lab/lab1/lab1-correction.html#end-of-the-lab",
    "title": "First steps with Spark - Correction",
    "section": "End of the Lab",
    "text": "End of the Lab\n\nExport your notebook\n\nRight clic and Download (.ipynb)\nFile &gt; Save and Export Notebook &gt; HTML\n\nDelete the Jupyter-pyspark service\n\nSSPCloud &gt; My services &gt; Delete"
  },
  {
    "objectID": "docs/lab/lab2/lab2-correction.html",
    "href": "docs/lab/lab2/lab2-correction.html",
    "title": "Spark ML",
    "section": "",
    "text": "Download this Jupyter Notebook\nFollow these instructions\n\n💡 Some transparent images may be difficult to view in dark mode. Feel free to switch to light mode using the button at the top right."
  },
  {
    "objectID": "docs/lab/lab2/lab2-correction.html#before-you-start",
    "href": "docs/lab/lab2/lab2-correction.html#before-you-start",
    "title": "Spark ML",
    "section": "",
    "text": "Download this Jupyter Notebook\nFollow these instructions\n\n💡 Some transparent images may be difficult to view in dark mode. Feel free to switch to light mode using the button at the top right."
  },
  {
    "objectID": "docs/lab/lab2/lab2-correction.html#outline",
    "href": "docs/lab/lab2/lab2-correction.html#outline",
    "title": "Spark ML",
    "section": "Outline",
    "text": "Outline\nIn this tutorial, we are going to perform exploratory and explanatory analyses of a massive dataset consisting in hundreds of thousands of AirBnB listings, as made available by the Inside AirBnB project.\nRémi Pépin has loaded a lot these listings on AWS at this address:\n\ns3://spark‑lab‑input‑data‑ensai20222023/airbnbb/ on AWS\ns3a://remipepin/diffusion/ensai/airbnb on SSPCloud"
  },
  {
    "objectID": "docs/lab/lab2/lab2-correction.html#how-to-distribute-elementary-statistical-tasks",
    "href": "docs/lab/lab2/lab2-correction.html#how-to-distribute-elementary-statistical-tasks",
    "title": "Spark ML",
    "section": "1 How to distribute elementary statistical tasks?",
    "text": "1 How to distribute elementary statistical tasks?\nThe map and reduce principle\nWhen your data is distributed, i.e is spread out across multiple hard disks / memories on different logical or physical machines, it is clearly not possible to load everything in memory to perform some computation. (No computer from the cluster would have enough storage space / memory space to load the full data set, and the exchange of information between the nodes of the cluster would take considerable amounts of time.) What can you do then?\nA surprisingly satisfying situation is when your algorithm can be expressed in a map-and-reduce model1. A map step, in computer science, is the equivalent a function in mathematics: from a given entry, return an output. Examples include counting the number of occurrences of a word in a text, squaring some number, subtracting some number, etc. A reduce step takes two inputs and produces one input, and can be called recursively onto its own outputs, progressively yielding the final result through a pyramid of accumulators (see diagram here under). Popular reduce functions include (pairwise) concatenation of character strings, (pairwise) product, (pairwise) minimum and (pairwise) maximum. But pairwise addition is probably the most used reduce function, with the aim goal of performing a complete addition:\n\nWhy is the map-and-reduce scheme so interesting? Well, say you have \\(n\\) entries and \\(k\\) worker nodes at your disposal. The map operation can always be performed locally on each node, since the transformation does not depend on the rest of the data set. This is an embarrassingly parallel problem and we roughly divide the execution time by \\(k\\). Then, most of the reduce steps can also happen on the worker nodes, until the local data has been completely summarized. This also an \\(k\\)_fold acceleration! Then, there remains only \\(k\\) reduce steps, and since \\(k \\ll n\\), this is usually quite negligible, even though the (potentially high) networking costs happen at this step. There is still some cost of task coordination and data exchange, but this usually small compared to the costs of parallelisation.\n\nThe reduce step\nA reduce function is an associative function \\(f: E \\times E \\mapsto E\\), where associativity means \\(\\forall (a,b,c) \\in E^3, f(a,f(b,c))=f(f(a,b),c)\\). This is required because the distribution of data blocks across the nodes is random, and that we want to minimize data transmission between the nodes.\nMoreover, \\(f\\) may or may not be commutative, in the sense that \\(f(a,b)=f(b,a)\\). If it is the case, such as with addition and multiplication, then the computing may happen in no particular order. This means that the central node need not wait for some partial results to be returned by a belated node. On the contrary, if \\(f\\) is not commutative, (a) the worker nodes must apply the function in a defined order, (b) the central node needs to reduce the intermediate outputs in a defined order, (c) it may have to delay the final reduce steps because of a lingering node.\nThe reduce function must not be defined on \\(E=\\mathbb{R}\\). For instance, in the context where data is a collection of text documents, a word-count function may return accumulator objects looking like: ((word1,count1), (word2,count2)). Also, the accumulators — that is, the outputs of the each intermediate reduce step — are not necessarily exactly the cumulative version of the final statistic our algorithm outputs! Rather, accumulators are information-dense, fast-to-compute summary statistics from which the required final statistics can be obtained.\nImagine you want to count the frequency of the vocal E in English, given a collection of texts. It is faster to count the number of Es as well as the total number of characters than to accumulate directly the frequencies, as shown in this diagram:\n\nOnline algorithms\nAn online algorithm is an algorithm with an inner state that can be actualized at low cost for any new arrival of data. A good metaphor is track-keeping of the number of people on a bus: every time a person enters or leaves, you apply ±1 to the count, without the need to systematically recount everyone. Said otherwise, an online algorithm is any algorithm whose last result can be actualized from new data, at a smaller cost than an alternative algorithm that uses both old and new data from scratch.\nIt turns out that respecting the map-and-reduce model gives us online algorithms for free, where the inner state of the algorithm is the output from the last reduce call. Indeed, writing \\(s_\\text{old}\\) and \\(s_\\text{new}\\) the old and new states (the old and new summary statistics), and \\(x_new\\) the latest data point, we have:\n\\[s_\\text{new}=\\text{reduce}(s_\\text{old}, \\text{map}(x_\\text{new}))\\]\nThus, writing an algorithm following the map-and-reduce model gives you both a parallelized batch algorithm and a stream algorithm at once.\nNumber of passes\nSo far we have discussed algorithms that require only one map and one reduce functions. But for some statistics, it is not sufficient. For instance, if we want to count the number of texts where the letter E is more common than average, we first have to compute the average frequency in a first pass, then to count the texts where the frequency exceed this number with a second one. We can NOT do this in only one run, since the global average frequency is not known !\nEach run is called a pass and some algorithms require several passes.\nLimits\n\nNot all statistical algorithms can be expressed according to the map-and-reduce algorithm, and when they can, it may require a significant re-writing compared to the standard algorithms.\nThere may be a trade-off between the number of passes, the speed of each map / reduce steps and the volume of data transferred between each reduce step.\n\n\n1.1 ✍ Hands-on 1\n\nYou are given errors, a distributed vector of prediction errors errors = [1, 2, 5, 10, 3, 4, 6, 8, 9]\nWrite a map-and-reduce algorithm for computing the total sum of squares.\n\nYou may want to create a Python version of this algorithm, using the map(function, vector) and reduce(function, vector) functions or you may use lambda-functions\nYou have to import reduce from the functools module\n\n\n\nfrom functools import reduce\n\nerrors = [1, 2, 5, 10, 3, 4, 6, 8, 9]\n\nreduce(lambda s1, s2: s1+s2, map(lambda e: e**2, errors))\n\n\nWrite two different map-and-reduce algorithm for computing the mean sum of squares. (One may include a final \\(O(1)\\) step.)\n\n\nerrors = [1, 2, 5, 10, 3, 4, 6, 8, 9]\n\ndef weighted_mean(obj1, obj2):\n  w = obj1[\"w\"] + obj2[\"w\"]\n  return ({\n      \"m\" : (obj1[\"m\"] * obj1[\"w\"] + obj2[\"m\"] * obj2[\"w\"]) / w,\n      \"w\" : w\n  })\n\nsquared_errors = map(lambda e: {\"m\" : e**2, \"w\" : 1}, errors)\nreduce(weighted_mean, squared_errors )\n\n\ndef sum_and_n(obj1, obj2) :\n  return {\n    \"sum\" : obj1[\"sum\"] + obj2[\"sum\"],\n    \"n\"   : obj1[\"n\"] + obj2[\"n\"]\n  }\n\nsquared_errors = map(lambda e: {\"sum\" : e**2, \"n\" : 1}, errors)\ntotal_sum_and_n = reduce(sum_and_n, squared_errors)\ntotal_sum_and_n[\"sum\"] / total_sum_and_n[\"n\"]\n\n\nIs the median easy to write as a map-and-reduce algorithm? Why?\n\nExact median computation is not readily decomposable into a map-and-reduce algorithm, because the full set of numbers is needed in order. Parallel algorithm exist for the median, using the pivot technique. This is beyond the scope of this tutorial.\n\nGiven a (distributed) series of numbers, the variance can be straightforwardly expressed as a two-pass algorithm: (a) in a first pass, compute the mean, then (b) in a second pass, compute the mean of the errors to the mean. Can it be expressed as a one-pass only algorithm? Is it more expensive to compute variance and mean instead of the variance alone?\n\nYes, using the fact that \\(V(X)=E((X-E(X))^2)=E(X^2)-E(X)^2\\). \\(E(X)\\) and \\(E(X^2)\\) can be computed during the same reduce map:\nmap :               x -&gt; {s1=x, s2=x^2, n=1}  \nreduce : (obj1, obj2) -&gt; {  \n    s1 = obj1.s1 + obj2.s1,  \n    s2 = obj1.s2 + obj2.s2,  \n    n = obj1.n + obj2.n  \nfinal_step : s2/n-s1^2/n\nWhen you compute the variance, the means is computed “for free”"
  },
  {
    "objectID": "docs/lab/lab2/lab2-correction.html#create-a-spark-session",
    "href": "docs/lab/lab2/lab2-correction.html#create-a-spark-session",
    "title": "Spark ML",
    "section": "2 Create a Spark session",
    "text": "2 Create a Spark session\n\nDepending on the chosen platform, initialize the Spark session\n\n\n2.1 Only on SSPCloud\n\nimport os\nfrom pyspark.sql import SparkSession\n\nspark = (SparkSession\n         .builder\n         # default url of the internally accessed Kubernetes API\n         # (This Jupyter notebook service is itself a Kubernetes Pod)\n         .master(\"k8s://https://kubernetes.default.svc:443\")\n         # Executors spark docker image: for simplicity reasons, this jupyter notebook is reused\n         .config(\"spark.kubernetes.container.image\", os.environ['IMAGE_NAME'])\n         # Name of the Kubernetes namespace\n         .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\n         # Allocated memory to the JVM\n         # Stay careful, by default, the Kubernetes pods has a higher limit which depends on other parameters.\n         .config(\"spark.executor.memory\", \"4g\")\n         .config(\"spark.kubernetes.driver.pod.name\", os.environ['KUBERNETES_POD_NAME'])\n         # dynamic allocation configuration\n         .config(\"spark.dynamicAllocation.enabled\",\"true\")\n         .config(\"spark.dynamicAllocation.initialExecutors\",\"1\")\n         .config(\"spark.dynamicAllocation.minExecutors\",\"1\")\n         .config(\"spark.dynamicAllocation.maxExecutors\",\"60\")\n         # Ratio match the number of pods to create for a given number of parallel tasks\n         # (100 parallel, ratio of 1, one aims at 100 pods, with 0.5 it would be 50 pods)\n         .config(\"spark.dynamicAllocation.executorAllocationRatio\",\"1\")\n         .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",\"true\")\n         .getOrCreate()\n        )\n\n\n\n2.2 Only on AWS\n\n#Spark session\nspark\n\n# Configuraion\nspark._jsc.hadoopConfiguration().set(\"fs.s3.useRequesterPaysHeader\",\"true\")\n\n\n\n2.3 Check spark session\n\nspark"
  },
  {
    "objectID": "docs/lab/lab2/lab2-correction.html#application-on-airbnb-data",
    "href": "docs/lab/lab2/lab2-correction.html#application-on-airbnb-data",
    "title": "Spark ML",
    "section": "3 Application on Airbnb Data",
    "text": "3 Application on Airbnb Data\n\nfrom pyspark.sql.types import FloatType, IntegerType, DateType\nfrom pyspark.sql.functions import regexp_replace, col\n\n# Uncomment one of the following lines\n# listings_raw = spark.read.csv(\"s3a://remipepin/diffusion/ensai/airbnb\", header=True, multiLine=True, escape='\"')\n# listings_raw = spark.read.csv(\"s3://spark‑lab‑input‑data‑ensai20222023/airbnbb/\", header=True, multiLine=True, escape='\"')\n\nlistings = (listings_raw\n  .withColumn(\"beds\",     listings_raw[\"beds\"    ].cast(IntegerType()))\n  .withColumn(\"bedrooms\", listings_raw[\"bedrooms\"].cast(IntegerType()))\n  .withColumn(\"time\", listings_raw[\"last_scraped\"].cast(DateType()))\n  .withColumn(\"price\", regexp_replace('price', '[$\\\\,]', '').cast(FloatType()))\n  .select(\"id\", \"beds\", \"bedrooms\", \"price\", \"city\", \"time\")\n  .dropna() # remove lines with missing values\n)\n\nlistings_raw.cache()\nlistings.cache()\n\n\n3.1 ✍ Hands-on 2\n\nHow many lines do the raw and the formatted datasets have?\n\n\nprint(f\"Observations before cleaning: {listings_raw.count()} rows\")\nprint(f\"Observations  after cleaning:  {listings.count()} rows)\")\n\n\nHow many columns are there?\n\nCan you list all the available columns?\n\n\n\nprint(\"There are\", len(listings_raw.columns), \"columns:\")\nprint('\\n'.join( [\"- \"+column for column in listings_raw.columns] ))\n\nSpark SQL’s summary() method\nIn Spark SQL, elementary univariate summary statistics can also be obtained through the summary() method. The summary() method takes either the names of the statistics to compute, or nothing, in which case it computes every possible statistics:\nlistings.summary(\"count\", \"min\", \"max\").show() # computes the selection of statistics\nlistings.summary().show() # computes every possible statistics\nThis is a way to incite you to compute all the statistics you want at the same moment : it avoids an extra pass on the data set because all accumulators can be computed simultaneously. You can fin a list of all supported statistics here in PySpark documentation: count, mean, standard-deviation, minimum, maximum, approximate median, approximate first and last quartiles. Null (missing) values will be ignored in numerical columns before calculation.\nSpark ML\nSpark ML is a Spark module that allow us to execute parallelised versions of most popular machine-learning algorithms, such as linear or logistic regression. However, we can also use Spark ML to compute elementaty univariate summary statistics. However the philosophy is quite different, and is worth explaining2.\n\nSpark ML has been built on top of Spark years into the project, and the core of Spark is not well adapted to machine-learning ;\nSpark ML is intended for much more advanced treatments than unviariate statistics, and we will see linear regression as an exemple at the end of this tutorial\n\nStep 1: vectorisation. A little counter-intuitively, spark ML operates on a single column of your data frame, typically called features. (Features is the word used in the machine-learning community for “variables”, see “Vocabulary” section hereunder.) This features column has the Vector type: each element contains an array of floating-point numbers, representing a subst of the variables from your dataset. The key is that this features column is usually redundant with the rest of the data frame: it just ensures the proper conversion from any type we wish (string, integer…) to a standardized numeric format. Indeed, it is often derived from the other columns, as this image illustrates:\n\nUnfortunately for us, the construction the features column is not performed automatically under the hood by Spark, like when doing statistics in R. On the contrary, we have to construct the column explicitly. The VectorAssembler() constructor is here for that:\nfrom pyspark.ml.feature import VectorAssembler\n\nvectorizer = VectorAssembler(\n    inputCols     = [\"price\", \"beds\", \"bedrooms\"], # the columns we want to put in the features column\n    outputCol     = \"features\",                    # the name of the column (\"features\")\n    handleInvalid = 'skip'                         # skip rows with missing / invalid values\n)\n\nlistings_vec = vectorizer.transform(listings)\n\n# Reminders:\n# Spark data sets are immutable: a copy is returned, and the original is unchanged.\n# Spark operations are lazy: listings_vec just contains the recipe for building vector column\n# but no item of the column is computed unless explicitly asked to.\n\nlistings_vec.show(5) # The first 5 values of the features column are computed.\nStep 2: summarization. Now that we have a vector column, we can use a Summarizer object to declare all the statistics we want to compute, in a similar fashion than with the Spark SQL summary() method. The following statistics are known: mean*, sum*, variance*, standard-deviation*, count*, number of non-zero entries, maximum*, minimum*, L2-norm, L1-norm, as can be read in the documentation. (Stars (*) denote statistics that could also be computed with the summary() method. Approximate quartiles are not computed.) Summarizers are created with the Summarizer.metrics() constructor. Here again, you are incited to declare all the summaries at once, so that they can all be computed in one pass:\nfrom pyspark.ml.stat    import Summarizer\n\nsummarizer = Summarizer.metrics(\"count\", \"min\", \"max\")\n\nlistings_vec.select( summarizer.summary(listings_vec.features), ).show(truncate=False)\n# By default, the output of columns is capped to a maximum width.\n# truncate=False prevents this behaviour.\nThis produces the output:\n\n\n\n3.2 ✍ Hands-on 3\n\nIs listings.summary() slower to run than listings.summary(\"count\", \"min\", \"max\") ? Why?\n\nYou can measure time in Python with this simple template:\n\nfrom timeit import default_timer as t\nstart = t()\n# the thing you want to measure\nprint(\"Time:\", t()-start)\n\n\nfrom timeit import default_timer as t\nstart = t()\nlistings.summary(\"count\", \"min\", \"max\").show()\nprint (\"Execution time for sumary(\\\"count\\\"...):\", t()-start, \"s\")\n\n\nstart = t()\nlistings.summary().show()\nprint (\"Execution time for sumary():\", t()-start, \"s\")\n\n# The restriction to only \"count\", \"min\", \"max\" is only marginally faster\n# This is because in both cases only one pass is performed, and the other\n# statistics come almost for free.\n\n\nCompute the average number of beds per property in Barcelona in four different ways:\n\nWhich method is the fastest?\n\ndirectly with the Spark SQL mean function,\nusing summary(),\nusing a Sumarizer object\nlocally after you collected the bed columns. Despite the operation being very common, Spark does not provide a simple syntax to collect a column as a local array. A work-around is to use the Pandas package and the asPanda() method (documentation). First install Pandas with !pip install pandas. Then you can collect a local copy of a dataframe called df with: df_local = df.toPandas(). A Pandas data frame possesses a mean() method, that compute the mean of each column of the data frame: more details are in Pandas’ documentation.\n\n\n\n\n# Spark SQL\n\nfrom pyspark.sql.functions import mean\n\nstart = t()\n\nlistings.filter(col(\"city\")==\"barcelona\").select(mean('beds')).show()\n\nprint (\"Execution time:\", t()-start, \"s\")\n\n\n# summary()\n\nstart = t()\nlistings\\\n  .filter(col(\"city\")==\"barcelona\")\\\n  .select(\"beds\")\\\n  .summary(\"mean\")\\\n  .show()\nprint (\"Execution time:\", t()-start, \"s\")\n\n\n# Sumarizer\n\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.stat    import Summarizer\n\nstart = t()\nvectorizer = VectorAssembler(\n    inputCols     = [\"beds\"],\n    outputCol     = \"features\"\n)\nsummarizer = Summarizer.metrics(\"mean\")\nlistings_vec = vectorizer\\\n  .transform(listings.filter(col(\"city\")==\"barcelona\"))\nlistings_vec\\\n  .select(summarizer.summary(listings_vec.features))\\\n  .show(truncate=False)\nprint (\"Execution time:\", t()-start, \"s\")\n\n\n!pip install pandas\n\n\n# locally after you collected the bed columns\nstart = t()\nm = listings\\\n  .filter(col(\"city\")==\"barcelona\")\\\n  .select('beds')\\\n  .toPandas()\\\n  .mean()\n\nprint(m.beds)\nprint(\"Execution time:\", t()-start, \"s\")\n\nThe most simple model is often surprisingly difficult to beat!\n\nCompute the mean price on the data set as a predictor for an AirBnB listing’s price and the total sum of squares. (We will elaborate in the next section.)\n\n\nfrom pyspark.sql.functions import mean\nprice_mean = listings.select(mean(\"price\"))        # this is a (remote) data frame\nprice_mean = price_mean.first()                    # this is a (local) row\nprice_mean = price_mean[0]                         # this is a (local) number\nprice_mean"
  },
  {
    "objectID": "docs/lab/lab2/lab2-correction.html#regression-with-spark-ml",
    "href": "docs/lab/lab2/lab2-correction.html#regression-with-spark-ml",
    "title": "Spark ML",
    "section": "4 Regression with Spark ML",
    "text": "4 Regression with Spark ML\nA better way to predict prices is to build a regression mode, which in Spark falls under the broad category of machine-learning problems. Regressions thus belong the the ml module, often called Spark ML, like the summarizer that we saw just before.\nThere is an old module called mllib that is also called “Spark ML”. That can cause confusion.\nThe ml module is built in a distinctive fashion than the rest of Spark. Firstly we have seen with Summarizer that we can not readily use the columns and that instead columns have to be first converted to a Vector format with the VectorAssembler function.\nSecondly, we need to distinguish between two different types of object classes: transformers and estimators classes. Transformers are a class of objects representing any process that modifies the dataset, and returns the modified version. It has a transform() method. Estimators on the other hand are classes of objects representing any process that produces a transformer based on some computed parameters from the data set. It has a fit() method. It is easier with an example. In the following example, regressor is an estimator, and we compute the regression coefficients with the fit() method. This produces model, the regression model itself, which is of class transformer. Indeed, we can use its transform() method to add predictions to the initial dataset.\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nvectorizer = VectorAssembler( # copy-pasted from previous section...\n    inputCols     = [\"beds\", \"bedrooms\"], # ... but without price\n    outputCol     = \"features\",\n    handleInvalid = 'skip'\n)\n\nlistings_vec = vectorizer.transform(listings)\n\nregressor = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\nmodel     = regressor.fit(listings_vec)\n\nmodel.coefficients\nmodel.intercept\n\nlistings_pred = model.transform(listings_vec)\nlistings_pred.show() # model and predictions from the regression\nVocabulary\nThe machine-learning community leaves at the border between computer science and mathematics. They borrow vocabulary from both sides, and it can sometimes be confusing when reading software documentation. Spark’s lib module uses conventions from this community :\n\nlabel, rather than “independent variable”. This comes from the fact that historically, machine-learning has originated from problems such as image labeling (for instance digit recognition). Even for continuous variables, machine-learners may use “label”\nfeatures, rather than “dependent variables” ; the number of features is often dubbed \\(d\\) like dimension (instead of \\(p\\) in statistics)\nmachine-learners don’t use the word “observation” or “unit” and prefer row\n\nPipelines\nIf you come to repeat several times the same series of transformations, you may take advantage of the pipeline objects. A pipeline is just a collections of steps applied to the same dataset. This helpful when you:\n\nrepeat the same analysis for different regions / periods\nwant to control predictions on a new, unseen test set, and ant to apply exactly the same process\n\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml import Pipeline\n\nvectorizer = VectorAssembler( # same vectorizer as before\n    inputCols     = [\"beds\", \"bedrooms\"],\n    outputCol     = \"features\",\n    handleInvalid = 'skip'\n)\nregressor = LinearRegression(featuresCol=\"features\", labelCol=\"price\") # same regressor\npipeline  = Pipeline(stages = [vectorizer, regressor]) # ... but now we pack them into a pipeline\n\nlistings_beij = listings.filter(listings.city==\"Beijing\")\nlistings_barc = listings.filter(listings.city==\"Barcelona\")\n\nmodel_beij = pipeline.fit(listings_beij) # vectorizer AND regressor are applied\nmodel_barc = pipeline.fit(listings_barc)\n\nprint(model_beij.stages[1].coefficients) # model.stages[0] is the first step, model.stages[1] the second...\nprint(model_beij.stages[1].intercept)\n\nprint(model_barc.stages[1].coefficients)\nprint(model_barc.stages[1].intercept)\n\n4.1 ✍ Hands-on 4\n\nInterpret the results of the general regression.\n\n\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nvectorizer = VectorAssembler( # copy-pasted from previous section...\n    inputCols     = [\"beds\", \"bedrooms\"], # ... but without price\n    outputCol     = \"features\",\n    handleInvalid = 'skip'\n)\n\nlistings_vec = vectorizer.transform(listings)\n\nregressor = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\nmodel     = regressor.fit(listings_vec)\n\n\nprint(f\"Coefficients : {model.coefficients}\")\nprint(f\"intercept : {model.intercept}\")\n\n\nCollect the model’s \\(R^2\\). How good is our model?\n\nModels have a summary property, that you can explore with dir(model.summary).\n\n\n\ndir(model.summary)\n\n\nmodel.summary.r2\n\n\nRepeat the estimation separately for barcelona, brussels and rome.\n\nAre the coefficients stable? You will build a pipeline object.\n\n\n\nlistings\\\n  .select(\"city\")\\\n  .distinct()\\\n  .show()\n\n\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql.functions import year\n\nvectorizer = VectorAssembler(\n    inputCols     = [\"beds\", \"bedrooms\"],\n    outputCol     = \"features\",\n    handleInvalid = 'skip'\n)\nregressor = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\npipeline  = Pipeline(stages = [vectorizer, regressor])\n\ncities = [\"rome\", \"barcelona\", \"brussels\"]\nmodels = {}\n\nfor city in cities :\n    models[city]= pipeline.fit(listings.filter(listings.city==city))\n\nfor city, model in models.items():\n    print(f\"Coeeficients for {city} are : {model.stages[-1].coefficients}\")\n\n\nAre the fit() and transform() methods called eagerly or lazily?\n\nCheck the execution plan with the explain() method for lazy evaluations.\n\n\n\n# fit() is eager ; transform() is lazy"
  },
  {
    "objectID": "docs/lab/lab2/lab2-correction.html#diving-deeper",
    "href": "docs/lab/lab2/lab2-correction.html#diving-deeper",
    "title": "Spark ML",
    "section": "5 Diving deeper",
    "text": "5 Diving deeper\nYou are in autonomy for this section. You will find helpful:\n\nThe general Spark documentation for the ml module\nThe PySpark documentation\n\n\n5.1 ✍ Hands-on 5\n\nAdd a categorical variable to the regression.\n\n\n# Code here\n\n\nCompute the p-values of your model as well as confidence intervals for the predictions.\n\n\n# Code here\n\n\nTime the regression in different settings and report the results on this shared spreadsheet. How does it scale with the number of listings (\\(n\\)) ? the number of regressors (\\(p\\)) ? the number of nodes in your cluster (\\(k\\)) ? You will only try a couple of configurations that have not been tested by others. Remember that you can order and revoke nodes from your cluster at any time from the AWS’s cluster view, in the hardware tab, on on the CORE line, “resize”.\n\n\n# Code here\n\n\nDown-sample your data set to \\(n=100000\\), while still keeping a few variables. Save it on S3, then download it on your computer. Run the regression locally on your computer in R. In your opinion, is the extra precision (in term of \\(R^2\\)) is worth the extra computation time?\n\n\n# Code here"
  },
  {
    "objectID": "docs/lab/lab2/lab2-correction.html#end-of-the-lab",
    "href": "docs/lab/lab2/lab2-correction.html#end-of-the-lab",
    "title": "Spark ML",
    "section": "End of the Lab",
    "text": "End of the Lab\n\nExport your notebook\n\nRight clic and Download (.ipynb)\nFile &gt; Save and Export Notebook &gt; HTML\n\nDelete the Jupyter-pyspark service\n\nSSPCloud &gt; My services &gt; Delete"
  },
  {
    "objectID": "docs/lab/lab2/lab2-correction.html#footnotes",
    "href": "docs/lab/lab2/lab2-correction.html#footnotes",
    "title": "Spark ML",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHadoop’s MapReduce is the name of what was to become today Apache Spark. The persons behind this framework were among the first to advocate for the map-and-reduce mode in order to achieve efficient parallelisation. Unfortunately, the similarity of the names causes a lot of confusion between the map-and-reduce theoretical model and the concrete Hadoop implementation. I will use “map-and-reduce” to help distinguish the algorithmic concept from the MapReduce program, but this is not standard in the literature.↩︎\nThe syntax of Spark ML may feel artificially convoluted ; this not only an impression, it is convoluted. However, there are grounds for this situation :↩︎"
  },
  {
    "objectID": "docs/lab/lab3/lab3-SSPCloud.html",
    "href": "docs/lab/lab3/lab3-SSPCloud.html",
    "title": "Stream processing with Spark",
    "section": "",
    "text": "Download this Jupyter Notebook\nFollow these instructions"
  },
  {
    "objectID": "docs/lab/lab3/lab3-SSPCloud.html#before-you-start",
    "href": "docs/lab/lab3/lab3-SSPCloud.html#before-you-start",
    "title": "Stream processing with Spark",
    "section": "",
    "text": "Download this Jupyter Notebook\nFollow these instructions"
  },
  {
    "objectID": "docs/lab/lab3/lab3-SSPCloud.html#outline",
    "href": "docs/lab/lab3/lab3-SSPCloud.html#outline",
    "title": "Stream processing with Spark",
    "section": "Outline",
    "text": "Outline\nThis tutorial will teach you the basic of stream processing with Spark. As soon as an application compute something with business value (for instance customer activity), and new inputs arrive continuously, companies will want to compute this result continuously too. Spark makes possible to process stream with the Structured Streaming API. This lab will teach you the basics of this Spark’s API. Because the Structured Streaming API is based on the DataFrame API most syntaxes of tutorial 1 are still relevant."
  },
  {
    "objectID": "docs/lab/lab3/lab3-SSPCloud.html#notebook-configuration",
    "href": "docs/lab/lab3/lab3-SSPCloud.html#notebook-configuration",
    "title": "Stream processing with Spark",
    "section": "1 Notebook configuration",
    "text": "1 Notebook configuration\nJust run the following cell\n\nimport os\nfrom pyspark.sql import SparkSession\n\nspark = (SparkSession \n         .builder\n         # default url of the internally accessed Kubernetes API\n         # (This Jupyter notebook service is itself a Kubernetes Pod)\n         .master(\"k8s://https://kubernetes.default.svc:443\")\n         # Executors spark docker image: for simplicity reasons, this jupyter notebook is reused \n         .config(\"spark.kubernetes.container.image\", os.environ['IMAGE_NAME'])\n         # Name of the Kubernetes namespace\n         .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\n         # Allocated memory to the JVM\n         # Stay careful, by default, the Kubernetes pods has a higher limit which depends on other parameters.\n         .config(\"spark.executor.memory\", \"4g\")\n         .config(\"spark.kubernetes.driver.pod.name\", os.environ['KUBERNETES_POD_NAME'])\n         # dynamic allocation configuration\n         .config(\"spark.dynamicAllocation.enabled\",\"true\")\n         .config(\"spark.dynamicAllocation.initialExecutors\",\"1\")\n         .config(\"spark.dynamicAllocation.minExecutors\",\"1\")\n         .config(\"spark.dynamicAllocation.maxExecutors\",\"5\")\n         # Ratio match the number of pods to create for a given number of parallel tasks \n         # (100 parallel, ratio of 1, one aims at 100 pods, with 0.5 it would be 50 pods)\n         .config(\"spark.dynamicAllocation.executorAllocationRatio\",\"1\")\n         .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",\"true\")\n         .getOrCreate()\n        )\n\n# Import all the needed library\nfrom time import sleep\nfrom pyspark.sql.functions import from_json, window, col, expr, size, explode, avg, min, max\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType, ArrayType, TimestampType, BooleanType, LongType, DoubleType\n\nExplanation:\n\nspark._jsc.hadoopConfiguration().set(\"fs.s3.useRequesterPaysHeader\",\"true\") : likes in lab2, you will be charged for the data transfer. without this configuration you can’t access the data.\nspark.conf.set(\"spark.sql.shuffle.partitions\", 5) : set the number of partitions for the shuffle phase. A partition is in Spark the name of a bloc of data. By default Spark use 200 partitions to shuffle data. But in this lab, our mini-batch will be small, and to many partitions will lead to performance issues.\nspark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n\n🤔 The shuffle dispatches data according to their key between a map and a reduce phase. For instance, if you are counting how many records have each g group, the map phase involve counting each group member in each Spark partition : {g1:5, g2:10, g4:1, g5:3} for one partition, {g1:1, g2:2, g3:23, g5:12} for another. The shuffle phase dispatch those first results and group them by key in the same partition, one partition gets {g1:5, g1:1, g2:10, g2:2}, the other gets : {g4:1, g5:3, g3:23, g5:12} Then each reduce can be done efficiently.\n\nImport all needed library\nfrom time import sleep\nfrom pyspark.sql.functions import from_json, window, col, expr\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType, ArrayType, TimestampType, BooleanType, LongType, DoubleType"
  },
  {
    "objectID": "docs/lab/lab3/lab3-SSPCloud.html#stream-processing",
    "href": "docs/lab/lab3/lab3-SSPCloud.html#stream-processing",
    "title": "Stream processing with Spark",
    "section": "2 Stream processing",
    "text": "2 Stream processing\nStream processing is the act to process data in real-time. When a new record is available, it is processed. There is no real beginning nor end to the process, and there is no “result”. The result is updated in real time, hence multiple versions of the results exist. For instance, you want to count how many tweet about cat are posted in twitter every hour. Until the end of an hour, you do not have you final result. And even at this moment, your result can change. Maybe some technical problems created some latency and you will get some tweets later. And you will need to update your previous count.\nSome commons use cases of stream processing are :\n\nNotifications and alerting : real-time bank fraud detection ; electric grid monitoring with smart meters ; medical monitoring with smart meters, etc.\nReal time reporting: traffic in a website updated every minute; impact of a publicity campaign ; stock option portfolio, etc.\nIncremental ELT (extract transform load): new unstructured data are always available and they need to be processed (cleaned, filtered, put in a structured format) before their integration in the company IT system.\nOnline machine learning : new data are always available and used by a ML algorithm to improve its performance dynamically.\n\nUnfortunately, stream processing has some issues. First because there is no end to the process, you cannot keep all the data in memory. Second, process a chain of event can be complex. How do you raise an alert when you receive the value 5, 6 and 3 consecutively ? Don’t forget you are in a distributed environment, and there is latency. Hence, the received order can be different from the emitted order."
  },
  {
    "objectID": "docs/lab/lab3/lab3-SSPCloud.html#spark-and-stream-processing",
    "href": "docs/lab/lab3/lab3-SSPCloud.html#spark-and-stream-processing",
    "title": "Stream processing with Spark",
    "section": "3 Spark and stream processing",
    "text": "3 Spark and stream processing\nStream processing was gradually incorporated in Spark. In 2012 Spark Streaming and it’s DStreams API was added to Spark (it was before an external project). This made it possible use high-level operator like map and reduce to process stream of data. Because of its implementation, this API has some limitations, and its syntax was different from the DataFrame one. Thus, in 2016 a new API was added, the Structured Streaming API. This API is directly build built on DataFrame, unlike DStreams. This has an advantage, you can process your stream like static data. Of course there are some limitations, but the core syntaxes is the same. You will chain transformations, because each transformation takes a DataFrame as input and produces a DataFrame as output. The big change is there is no action at the end, but an output sink.\n\n\n\ndata stream\n\n\nFigure 1 : data stream representation (source structured streaming programming guide)\nSpark offer two ways to process stream, one record at a time, or processing micro batching (processing a small amount of line at once).\n\none record at a time every time a new record is available it’s processed. This has a big advantage, it achieves very low latency . But there is a drawback, the system can not handle too much data at the same time (low throughput). It’s the default mode. Because in this lab, you will process files with record, even if you will process one file at a time, you will process mini batch of records\nas for micro batching it process new records every t seconds. Hence records are not process really in “real-time”, but periodically, the latency will be higher, and so the throughput. Unless you really need low latency, make it you first choice option.\n\n\n🧐 To get the best ratio latency/throughput, a good practice is to decrease the micro-batch size until the mini-batch throughput is the same as the input throughput. Then increase the size to have some margin\n\n\nFigure 2 : which Spark solution suit best giving latency requirement (source : Learning Spark, O’Reilly)\n\nTo understand why processing one record at a time has lower latency and throughput than batch processing, imagine a restaurant. Every time a client order something the chef cooks its order independently of the other current orders. So if two clients order pizza, the chief makes two small doughs, and cook them individually. If clients there is only a few clients, the chief can finish each order before a new client comes. The latency is the lowest possible when the chief is idle when a client come. Know imagine a restaurant were the chief process the orders by batch. He waits some minutes to gather all the orders than he mutualizes the cooking. If there are 5 pizza orders, he only does one big dough, divides it in five, add the toppings then cook all five at once. The latency is higher because the chief waits before cooking, but so the throughput because he can cook multiple things at once."
  },
  {
    "objectID": "docs/lab/lab3/lab3-SSPCloud.html#the-basics-of-sparks-structured-streaming",
    "href": "docs/lab/lab3/lab3-SSPCloud.html#the-basics-of-sparks-structured-streaming",
    "title": "Stream processing with Spark",
    "section": "4 The basics of Spark’s Structured Streaming",
    "text": "4 The basics of Spark’s Structured Streaming\n\n4.1 The different sources for stream processing in Spark\nIn lab 2 you discovered Spark DataFrame, in this lab you will learn about Structured Streaming. It’s a stream processing framework built on the Spark SQL engine, and it uses the existing structured APIs in Spark. So one you define a way to read a stream, you will get a DataFrame. Like in lab2 ! So except state otherwise, all transformations presented in lab2 are still relevant in this lab.\nSpark Streaming supports several input source for reading in a streaming fashion :\n\nApache Kafka an open-source distributed event streaming platform (not show in this lab)\nFiles on distributed file system like HDFS or S3 (Spark will continuously read new files in a directory)\nA network socket : an end-point in a communication across a network (sort of very simple webservice). It’s not recommend for production application, because a socket connection doesn’t provide any mechanism to check the consistency of data.\n\nDefining an input source is like loading a DataFrame but, you have to replace spark.read by spark.readStream. For instance, if I want to open a stream to a folder located in S3 you have to read every new files put in it, just write\nmy_first_stream = spark\\\n.readStream\\\n.schema(schema_tweet)\\\n.json(\"s3://my-awesome-bucket/my-awesome-folder\")\nThe major difference with lab2, it is Spark cannot infer the schema of the stream. You have to pass it to Spark. There is two ways :\n\nA reliable way : you define the schema by yourself and gave it to Spark\nA quick way : you load one file of the folder in a DataFrame, extract the schema and use it. It works, but the schema can be incomplete. It’s a better solution to create the schema by hand and use it.\n\nFor Apache Kafka, or socket , it’s a slightly more complex, (not used today, it’s jute for you personal knowledge) :\nmy_first_stream = spark\\\n.readStream\\\n.format(\"kafka\")\n.option(\"kafka.bootstrat.servers\", \"host1:port1, host2:port2 etc\")\n.option(\"subscribePattern\", \"topic name\")\n.load()\n\nWhy is a folder a relevant source in stream processing ?\nPreviously, in lab 1, you loaded all the files in a folder stored in MinIO with Spark. Powered by Kubernetes, MinIO delivers scalable, secure, S3 compatible object storage to every public cloud. And it worked pretty well. But this folder was static, in other words, Its content didn’t change. But in some cases, new data are constantly written into a folder. For instance, in this lab you will process a stream of tweets. A python script is running in a VS Code service reading tweets from the Twitter’s web service and writing them in a S3 buckets. Every 2 seconds or so, a new file is added to the bucket with 1000 tweets. If you use DataFrame like in lab 1, your process cannot proceed those new files. You should relaunch your process every time. But with Structured Streaming Spark will dynamically load new files.\n\nFigure 3 : Complete lab architecture to stream process tweets\n\nThe remaining question is, why don’t we connect Spark to the twitter webservice directly ? And the answer is : we can’t. Spark cannot be connected to a webservice directly. You need a middle-man between Spark and a webservice. There are multiple solutions, but an easy and reliable one is to write tweet to MinIO, the open implementation of s3 (because we use Onyxia, if you use Microsoft Azure, Google Cloud Platform or OVH cloud replace S3 by their storage service).\n\n\n\n\n4.2 ✍Hand-on 1 : Open a stream\nLike in lab 1, you will use tweets in this lab. The tweets are stored in jsonl file (json line every line of the file is a complete json). Here is an example. The schema changed a little, because this time tweets aren’t pre-processed.\n{\n    \"data\": {\n        \"public_metrics\": {\n            \"retweet_count\": 0,\n            \"reply_count\": 0,\n            \"like_count\": 0,\n            \"quote_count\": 0\n        },\n        \"text\": \"Day 93. Tweeting every day until Colby cheez its come back #bringcolbyback @cheezit\",\n        \"possibly_sensitive\": false,\n        \"created_at\": \"2021-05-03T07:55:46.000Z\",\n        \"id\": \"1389126523853148162\",\n        \"entities\": {\n            \"annotations\": [\n                {\n                    \"start\": 33,\n                    \"end\": 43,\n                    \"probability\": 0.5895,\n                    \"type\": \"Person\",\n                    \"normalized_text\": \"Colby cheez\"\n                }\n            ],\n            \"mentions\": [\n                {\n                    \"start\": 75,\n                    \"end\": 83,\n                    \"username\": \"cheezit\"\n                }\n            ],\n            \"hashtags\": [\n                {\n                    \"start\": 59,\n                    \"end\": 74,\n                    \"tag\": \"bringcolbyback\"\n                }\n            ]\n        },\n        \"lang\": \"en\",\n        \"source\": \"Twitter for iPhone\",\n        \"author_id\": \"606856313\"\n    },\n    \"includes\": {\n        \"users\": [\n            {\n                \"created_at\": \"2012-06-13T03:36:00.000Z\",\n                \"username\": \"DivinedHavoc\",\n                \"verified\": false,\n                \"name\": \"Justin\",\n                \"id\": \"606856313\"\n            }\n        ]\n    }\n}\n\nDefine a variable with this schema (you will find a file schema pyspark tweet on moodle with the schema to copy /aste)\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType, ArrayType, TimestampType, BooleanType, LongType, DoubleType\n\nStructType([\n  StructField(\"data\", StructType([\n      StructField(\"author_id\",StringType(),True),\n      StructField(\"text\",StringType(),True),\n      StructField(\"source\",StringType(),True),\n      StructField(\"lang\",StringType(),True),\n      StructField(\"created_at\",TimestampType(),True),\n      StructField(\"entities\",StructType([\n          StructField(\"annotations\", ArrayType(StructType([\n              StructField(\"end\", LongType(), True),\n              StructField(\"normalized_text\", StringType(), True),\n              StructField(\"probability\", DoubleType(), True),\n              StructField(\"start\", LongType(), True),\n              StructField(\"type\", StringType(), True)\n          ]),True),True),\n          StructField(\"cashtags\", ArrayType(StructType([\n              StructField(\"end\", LongType(), True),\n              StructField(\"start\", LongType(), True),\n              StructField(\"tag\", StringType(), True)\n          ]),True),True),\n           StructField(\"hashtags\", ArrayType(StructType([\n              StructField(\"end\", LongType(), True),\n              StructField(\"start\", LongType(), True),\n              StructField(\"tag\", StringType(), True)\n          ]),True),True),\n          StructField(\"mentions\", ArrayType(StructType([\n              StructField(\"end\", LongType(), True),\n              StructField(\"start\", LongType(), True),\n              StructField(\"username\", StringType(), True)\n          ]),True),True),\n          StructField(\"urls\", ArrayType(StructType([\n              StructField(\"description\", StringType(), True),\n              StructField(\"display_url\", StringType(), True),\n              StructField(\"end\", LongType(), True),\n              StructField(\"expanded_url\", StringType(), True),\n              StructField(\"images\", ArrayType(StructType([\n                      StructField(\"height\", LongType(), True),\n                      StructField(\"url\", StringType(), True),\n                      StructField(\"width\", LongType(), True)\n                  ]),True),True),\n              StructField(\"start\", LongType(), True),\n              StructField(\"status\", LongType(), True),\n              StructField(\"title\", StringType(), True),\n              StructField(\"unwound_url\", StringType(), True),\n              StructField(\"url\", StringType(), True),\n          ]),True),True),\n      ]),True),\n      StructField(\"public_metrics\", StructType([\n          StructField(\"like_count\", LongType(), True),\n          StructField(\"reply_count\", LongType(), True),\n          StructField(\"retweet_count\", LongType(), True),\n          StructField(\"quote_count\", LongType(), True),\n      ]),True)\n    ]),True),\n  StructField(\"includes\", StructType([\n      StructField(\"users\", ArrayType(StructType([\n          StructField(\"created_at\", TimestampType(), True),\n          StructField(\"id\", StringType(), True),\n          StructField(\"name\", StringType(), True),\n          StructField(\"username\", StringType(), True),\n          StructField(\"verified\", BooleanType(), True)\n      ]),True),True)\n  ]),True)\n  ])\n\n\n# Code Here\n\n\nCreate a stream to this s3 bucket : s3a:/s3a://remipepin/diffusion/ensai/stream_twee/remipepin/diffusion/ensai/stream_tweet.\n\nName it tweet_stream.\nUse the option option(\"maxFilePerTrigger\", \"1\") to process each new files one by one.\n\n\n\n🤔 Nothing happen ? It’s normal ! Do not forget, Spark use lazy evaluation. It doesn’t use data if you don’t define an action. For now Spark only know how to get the stream, that’s all.\n\n\ntweet_stream = spark\\\n.readStream\\\n.schema(schema_tweet)\\\n.option(\"maxFilePerTrigger\", \"1\")\\\n.json(\"s3a://remipepin/diffusion/ensai/stream_tweet\")\n\n\nIn a cell just execute tweet_stream.\n\nIt should print the type of tweet_stream and the associated schema. You can see you created a DataFrame like in lab2 !\n\n\n\n# Code Here\n\n\nPrint the size of your DataFrame by using this piece of code :\nstream_size_query= tweet_stream\\\n.writeStream\\\n.queryName(\"stream_size\")\\\n.format(\"memory\")\\\n.start()\n\nfor _ in range(10): # we use an _ because the variable isn't used. You can use i if you prefere\n    sleep(3)\n    spark.sql(\"\"\"\n      SELECT count(1) FROM stream_size\n    \"\"\").show()\nstream_size_query.stop() #needed to close the query\n\n\n# Code Here\n\n\n\n4.3 How to output a stream ?\nRemember, Spark has two types of methods to process DataFrame:\n\nTransformations which take a DataFrame has input and produce an other Dataframe\nAnd actions, which effectively run computation and produce something, like a file, or a output in you notebook/console.\n\nStream processing looks the same as DataFrame processing. Hence, you still have transformations, the exact same one that can be apply on classic DataFrame (with some restriction, for example you can not sample a stream with the sample() transformation). The action part is a little different. Because a stream runs continuously, you cannot just print the data or run a count at the end of the process. In fact actions will nor work on stream. To tackle this issue, Spark proposes different outputs sinks. An output sink is a possible output for your stream. The different output sink are (this part came from the official Spark documentation) :\n\nFile sink - Stores the output to a file. The file can be stored locally (on the cluster), remotely (on S3). The file format can be json, csv etc\nwriteStream\\\n.format('json')\\\n.option(\"checkpointLocation\", \"output_folder/history\") \\ \n.option(\"path\", \"output_folder\")\\\n.start()\nKafka sink - Stores the output to one or more topics in Kafka.\nForeach sink - Runs arbitrary computation on the records in the output. It does not produce an DataFrame. Each processed lines lost\nwriteStream\n    .foreach(...)\n    .start()\nConsole sink (for debugging) - Prints the output to the console standard output (stdout) every time there is a trigger. This should be used for debugging purposes on low data volumes as the entire output is collected and stored in the driver’s memory after every trigger. Sadly console sink does not work with jupyter notebook.\nwriteStream\n    .format(\"console\")\n    .start()\nMemory sink (for debugging) - The output is stored in memory as an in-memory table. This should be used for debugging purposes on low data volumes as the entire output is collected and stored in the driver’s memory. Hence, use it with caution. Because we are in a simple lab, you will use this solution. But keep in mind it’s a very bad idea because data must fit in the the ram of the driver node. And in a big data context it’s impossible. Because it’s not a big data problem if one computer can tackle it.\nwriteStream\n    .format(\"memory\")\n    .queryName(\"tableName\") # to resquest the table with spark.sql()\n    .start()\n\nWe just talked where we can output a stream, but there is another question, how ?\nTo understand why it’s a issue, let’s talk about two things that spark can do with streams : filter data and group by + aggregation\n\nFilter : your process is really simple. Every time you get a new data you just compute a score and drop records with a score less than a threshold. Then you write into a file every kept record. In a nutshell, you just append new data to a file. Spark does not have to read an already written row, it just add new data.\nGroup by + aggregation : in this case you want to group by your data by key than compute a simple count. Then you want to write the result in a file. But now there is an issue, Spark needs to update some existing rows in your file every time it writes somethings. But is your file is stored in HDFS of S3, it’s impossible to update in a none append way a file. In a nutshell, it’s impossible to output in a file your operation.\n\nTo deal with this issue, Spark proposes 3 mode. And you cannot use every mode with every output sink, with every transformation. The 3 modes are (more info here) :\n\nAppend mode (default) - This is the default mode, where only the new rows added to the Result Table since the last trigger will be outputted to the sink. This is supported for only those queries where rows added to the Result Table is never going to change. Hence, this mode guarantees that each row will be output only once (assuming fault-tolerant sink). For example, queries with only select, where, map, flatMap, filter, join, etc. will support Append mode.\nComplete mode - The whole Result Table will be outputted to the sink after every trigger. This is supported for aggregation queries.\nUpdate mode - (Available since Spark 2.1.1) Only the rows in the Result Table that were updated since the last trigger will be outputted to the sink. More information to be added in future releases.\n\n\n\n\nSink\nSupported Output Modes\n\n\n\n\nFile Sink\nAppend\n\n\nKafka Sink\nAppend, Update, Complete\n\n\nForeach Sink\nAppend, Update, Complete\n\n\nForeachBatch Sink\nAppend, Update, Complete\n\n\nConsole Sink\nAppend, Update, Complete\n\n\nMemory Sink\nAppend, Complete\n\n\n\n\n\n4.4 How to output a stream : summary\nTo sum up to output a stream you need\n\nDataFrame (because once load a stream is a DataFrame)\nA format for your output, like console to print in console, memory to keep the Result Table in memory, json to write it to a file etc\nA mode to specify how the Result Table will be updated.\n\nFor instance for the memory sink\nmemory_sink = df\\\n.writeStream\\\n.queryName(\"my_awesome_name\")\\\n.format('memory')\\\n.outputMode(\"complete\" or \"append\")\\\n.start() #needed to start the stream\n\nFigure 4 : The different phases of stream processing in Spark\n\n\n4.5 ✍Hand-on 2 : output a stream\n\nLang count\n\nCompute a DataFrame that group and count data by the lang column.\n\nName your DataFrame lang_count\n\n\n\n# Code Here\n\n\nUse this DataFrame to create a output stream with the following configuration :\n\nNames the variable lang_query\nMemory sink\nComplete mode (because we are doing an agregation)\nName you query lang_count\n\n\n\n# Code Here\n\n\nThen past this code\nfor _ in range(10): # we use an _ because the variable isn't use. You can use i if you prefere\n    sleep(3)\n    spark.sql(\"\"\"\n    SELECT * FROM lang_count\"\"\").show()\nlang_query.stop() #needed to close the stream\n\n\n# Code Here\n\n\nAfter 30 seconds, 10 tables will appeared in your notebook. Each table represents the contain of lang_count at a certain time. The .stop() method close the stream.\nIn the rest of this tutorial, to will need two steps to print data :\n\nDefine a stream query with a memory sink\nRequest this stream with the spark.sql() function\n\nInstead of a for loop, you can just write you spark.sql() statement in a cell and rerun it. In this case you will need a third cell with a stop() method to close your stream.\nFor instance:\nCell 1\nmy_query = my_df\\\n    .writeStream\\\n    .format(\"memory\")\\\n    .queryName(\"query_table\")\\\n    .start()\nCell 2\nspark.sql(\"SELECT * FROM query_table\").show()\nCell 3\nmy_query.stop()\n\n\n\nCount tweets with and without hashtag\n\nAdd a column has_hashtag to your DataFrame.\n\nThis column equals True if data.entities.hashtags is not null. Else it’s false.\nUse the withColumn transformation to add a column.\nYou can count the size of data.entities.hashtags to check if it’s empty or not.\n\n\n\n# Code Here\n\n\nGroup and count by the has_hashtag column\n\n\n# Code Here\n\n\nPrint some results\n\n\n# Code Here\n\n\n\n\n4.6 Debugging tip\nIf at any moment of this lab you encounter an error like this one :\n'Cannot start query with name has_hashtag as a query with that name is already active'\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py\", line 1109, in start\n    return self._sq(self._jwrite.start())\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 79, in deco\n    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)\npyspark.sql.utils.IllegalArgumentException: 'Cannot start query with name has_hashtag as a query with that name is already active'\nRun in a cell the following code :\nfor stream in spark.streams.active:\n    stream.stop()\nspark.streams.active returns an array with all the active stream, and the code loops over all the active stream and closes them."
  },
  {
    "objectID": "docs/lab/lab3/lab3-SSPCloud.html#stream-processing-basics",
    "href": "docs/lab/lab3/lab3-SSPCloud.html#stream-processing-basics",
    "title": "Stream processing with Spark",
    "section": "5 Stream processing basics",
    "text": "5 Stream processing basics\n\n5.1 ✍Hand-on 3 : transformations on stream\n\nFilter all records with missing / null value then count how many records you keep\n\nFor this filter, you will use the na.drop(\"any\") transformation. The na.drop(\"any\") drop every line with a null value in at least one column. It’s simpler than using a filter() transformation because you don’t have to specify all the column. For more precise filter you can use na.drop(\"any\" or \"all\", subset=list of col) (all will drop rows with only null value in all columns or in the specified list).\nUse the SQL COUNT(1) function in the sql request to get the count\nBecause you don’t perform aggregation the outputMode() must be append\n\n\nYou will notice no record are dropped.\n\n# Code Here\n\n\nDrop all records with unverified (includes.users.verified == True)user then group the remaining records by hashtag.\n\nincludes.users is an array with only one element. You will need to extract it.\ndata.entities.hashtags is an array too ! To group by tag (the hashtag content) you will need to explode it too.\n\n\n\n# Code Here\n\n\nFind ukraine related tweet (or any other topic like cat, dog, spring, batman, dogecoin etc) :\n\nDefine a new column, name ukraine_related. This column is equal to True if data.text contains “ukraine”, else it’s equal toFalse.\nUse the withColumn() transformation, and the expr() function to define the column. expr() takes as input an SQL expression. You do not need a full SQL statement (SELECT ...   FROM ... WHERE ...) but just an SQL expression that return True or False if data.text contains “ukraine”. To help you :\n\nLOWER() put in lower case a string\ninput_string LIKE wanted_string return True if input_string is equal to wanted_string\nYou can use % as wildcards For more help\n\nOnly keep data.text, data.lang, data.public_metrics and data.created_at\n\n\n\n# Code Here\n\n\n\n5.2 ✍Hand-on 4 : Aggregation and grouping on stream\n\nCount the number of different hashtag.\n\n\n# Code Here\n\n\nGroup by hashtag and compute the average, min and max of like_count\n\nUse the groupBy() and agg() transformations\n\n\n\n# Code Here\n\n\nCompute the average of like_count, retweet_count and quote_count :\n\nacross all hashtag and lang\nfor each lang across all hashtag\nfor each hashtag across all lang\nfor each hashtag and each lang\n\nTo do so, replace the groupBy() transformation by the cube() one. cube() group compute all possible cross between dimensions passed as parameter. You will get something like this\n\n\n\n\n\n\n\n\n\n\nhashtag\nlang\navg(like_count)\navg(retweet_count)\navg(quote_count)\n\n\n\n\ncat\nnull\n1\n2\n3\n\n\ndog\nnull\n4\n5\n6\n\n\n…\n…\n…\n…\n…\n\n\nbird\nfr\n7\n8\n9\n\n\nnull\nen\n10\n11\n12\n\n\nnull\nnull\n13\n14\n15\n\n\n\nA null value mean this dimension wasn’t use for this row. For instance, the first row gives the averages when hashtag==cat independently of the lang. The before last row gives averages when lang==en independently of the hashtag. And the last row gives the averages for the full DataFrame.\n\n\n# Code Here"
  },
  {
    "objectID": "docs/lab/lab3/lab3-SSPCloud.html#event-time-processing",
    "href": "docs/lab/lab3/lab3-SSPCloud.html#event-time-processing",
    "title": "Stream processing with Spark",
    "section": "6 Event-time processing",
    "text": "6 Event-time processing\nEvent-time processing consists in processing information with respect to the time that it was created, not received. It’s a hot topic because sometime you will receive data in an order different from the creation order. For example, you are monitoring servers distributed across the globe. Your main datacentre is located in Paris. Something append in New York, and a few milliseconds after something append in Toulouse. Due to location, the event in Toulouse is likely to show up in your datacentre before the New York one. If you analyse data bases on the received time the order will be different than the event time. Computers and network are unreliable. Hence, when temporality is important, you must consider the creation time of the event and not it’s received time.\nHopefully, Spark will handle all this complexity for you ! If you have a timestamp column with the event creation spark can update data accordingly to the event time.\nFor instance is you process some data with a time window, Spark will update the result based on the event-time not the received time. So previous windows can be updated in the future.\n\nFigure 5 : Time-event processing, event grouped by time windows\nTo work with time windows, Spark offers two type of windows\n\nNormal windows. You only consider event in a given windows. All windows are disjoint, and a event is only in one window.\nSliding windows. You have a fix window size (for example 1 hour) and a trigger time (for example 10 minute). Every 10 minute, you will process the data with an event time less than 1h.\n\n\nFigure 6 : Time-event processing, event grouped by sliding time windows\nTo create time windows, you need :\n\nto define a time window : window(column_with_time_event : str or col, your_time_window : str, timer_for_sliding_window) : str\ngrouping row by event-time using your window : df.groupeBy(window(...))\n\nTo produce the above processes :\n# Need some import\nfrom pyspark.sql.functions import window, col\n\n# word count + classic time window\ndf_with_event_time.groupBy(\n    window(df_with_event_time.event_time, \"5 minutes\"),\n    df_with_event_time.word).count()\n\n# word count + sliding time window\ndf_with_event_time.groupBy(\n    window(df_with_event_time.event_time, \"10 minutes\", \"5 minutes\"),\n    df_with_event_time.word).count()\n\n6.1 ✍Hand-on 5 : Event-time processing\n\nCount the number of event with a 10 seconds time window (use the created_at column)\n\n\n# Code Here\n\n\nCount the number of event by verified / unverified user with a 10 seconds time window (use the Creation_Time column)\n\n\n# Code Here\n\n\nCount the number of event with a 10 seconds time window sliding every 5 seconds\n\n\n# Code Here\n\n\n\n6.2 ​Handling late data with watermarks\nProcessing accordingly to time-event is great, but currently there is one flaw. We never specified how late we expect to see data. This means, Spark will keep some data in memory forever. Because streams never end, Spark will keep in memory every time windows, to be able to update some previous results. But in some cases, you know that after some time, you don’t expect new data, or very late data aren’t relevant any more. In other words, after a certain amount of time you want to freeze old results.\nOnce again, Spark can handle such process, with watermarks.\n\nFigure 7 : Time-event processing with watermark\nTo do so, you have to define column as watermark and a the max delay. You have to use the withWatermark(column, max_delay) method.\n# Need some import\nfrom pyspark.sql.functions import window, col\n\n# word count + classic time window\ndf_with_event_time.withWatermark(df_with_event_time.event_time, \"4 minutes\")\\\n.groupBy(\n    window(df_with_event_time.event_time, \"5 minutes\"),\n    df_with_event_time.word).count()\n\n# word count + sliding time window\ndf_with_event_time.withWatermark(df_with_event_time.event_time, \"4 minutes\")\\\n.groupBy(\n    window(df_with_event_time.event_time, \"10 minutes\", \"5 minutes\"),\n    df_with_event_time.word).count()\nBe careful, the watermark field cannot be a nested field (link)\n\n✍Hand-on 6 : Handling late data with watermarks\n\nCount the number of event with a 10 seconds time window (use the created_at column) with a 5 seconds watermark\n\n\n# Code Here\n\n\nCount the number of event by hashtag with a 30 seconds time window with a 1 minute watermark\n\n\n# Code Here\n\n\nCount the number of events post by verified user with a 10 seconds time window sliding every 5 seconds with 25 seconds watermark. Write the the result in a file sorted in S3.\n\n\n# Code Here"
  },
  {
    "objectID": "docs/lab/lab3/lab3-SSPCloud.html#for-more-details",
    "href": "docs/lab/lab3/lab3-SSPCloud.html#for-more-details",
    "title": "Stream processing with Spark",
    "section": "7 For more details",
    "text": "7 For more details\n\nSpark official documentation\nZAHARIA, B. C. M. (2018). Spark: the Definitive Guide. , O’Reilly Media, Inc. https://proquest.safaribooksonline.com/9781491912201\nhttps://databricks.com/blog/2018/03/13/introducing-stream-stream-joins-in-apache-spark-2-3.html\nhttps://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\nhttps://databricks.com/blog/2015/07/30/diving-into-apache-spark-streamings-execution-model.html"
  },
  {
    "objectID": "docs/lab/lab3/lab3-SSPCloud.html#end-of-the-lab",
    "href": "docs/lab/lab3/lab3-SSPCloud.html#end-of-the-lab",
    "title": "Stream processing with Spark",
    "section": "End of the Lab",
    "text": "End of the Lab\n\nExport your notebook\n\nRight clic and Download (.ipynb)\nFile &gt; Save and Export Notebook &gt; HTML\n\nDelete the Jupyter-pyspark service\n\nSSPCloud &gt; My services &gt; Delete\n\n\n\n\n\ndata stream"
  }
]